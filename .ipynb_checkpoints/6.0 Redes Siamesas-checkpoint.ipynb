{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "194274f5-67fc-46f5-a7b4-d1298181ed0e",
    "_uuid": "004920a0c8f7fa0a7b76c84769d703e91414ca29"
   },
   "source": [
    "# Overview\n",
    "With the kernel I am trying to run a simple test on using Siamese networks for similarity on a slightly more complicated problem than standard MNIST.  The idea is to take a randomly initialized network and apply it to images to find out how similar they are. The models should make it much easier to perform tasks like Visual Search on a database of images since it will have a simple similarity metric between 0 and 1 instead of 2D arrays.\n",
    "\n",
    " * [Source Blog Post](http://sujitpal.blogspot.ch/2017/04/predicting-image-similarity-using.html) with this [notebook](https://github.com/sujitpal/holiday-similarity/blob/master/src/02-holidays-siamese-network.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "84b0e79e-2070-4000-a76b-8c6f8786117b",
    "_uuid": "7571b744b894ca7d001cf78a7ce17b0939c50eb4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.utils.np_utils import to_categorical\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "aac47efc-c3f8-4e06-852d-e060fcc7170c",
    "_uuid": "901dc01a65cdec0b544a0bc5f86b6584163ffd7d"
   },
   "source": [
    "# Load and Organize Data\n",
    "Here we load and organize the data so we can easily use it inside of Keras models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "e79131cd-af9f-47bc-8cc0-41539beaad8d",
    "_uuid": "f172fc2cf53f1d2a8abdc355810cde4e0b1aacaa"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "data_train = pd.read_csv('./data/fashion-mnist_train.csv.zip',compression='zip')\n",
    "X_full = data_train.iloc[:,1:]\n",
    "y_full = data_train.iloc[:,:1]\n",
    "x_train, x_test, y_train, y_test = train_test_split(X_full, y_full, test_size = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "715029d1-dbb0-4b4f-b561-2940af7c6b8b",
    "_uuid": "a9ef1b7565b452a157780333b349e9aa238e2a9d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training (42000, 28, 28, 1) 1.0\n",
      "Testing (18000, 28, 28, 1) 1.0\n"
     ]
    }
   ],
   "source": [
    "x_train = x_train.values.reshape(-1, 28, 28, 1).astype('float32') / 255.\n",
    "x_test = x_test.values.reshape(-1, 28, 28, 1).astype('float32') / 255.\n",
    "y_train = y_train.values.astype('int')\n",
    "y_test = y_test.values.astype('int')\n",
    "print('Training', x_train.shape, x_train.max())\n",
    "print('Testing', x_test.shape, x_test.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "a8d657e3-188b-4e54-983d-d15d4b3bab35",
    "_uuid": "1a9921746a6218acb06843fafd0d6b5295f632af"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train groups: [4222, 4185, 4224, 4207, 4174, 4176, 4192, 4171, 4238, 4211]\n",
      "test groups: [1778, 1815, 1776, 1793, 1826, 1824, 1808, 1829, 1762, 1789]\n"
     ]
    }
   ],
   "source": [
    "# reorganize by groups\n",
    "train_groups = [x_train[np.where(y_train==i)[0]] for i in np.unique(y_train)]\n",
    "test_groups = [x_test[np.where(y_test==i)[0]] for i in np.unique(y_train)]\n",
    "print('train groups:', [x.shape[0] for x in train_groups])\n",
    "print('test groups:', [x.shape[0] for x in test_groups])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "f808e85d-a06f-46fd-bfa5-3abdd1891ac6",
    "_uuid": "f5e65ee27028b8a12dc0262b6dda46fc961da960"
   },
   "source": [
    "### Batch Generation\n",
    "Here the idea is to make usuable batches for training the network. We need to create parallel inputs for the $A$ and $B$ images where the output is the distance. Here we make the naive assumption that if images are in the same group the similarity is 1 otherwise it is 0.\n",
    "\n",
    "If we randomly selected all of the images we would likely end up with most images in different groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "92363237-06a8-45fb-8fee-ca3cbfceee28",
    "_uuid": "be862af7ac7aed4d0750901f7607f99fa13ccd9f",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_random_batch(in_groups, batch_halfsize = 8):\n",
    "    out_img_a, out_img_b, out_score = [], [], []\n",
    "    all_groups = list(range(len(in_groups)))\n",
    "    for match_group in [True, False]:\n",
    "        group_idx = np.random.choice(all_groups, size = batch_halfsize)\n",
    "        out_img_a += [in_groups[c_idx][np.random.choice(range(in_groups[c_idx].shape[0]))] for c_idx in group_idx]\n",
    "        if match_group:\n",
    "            b_group_idx = group_idx\n",
    "            out_score += [1]*batch_halfsize\n",
    "        else:\n",
    "            # anything but the same group\n",
    "            non_group_idx = [np.random.choice([i for i in all_groups if i!=c_idx]) for c_idx in group_idx] \n",
    "            b_group_idx = non_group_idx\n",
    "            out_score += [0]*batch_halfsize\n",
    "            \n",
    "        out_img_b += [in_groups[c_idx][np.random.choice(range(in_groups[c_idx].shape[0]))] for c_idx in b_group_idx]\n",
    "            \n",
    "    return np.stack(out_img_a,0), np.stack(out_img_b,0), np.stack(out_score,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "371eb798-eb9c-4318-a85e-99a437f13e9e",
    "_uuid": "9b7bea9e57008ba6d744e659cee7edaeecf63dcc"
   },
   "source": [
    "## Validate Data\n",
    "Here we make sure the generator is doing something sensible, we show the images and their similarity percentage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "724c5b9c-146e-4473-8a76-c9e71a3e461e",
    "_uuid": "59fcb14fe3e2dabc3891c77ab9c68f83bb0f8e90",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pv_a, pv_b, pv_sim = gen_random_batch(train_groups, 3)\n",
    "fig, m_axs = plt.subplots(2, pv_a.shape[0], figsize = (12, 6))\n",
    "for c_a, c_b, c_d, (ax1, ax2) in zip(pv_a, pv_b, pv_sim, m_axs.T):\n",
    "    ax1.imshow(c_a[:,:,0])\n",
    "    ax1.set_title('Image A')\n",
    "    ax1.axis('off')\n",
    "    ax2.imshow(c_b[:,:,0])\n",
    "    ax2.set_title('Image B\\n Similarity: %3.0f%%' % (100*c_d))\n",
    "    ax2.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "7ad59d9a-ed49-42bf-81c6-6c13801a6ba1",
    "_uuid": "d07aca571e287b5f7fa8a71c7f70cb6f62441b1e"
   },
   "source": [
    "# Feature Generation\n",
    "Here we make the feature generation network to process images into features. The network starts off randomly initialized and will be trained to generate useful vector features from input images (_hopefully_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "f6ed64c0-ad8b-40d8-a5a0-b2c5a99ea460",
    "_uuid": "be64ceda9605e1dd493fe2b429172111a422bcf3",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Conv2D, BatchNormalization, MaxPool2D, Activation, Flatten, Dense, Dropout\n",
    "img_in = Input(shape = x_train.shape[1:], name = 'FeatureNet_ImageInput')\n",
    "n_layer = img_in\n",
    "for i in range(2):\n",
    "    n_layer = Conv2D(8*2**i, kernel_size = (3,3), activation = 'linear')(n_layer)\n",
    "    n_layer = BatchNormalization()(n_layer)\n",
    "    n_layer = Activation('relu')(n_layer)\n",
    "    n_layer = Conv2D(16*2**i, kernel_size = (3,3), activation = 'linear')(n_layer)\n",
    "    n_layer = BatchNormalization()(n_layer)\n",
    "    n_layer = Activation('relu')(n_layer)\n",
    "    n_layer = MaxPool2D((2,2))(n_layer)\n",
    "n_layer = Flatten()(n_layer)\n",
    "n_layer = Dense(32, activation = 'linear')(n_layer)\n",
    "n_layer = Dropout(0.5)(n_layer)\n",
    "n_layer = BatchNormalization()(n_layer)\n",
    "n_layer = Activation('relu')(n_layer)\n",
    "feature_model = Model(inputs = [img_in], outputs = [n_layer], name = 'FeatureGenerationModel')\n",
    "feature_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "8acfa1dc-9077-4fc9-8b6d-564f6a30729b",
    "_uuid": "8f84b5cf735c25b94286fc8f56b9ab45f36d3ba1"
   },
   "source": [
    "# Siamese Model\n",
    "We apply the feature generating model to both images and then combine them together to predict if they are similar or not. The model is designed to very simple. The ultimate idea is when a new image is taken that a feature vector can be calculated for it using the _FeatureGenerationModel_. All existing images have been pre-calculated and stored in a database of feature vectors. The model can be applied using a few vector additions and multiplications to determine the most similar images. These operations can be implemented as a stored procedure or similar task inside the database itself since they do not require an entire deep learning framework to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "dc5ea3d4-1d80-4cca-8e7b-d1c58dd458a7",
    "_uuid": "c5ddc93960b88d6a983a18e1a1c807314ad150b9",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import concatenate\n",
    "img_a_in = Input(shape = x_train.shape[1:], name = 'ImageA_Input')\n",
    "img_b_in = Input(shape = x_train.shape[1:], name = 'ImageB_Input')\n",
    "img_a_feat = feature_model(img_a_in)\n",
    "img_b_feat = feature_model(img_b_in)\n",
    "combined_features = concatenate([img_a_feat, img_b_feat], name = 'merge_features')\n",
    "combined_features = Dense(16, activation = 'linear')(combined_features)\n",
    "combined_features = BatchNormalization()(combined_features)\n",
    "combined_features = Activation('relu')(combined_features)\n",
    "combined_features = Dense(4, activation = 'linear')(combined_features)\n",
    "combined_features = BatchNormalization()(combined_features)\n",
    "combined_features = Activation('relu')(combined_features)\n",
    "combined_features = Dense(1, activation = 'sigmoid')(combined_features)\n",
    "similarity_model = Model(inputs = [img_a_in, img_b_in], outputs = [combined_features], name = 'Similarity_Model')\n",
    "similarity_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "9a479eeb-bfa2-4a17-9e4d-154799d0300d",
    "_uuid": "bf91b28c1dac2b28f34569e9b3ca856d1152f4a9",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# setup the optimization process\n",
    "similarity_model.compile(optimizer='adam', loss = 'binary_crossentropy', metrics = ['mae'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6c05133b987ae8cbd3dacd4c836db429d6dd842c"
   },
   "source": [
    "## Visual Model Feedback\n",
    "Here we visualize what the model does by taking a small sample of randomly selected A and B images the first half from the same category and the second from different categories. We then show the actual distance (0 for the same category and 1 for different categories) as well as the model predicted distance. The first run here is with a completely untrained network so we do not expect meaningful results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d198e8b8ee793b742c60526526a1cbce1e7b8a5c",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def show_model_output(nb_examples = 3):\n",
    "    pv_a, pv_b, pv_sim = gen_random_batch(test_groups, nb_examples)\n",
    "    pred_sim = similarity_model.predict([pv_a, pv_b])\n",
    "    fig, m_axs = plt.subplots(2, pv_a.shape[0], figsize = (12, 6))\n",
    "    for c_a, c_b, c_d, p_d, (ax1, ax2) in zip(pv_a, pv_b, pv_sim, pred_sim, m_axs.T):\n",
    "        ax1.imshow(c_a[:,:,0])\n",
    "        ax1.set_title('Image A\\n Actual: %3.0f%%' % (100*c_d))\n",
    "        ax1.axis('off')\n",
    "        ax2.imshow(c_b[:,:,0])\n",
    "        ax2.set_title('Image B\\n Predicted: %3.0f%%' % (100*p_d))\n",
    "        ax2.axis('off')\n",
    "    return fig\n",
    "# a completely untrained model\n",
    "_ = show_model_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "9e2f2557-63c3-4c35-83bf-bd38351e1867",
    "_uuid": "7dbe0850a234b0763f9f00eb6949588af1e899bb",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# make a generator out of the data\n",
    "def siam_gen(in_groups, batch_size = 32):\n",
    "    while True:\n",
    "        pv_a, pv_b, pv_sim = gen_random_batch(train_groups, batch_size//2)\n",
    "        yield [pv_a, pv_b], pv_sim\n",
    "# we want a constant validation group to have a frame of reference for model performance\n",
    "valid_a, valid_b, valid_sim = gen_random_batch(test_groups, 1024)\n",
    "loss_history = similarity_model.fit_generator(siam_gen(train_groups), \n",
    "                               steps_per_epoch = 500,\n",
    "                               validation_data=([valid_a, valid_b], valid_sim),\n",
    "                                              epochs = 10,\n",
    "                                             verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "40b32060-c873-4f29-bd3a-65a7dc64f88e",
    "_uuid": "8f7b141665f09e06954f50583f43481e34c1e66a",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_ = show_model_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1e322e07d80a5daf422a58fe35a6444cfa203f14"
   },
   "source": [
    "# T-Shirt vs Ankle Boot-Plot\n",
    "Here we take an random t-shirt and ankle boot (categories 0 and 9) images and calculate the distance using our network to the other images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "150a8706c788dea46816d837cb1495faf1e82770",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t_shirt_vec = np.stack([train_groups[0][0]]*x_test.shape[0],0)\n",
    "t_shirt_score = similarity_model.predict([t_shirt_vec, x_test], verbose = True, batch_size = 128)\n",
    "ankle_boot_vec = np.stack([train_groups[-1][0]]*x_test.shape[0],0)\n",
    "ankle_boot_score = similarity_model.predict([ankle_boot_vec, x_test], verbose = True, batch_size = 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "fac5654fef0f2552b001fd1dd7ac70dec48734cf",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "obj_categories = ['T-shirt/top','Trouser','Pullover','Dress',\n",
    "                  'Coat','Sandal','Shirt','Sneaker','Bag','Ankle boot'\n",
    "                 ]\n",
    "colors = plt.cm.rainbow(np.linspace(0, 1, 10))\n",
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "for c_group, (c_color, c_label) in enumerate(zip(colors, obj_categories)):\n",
    "    plt.scatter(t_shirt_score[np.where(y_test == c_group), 0],\n",
    "                ankle_boot_score[np.where(y_test == c_group), 0],\n",
    "                marker='.',\n",
    "                color=c_color,\n",
    "                linewidth='1',\n",
    "                alpha=0.8,\n",
    "                label=c_label)\n",
    "plt.xlabel('T-Shirt Dimension')\n",
    "plt.ylabel('Ankle-Boot Dimension')\n",
    "plt.title('T-Shirt and Ankle-Boot Dimension')\n",
    "plt.legend(loc='best')\n",
    "plt.savefig('tshirt-boot-dist.png')\n",
    "plt.show(block=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "fbf9e442-4604-4a4d-b8b5-9d30cb99299e",
    "_uuid": "37934c83ab75a0930e1f6ec1e5dc1303e29869c3",
    "collapsed": true
   },
   "source": [
    "## Examining the Features\n",
    "Here we aim to answer the more general question: did we generate useful features with the Feature Generation model? And how can we visualize this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "f6fba813-fcec-427d-a69a-f99f574c846b",
    "_uuid": "1639aebf47636318e0ab22ad206fbe538e758338",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_test_features = feature_model.predict(x_test, verbose = True, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ca07855a4a47ff31429dff8c576896b9390787bd"
   },
   "source": [
    "## Neighbor Visualization\n",
    "For this we use the TSNE neighborhood embedding to visualize the features on a 2D plane and see if it roughly corresponds to the groups. We use the test data for this example as well since the training has been contaminated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "e851958f-df6f-4efe-9653-da2a5b6251ca",
    "_uuid": "7adba53e5b837d6efca4d022067d2c7c3a8109ba",
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.manifold import TSNE\n",
    "tsne_obj = TSNE(n_components=2,\n",
    "                         init='pca',\n",
    "                         random_state=101,\n",
    "                         method='barnes_hut',\n",
    "                         n_iter=500,\n",
    "                         verbose=2)\n",
    "tsne_features = tsne_obj.fit_transform(x_test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "c1681b97-5607-4051-a8d1-efbe2a4c1ad8",
    "_uuid": "26ea6aec333510280b143c870bc37e3fc4263b84",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "obj_categories = ['T-shirt/top','Trouser','Pullover','Dress',\n",
    "                  'Coat','Sandal','Shirt','Sneaker','Bag','Ankle boot'\n",
    "                 ]\n",
    "colors = plt.cm.rainbow(np.linspace(0, 1, 10))\n",
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "for c_group, (c_color, c_label) in enumerate(zip(colors, obj_categories)):\n",
    "    plt.scatter(tsne_features[np.where(y_test == c_group), 0],\n",
    "                tsne_features[np.where(y_test == c_group), 1],\n",
    "                marker='o',\n",
    "                color=c_color,\n",
    "                linewidth='1',\n",
    "                alpha=0.8,\n",
    "                label=c_label)\n",
    "plt.xlabel('Dimension 1')\n",
    "plt.ylabel('Dimension 2')\n",
    "plt.title('t-SNE on Testing Samples')\n",
    "plt.legend(loc='best')\n",
    "plt.savefig('clothes-dist.png')\n",
    "plt.show(block=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "e5363761-e430-4931-850e-c969bc7e4310",
    "_uuid": "2d30521ad626f9f6b5492f1a10a4bef6e276a0b3",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_model.save('fashion_feature_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "1ef2bc89-e47d-4260-b2d9-8fa5177921ca",
    "_uuid": "8a98c77220863cd966a0f720aa06a7ca64567b2f",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "similarity_model.save('fashion_similarity_model.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
