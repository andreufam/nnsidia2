{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNNs Sequence to Sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nas RNN $n \\times m$ que vimos, a \"interpretação\" ocorre concomitantemente à \"leitura\". Muitas vezes, contudo, é melhor esperar primeiro por uma interpretação do todo para, só então, iniciar o processo de decodificação. \n",
    "\n",
    "Arquiteturas que usam esta estratégias são as __Seq2seq__. Elas consistem normalmente de duas RNNs, uma codificadora e uma decodificadora, que operam como ilustrado a seguir:\n",
    "\n",
    "_sequência-entrada_ -> **[codificador]** -> _representação_ -> **[decodificador]** -> _sequência-saída_\n",
    "\n",
    "Assim, a ideia geral é usar a representação interna de uma rede codificadora para capturar o significado e contexto da entrada. Esta informação é então fornecida para a decodificadora que pode, a partir de um símbolo de partida e da representação da codificadora, ir prevendo a próxima saída decodificada até o fim da sequência.  \n",
    "\n",
    "Vamos estudar esta rede com uma aplicação em um problema muito comum em países de língua inglesa: soletrar uma palavra a partir de sua pronúncia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Soletrando a partir de pronúncias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No problema que vamos abordar, queremos traduzir a pronuncia de uma palavra, dada como uma lista de fonemas, para a grafia da palavra. Este problema é mais simples que _fala para texto_ ou _tradução_ (no sentido de não precisarmos de quantidades colossais de dados para ver algo acontecer [:)]); contudo, uma dificuldade aqui é a avaliação na escrita de palavras nunca vistas antes. Isto é díficil porque (1) há muitas pronúncias com várias transcrições razoáveis além de (2) palavras homônicas com transcrições distintas (_read_, no passado e presente, por exemplo). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manipulando os dados..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inicialmente temos que ler o dicionário de fonemas da CMU, _The CMU pronouncing dictionary_. Neste exemplo, disponibilizamos uma versão dele como CSV, já restritoa apenas a palavras, evitando símbolos especiais."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdic = pd.read_csv('data/cmudict-compact.csv', comment=';', \n",
    "                   header = -1, names = ['word', 'pronunciation'],\n",
    "                   keep_default_na = False,encoding='latin1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Em uma pequena amostra do dicionário, podemos ver as pronúncias codificadas com os símbolos da ARPAbet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>pronunciation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>40150</th>\n",
       "      <td>FACEY</td>\n",
       "      <td>F EY1 S IY0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40151</th>\n",
       "      <td>FACHET</td>\n",
       "      <td>F AE1 CH AH0 T</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40152</th>\n",
       "      <td>FACIAL</td>\n",
       "      <td>F EY1 SH AH0 L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40153</th>\n",
       "      <td>FACIALS</td>\n",
       "      <td>F EY1 SH AH0 L Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40154</th>\n",
       "      <td>FACIANE</td>\n",
       "      <td>F AA0 S IY0 AA1 N EY0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          word          pronunciation\n",
       "40150    FACEY            F EY1 S IY0\n",
       "40151   FACHET         F AE1 CH AH0 T\n",
       "40152   FACIAL         F EY1 SH AH0 L\n",
       "40153  FACIALS       F EY1 SH AH0 L Z\n",
       "40154  FACIANE  F AA0 S IY0 AA1 N EY0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdic[40150:40155]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embora não estritamente necessária para esta aula, temos abaixo um dicionário que indica para cada símbolo ARPAbet, seu correspondente IPA e uma palavra de exemplo que ilustra o som do fonema. Por exemplo, o som ʒ (ARPAbet ZH) corresponde ao som de /s/ em _pleasure_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "IPA_SYMBOLS = {'AXR':('ɚ','letter'), 'IY':('i','beat'), 'W':('w','wise'), \n",
    "               'DH':('ð','thy'), 'EL':('l̩','bottle'), 'WH':('ʍ','why'), \n",
    "               'Y':('j','yacht'), 'HH':('h','high'), 'CH':('tʃ','China'), \n",
    "               'JH':('dʒ','jive'), 'DX':('ɾ','butter'), 'ZH':('ʒ','pleasure'), \n",
    "               'EM':('m̩','rhythm'), 'D':('d','die'), 'NG':('ŋ','sing'), \n",
    "               'NX':('ɾ̃','winner'), 'TH':('θ','thigh'), 'H':('h','high'), \n",
    "               'AA':('ɑ','bot'), 'IX':('ɨ','rabbit'), 'B':('b','buy'), \n",
    "               'AE':('æ','bat'), 'EH':('ɛ','bet'), 'G':('ɡ','guy'), \n",
    "               'F':('f','fight'), 'AH':('ʌ','butt'), 'K':('k','kite'), \n",
    "               'M':('m','my'), 'L':('l','lie'), 'AO':('ɔ','bought'), \n",
    "               'N':('n','nigh'), 'Q':('ʔ','uh-oh'), 'IH':('ɪ','bit'), \n",
    "               'S':('s','sigh'), 'R':('ɹ','rye'), 'EY':('eɪ','bait'), \n",
    "               'T':('t','tie'), 'AW':('aʊ','bout'), 'V':('v','vie'), \n",
    "               'AY':('aɪ','bite'), 'AX':('ə','about'), 'Z':('z','zoo'), \n",
    "               'ER':('ɝ','bird'), 'UX':('ʉ','dude'), 'P':('p','pie'), \n",
    "               'UW':('u','boot'), 'SH':('ʃ','shy'), 'UH':('ʊ','book'), \n",
    "               'OY':('ɔɪ','boy'), 'OW':('oʊ','boat'), 'EN':('n̩','button')}\n",
    "\n",
    "def get_ipa_symbol(s):\n",
    "    lc = ''\n",
    "    if s[-1].isdigit():\n",
    "        lc = s[-1]\n",
    "        s = s[:-1]\n",
    "    return IPA_SYMBOLS[s][0] + lc if s in IPA_SYMBOLS else ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assim, por exemplo, a palavra _facial_ tem pronúnica _f-eɪ-ʃ-ʌ-l_ (F-EY1-SH-AH0-L, em ARPAbet, ou os sons de /f/ em _fight_, /ai/ em _bait_, /sh/ em _shy_, /u/ em _butt_ e /l/ em _lie_)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para manter o problema em um tamanho razoaável, vamos usar apenas uma fração do dicionário de fonemas. Também vamos filtrar as palavras muito curtas, muito longas ou com símbolos especiais."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_samples = 50000  # Number of samples to train on.\n",
    "pdic = pdic.sample(n = num_samples)\n",
    "\n",
    "def filter_input(inp):    \n",
    "    return ((len(inp) < 5 or      # filter long words \n",
    "             len(inp) > 15) or\n",
    "            # filter words with not alphabetical chars\n",
    "            any((not s.isalpha() for s in inp)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Em nosso problema, a entrada seão as sequências de fonemas e a saída, as palavras. O script abaixo extrai todas as entradas (listas de fonemas) e alvos (palavras), bem como os conjuntos de símbolos observados nas entradas e alvos (note que os alvos são sempre precedidos de um símbolo que indica início de sequência ['\\t'] e terminados em um que indica fim de sequência ['\\n']):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Vectorize the data.\n",
    "input_texts = []\n",
    "target_texts = []\n",
    "input_symbols = set()\n",
    "target_symbols = set()\n",
    "for idx, cols in pdic.iterrows():\n",
    "    target = cols['word']\n",
    "    if filter_input(target):\n",
    "        continue\n",
    "    # We use \"tab\" as the \"start sequence\" character\n",
    "    # for the targets, and \"\\n\" as \"end sequence\" character.\n",
    "    target_text = '\\t' + target + '\\n' # sequence of letters\n",
    "    target_texts.append(target_text) \n",
    "    input_text = cols['pronunciation'].split() # sequence of phonemes\n",
    "    input_texts.append(input_text)\n",
    "    for symbol in input_text:\n",
    "        if symbol not in input_symbols:\n",
    "            input_symbols.add(symbol)\n",
    "    for symbol in target_text:\n",
    "        if symbol not in target_symbols:\n",
    "            target_symbols.add(symbol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Abaixo, obtemos algumas estatísticas sobre nossos dados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 40310\n",
      "Max sequence length for inputs: 16\n",
      "Max sequence length for outputs: 17\n",
      "Number of unique input tokens: 69\n",
      "- AA0 AA1 AA2 AE0 AE1 AE2 AH0 AH1 AH2 AO0 AO1 AO2 AW0 AW1 AW2 AY0 AY1 AY2 B CH D DH EH0 EH1 EH2 ER0 ER1 ER2 EY0 EY1 EY2 F G HH IH0 IH1 IH2 IY0 IY1 IY2 JH K L M N NG OW0 OW1 OW2 OY0 OY1 OY2 P R S SH T TH UH0 UH1 UH2 UW0 UW1 UW2 V W Y Z ZH\n",
      "Number of unique output tokens: 28\n",
      "- '\\t \\n A B C D E F G H I J K L M N O P Q R S T U V W X Y Z'\n"
     ]
    }
   ],
   "source": [
    "input_symbols = sorted(list(input_symbols))\n",
    "target_symbols = sorted(list(target_symbols))\n",
    "num_encoder_tokens = len(input_symbols)\n",
    "num_decoder_tokens = len(target_symbols)\n",
    "max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
    "max_decoder_seq_length = max([len(txt) for txt in target_texts])\n",
    "\n",
    "print('Number of samples:', len(input_texts))\n",
    "print('Max sequence length for inputs:', max_encoder_seq_length)\n",
    "print('Max sequence length for outputs:', max_decoder_seq_length)\n",
    "print('Number of unique input tokens:', num_encoder_tokens)\n",
    "print('-', ' '.join(input_symbols))\n",
    "print('Number of unique output tokens:', num_decoder_tokens)\n",
    "print('-', repr(' '.join(target_symbols)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Abaixo, vamos criar os mapas que vão fornecer os mapeamentos de cada símbolo para o seu índice correspondente. Com isso, iniciamos os vetores de embeddings que serão usados para representar cada um dos símbolos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_token_index = dict(\n",
    "    [(s, i) for i, s in enumerate(input_symbols)])\n",
    "target_token_index = dict(\n",
    "    [(s, i) for i, s in enumerate(target_symbols)])\n",
    "\n",
    "encoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_encoder_seq_length, num_encoder_tokens),\n",
    "    dtype='float32')\n",
    "decoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
    "    dtype='float32')\n",
    "decoder_target_data = np.zeros(\n",
    "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
    "    dtype='float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A codificação que vamos usar aqui é _one-hot-vector_:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
    "    for t, sym in enumerate(input_text):\n",
    "        encoder_input_data[i, t, input_token_index[sym]] = 1.\n",
    "    for t, sym in enumerate(target_text):\n",
    "        # decoder_target_data is ahead of decoder_target_data by one timestep\n",
    "        decoder_input_data[i, t, target_token_index[sym]] = 1.\n",
    "        if t > 0:\n",
    "            # decoder_target_data will be ahead by one timestep\n",
    "            # and will not include the start character.\n",
    "            decoder_target_data[i, t - 1, target_token_index[sym]] = 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Abaixo, podemos visualizar uma das matrizes de representação esparsas geradas. As colunas correspondem aos símbolos e as linhas ao tempo. Cada 1 foi representado como um asterisco em um mar de pontos ;). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'\\tDECIPHER\\n'\n",
      "   tnABCDEFGHIJKLMNOPQRSTUVWXYZ\n",
      " 0 .....*......................\n",
      " 1 ......*.....................\n",
      " 2 ....*.......................\n",
      " 3 ..........*.................\n",
      " 4 .................*..........\n",
      " 5 .........*..................\n",
      " 6 ......*.....................\n",
      " 7 ...................*........\n",
      " 8 .*..........................\n",
      " 9 ............................\n",
      "10 ............................\n",
      "11 ............................\n",
      "12 ............................\n",
      "13 ............................\n",
      "14 ............................\n",
      "15 ............................\n",
      "16 ............................\n"
     ]
    }
   ],
   "source": [
    "def print_mat_as_map(m):\n",
    "    print('   tnABCDEFGHIJKLMNOPQRSTUVWXYZ')\n",
    "    for i in range(m.shape[0]):\n",
    "        print('%2d ' % i, end='')\n",
    "        for j in range(m.shape[1]):\n",
    "            print('%s' % '.' if m[i,j]==0 else '*', end='')\n",
    "        print('\\n', end='')\n",
    "        \n",
    "print('%s'%repr(target_texts[0]))\n",
    "print_mat_as_map(decoder_target_data[0, :, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Em uma aplicação com palavras (ex: tradução de idiomas), esta codificação seria muito ineficiente em espaço e carga semântica. Nestes casos, vamos optar por uma representação densa, ou seja, um _embedding_ de palavras. Estes serão obtidos de rede neural que modele linguagem. Há muitas disponíveis, incluindo várias arquiteturas baseadas em RNN/LSTM. As mais comumente usadas hoje, contudo, são redes rasas que codificam as palavras considerando suas co-ocorrências em grandes corpora de texto (ex: word2vec e Glove)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementando em tensorflow, usando Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora que os dados foram representados, podemos criar nossa arquitetura seq2seq:\n",
    "\n",
    "<img src=\"images/rnn_s2s0.png\" alt=\"Exemplo de RNN\" style=\"width: 600px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A primeira metade da RNN age como um decodificador, processando a sequência de entrada e retornando seu próprio estado interno. As saídas são descartadas e nem aparecem no desenho. O estado interno serve como contexto e condiciona a estimativa do decodificador."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:rgba(229, 255, 204, 0.5); text-align:left; vertical-align: middle; padding:10px 10px;\">\n",
    "\n",
    "Note que como vamos usar unidades LSTMs, o estado interno é constituído pela codificação que a LSTM gerou da entrada ($h_t$) e pela sua memória ($c_t$), conforme ilustrado abaixo:\n",
    "\n",
    "<img src=\"images/lstmcell.png\" alt=\"célula LSTM\" style=\"width: 200px;\"/>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A outra RNN é o decodificador. Ela é treinada para prever o próximo elemento na sequência alvo, dado seus predecessores. Ou seja, para ela a entrada é a sequência alvo e o alvo é a sequência alvo, um passo à frente (esse processo é chamado de 'teacher forcing'). O estado inicial do decodificador é o estado final do codificador. Ou seja, o decodificador aprende a gerar alvos[t+1..] de alvos[t...] condicionado à sequência de entrada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "latent_dim = 256  # Latent dimensionality of the encoding space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define an input sequence and process it.\n",
    "encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
    "encoder = LSTM(latent_dim, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "# We discard `encoder_outputs` and only keep the states.\n",
    "encoder_states = [state_h, state_c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set up the decoder, using `encoder_states` as initial state.\n",
    "decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
    "# We set up our decoder to return full output sequences,\n",
    "# and to return internal states as well. We don't use the\n",
    "# return states in the training model, but we will use them in inference.\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs,\n",
    "                                     initial_state=encoder_states)\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define the model that will turn\n",
    "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 64  # Batch size for training.\n",
    "epochs = 20 # 10  # Number of epochs to train for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 32248 samples, validate on 8062 samples\n",
      "Epoch 1/20\n",
      "32248/32248 [==============================] - 32s - loss: 1.1920 - val_loss: 0.9915\n",
      "Epoch 2/20\n",
      "32248/32248 [==============================] - 31s - loss: 0.9021 - val_loss: 0.8439\n",
      "Epoch 3/20\n",
      "32248/32248 [==============================] - 31s - loss: 0.7343 - val_loss: 0.6539\n",
      "Epoch 4/20\n",
      "32248/32248 [==============================] - 31s - loss: 0.5928 - val_loss: 0.5553\n",
      "Epoch 5/20\n",
      "32248/32248 [==============================] - 30s - loss: 0.4831 - val_loss: 0.4516\n",
      "Epoch 6/20\n",
      "32248/32248 [==============================] - 31s - loss: 0.4093 - val_loss: 0.3894\n",
      "Epoch 7/20\n",
      "32248/32248 [==============================] - 31s - loss: 0.3575 - val_loss: 0.3718\n",
      "Epoch 8/20\n",
      "32248/32248 [==============================] - 31s - loss: 0.3195 - val_loss: 0.3309\n",
      "Epoch 9/20\n",
      "32248/32248 [==============================] - 30s - loss: 0.2882 - val_loss: 0.3084\n",
      "Epoch 10/20\n",
      "32248/32248 [==============================] - 30s - loss: 0.2621 - val_loss: 0.2910\n",
      "Epoch 11/20\n",
      "32248/32248 [==============================] - 31s - loss: 0.2404 - val_loss: 0.2777\n",
      "Epoch 12/20\n",
      "32248/32248 [==============================] - 31s - loss: 0.2220 - val_loss: 0.2672\n",
      "Epoch 13/20\n",
      "32248/32248 [==============================] - 30s - loss: 0.2052 - val_loss: 0.2572\n",
      "Epoch 14/20\n",
      "32248/32248 [==============================] - 31s - loss: 0.1900 - val_loss: 0.2525s: 0.1\n",
      "Epoch 15/20\n",
      "32248/32248 [==============================] - 31s - loss: 0.1766 - val_loss: 0.2526\n",
      "Epoch 16/20\n",
      "32248/32248 [==============================] - 30s - loss: 0.1643 - val_loss: 0.2519\n",
      "Epoch 17/20\n",
      "32248/32248 [==============================] - 31s - loss: 0.1527 - val_loss: 0.2504\n",
      "Epoch 18/20\n",
      "32248/32248 [==============================] - 30s - loss: 0.1414 - val_loss: 0.2496\n",
      "Epoch 19/20\n",
      "32248/32248 [==============================] - 31s - loss: 0.1316 - val_loss: 0.2517\n",
      "Epoch 20/20\n",
      "32248/32248 [==============================] - 31s - loss: 0.1219 - val_loss: 0.2551\n"
     ]
    }
   ],
   "source": [
    "# Run training\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
    "model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_split=0.2)\n",
    "# Save model\n",
    "model.save('/tmp/ks2s.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para fazer a inferência, iremos usar a seguinte estratégia:\n",
    "\n",
    "1. Obtenha o estado do codificador para a sequência de entrada.\n",
    "2. Inicie com uma sequência alvo de tamanho 1 (apenas o símbolo de início de sequência).\n",
    "3. Dê o estado do codificador e a sequência criada até agora para o decodificador produzir uma distribuição de probabilidade para o próximo símbolo.\n",
    "4. Amostre o próximo símbolo usando a distribuição (no exemplo a sequir, é apenas usado argmax).\n",
    "5. Concatene o símbolo amostrado para a sequêcia alvo\n",
    "6. Repita desde 1 até encontrar o símbolo de fim de sequência ou alcançar o tamanho máximo de representação da saída.\n",
    "\n",
    "Note que esta estratégia poderia ter sido usada para treinar a rede também."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Next: inference mode (sampling).\n",
    "# Here's the drill:\n",
    "# 1) encode input and retrieve initial decoder state\n",
    "# 2) run one step of decoder with this initial state\n",
    "# and a \"start of sequence\" token as target.\n",
    "# Output will be the next target token\n",
    "# 3) Repeat with the current target token and current states\n",
    "\n",
    "# Define sampling models\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "    decoder_inputs, initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs] + decoder_states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos testar o nosso modelo de inferência."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# code a set of phonemes into an input for the RNN\n",
    "def encoder_test(phones):\n",
    "    encoder_test_data = np.zeros((1, max_encoder_seq_length, \n",
    "                                  num_encoder_tokens), dtype='float32')\n",
    "    for t, sym in enumerate(phones.split(' ')):\n",
    "        encoder_test_data[0, t, input_token_index[sym]] = 1.\n",
    "    return encoder_test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# show decoder softmax distribuition \n",
    "def plot_decoded_dist(v):\n",
    "    axis = np.arange(num_decoder_tokens)\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.bar(axis, v, align = 'center')\n",
    "    ax.set_xticks(axis)\n",
    "    ax.set_xticklabels([c for c in '<>ABCDEFGHIJKLMNOPQRSTUVWXYZ'])\n",
    "    plt.xlim([0, num_decoder_tokens])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoder_test_data = encoder_test('UW1 L ER0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 256) (1, 256)\n"
     ]
    }
   ],
   "source": [
    "# Encode the input as state vectors.\n",
    "states_value = encoder_model.predict(encoder_test_data)\n",
    "print(states_value[0].shape, states_value[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1, 28) (1, 256) (1, 256)\n"
     ]
    }
   ],
   "source": [
    "# Generate empty target sequence of length 1.\n",
    "target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "# Populate the first character of target sequence with the start character.\n",
    "target_seq[0, 0, target_token_index['\\t']] = 1.\n",
    "decoder_input = [target_seq] + states_value\n",
    "print(decoder_input[0].shape, decoder_input[1].shape, decoder_input[2].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "output_tokens, h, c = decoder_model.predict(decoder_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXEAAAEACAYAAABF+UbAAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAFolJREFUeJzt3X+8XHdd5/HXOy3lN8Vat8HUNisFihWUqqEsCFMqEhBN\nWRZIUFARyGMfBovo0vJA6K2r7lZRWbeIBAsLLEtQi5BdC3RF5iHQVlJ+lLYkJFA2m7QVKZUVKUga\nPvvHmVuGydyZc3PnNjm5r+fjMY8755zP+Z7vnTvzvud8z5mZVBWSpG5adaQ7IEk6fIa4JHWYIS5J\nHWaIS1KHGeKS1GGGuCR1WKsQT7I+ya4ku5NcOGb5dyd5X5JPJbkhyS/MvKeSpENk2nXiSVYBu4Hz\ngFuBHcDGqto1VHMxcJ+qemWSk4HPAqdU1V3L1nNJUqs98XXAnqraW1UHgG3AhpGavwceOLj/QODL\nBrgkLb/jW9SsAfYNTe+nCfZhbwI+mORW4AHAc2fTPUnSJLM6sflK4Pqq+l7gMcDrkzxgRm1LkhbQ\nZk/8FuC0oelTB/OGPR74bYCq+nySLwBnAtcNFyXxg1ok6TBUVcbNb7MnvgM4I8npSU4ANgLbR2p2\nAj8BkOQU4OHAzQt0pPXt4osvtt76FVl/NPXF+iNfP8nUPfGqOphkC3AVTehfXlU7k2xuFtdW4D8B\nb0lyPRDgFVV1x7S2JUlL02Y4hap6P/CIkXlvHLp/O/DTs+2aJGma4+bm5u6xjV1yySVzi93e2rVr\nrbd+RdYfTX2x/sjWX3LJJczNzV0ybtnUN/vMUpK6J7cnSceCJNQSTmxKko5ShrgkdZghLkkdZohL\nUocZ4pLUYYa4JHWYIS5JHWaIS1KHGeKS1GGGuCR1mCEuSR1miEtShxniktRhhrgkdZghLkkdZohL\nUocZ4pLuUatXryXJ1Nvq1WuPdFc7odU3+yRZD7yOb39R8qUjy38d+FmggHsBjwROrqqvjNT5zT7S\nCpeEJiqmVk79pveVYtI3+0wN8SSrgN3AecCtwA5gY1XtWqD+GcDLquonxiwzxKUVzhBfvKV+Pds6\nYE9V7a2qA8A2YMOE+k3AOxffTUnSYrUJ8TXAvqHp/YN5h0hyX2A9cMXSuyZJmmbWJzZ/GvjI6Fi4\nJGl5HN+i5hbgtKHpUwfzxtnIlKGUubm5u+/3ej16vV6LLkjSytHv9+n3+61q25zYPA74LM2JzduA\njwGbqmrnSN2JwM3AqVX19QXa8sSmtMJ5YnPxJp3YnLonXlUHk2wBruLblxjuTLK5WVxbB6XnAx9Y\nKMAlSbPX6jrxmW3MPXFpxXNPfPGWeomhJOkoZYhLUocZ4pLUYYa4JHWYIS5JHWaIS1KHGeKS1GGG\nuCR1mCEuSR1miEtShxniktRhhrgkdZghLkkdZohLUocZ4pLUYYa4JHWYIS5JHWaIS1KHGeKS1GGt\nQjzJ+iS7kuxOcuECNb0kn0xyY5IPzbabkqRxpn5RcpJVwG7gPOBWYAewsap2DdWcCFwN/GRV3ZLk\n5Kq6fUxbflGytML5RcmLt9QvSl4H7KmqvVV1ANgGbBipeR5wRVXdAjAuwCVJs9cmxNcA+4am9w/m\nDXs4cFKSDyXZkeT5s+qgJGlhx8+wnbOBJwP3B65Jck1VfW60cG5u7u77vV6PXq83oy5I0rGh3+/T\n7/db1bYZEz8HmKuq9YPpi4CqqkuHai4E7lNVlwym/xR4X1VdMdKWY+LSCueY+OItdUx8B3BGktOT\nnABsBLaP1LwXeEKS45LcD3gssHMpnZYkTTd1OKWqDibZAlxFE/qXV9XOJJubxbW1qnYl+QDwaeAg\nsLWqPrOsPZckTR9OmenGHE6RVjyHUxZvqcMpkqSjlCEuSR1miEtShxniktRhhrgkdZghLkkdZohL\nUocZ4pLUYYa4JHWYIS5JHWaIS1KHGeKS1GGGuCR1mCEuSR1miEtShxniktRhhrgkdZghLkkdZohL\nUoe1CvEk65PsSrI7yYVjlj8pyVeSfGJw+43Zd1WSNGrqt90nWQVcBpwH3ArsSPLeqto1Uvq3VfUz\ny9BHSdIC2uyJrwP2VNXeqjoAbAM2jKkb+03MkqTl0ybE1wD7hqb3D+aNelySTyX5qyQ/MJPeSZIm\nmjqc0tLHgdOq6s4kTwPeAzx8XOHc3Nzd93u9Hr1eb0ZdkKRjQ7/fp9/vt6pNVU0uSM4B5qpq/WD6\nIqCq6tIJ63wB+JGqumNkfk3bnqRjWxKgTQ4E86KRhKoaO2TdZjhlB3BGktOTnABsBLaPbOCUofvr\naP453IEkaVlNHU6pqoNJtgBX0YT+5VW1M8nmZnFtBf5dkn8PHAC+Djx3OTstSWpMHU6Z6cYcTpFW\nPIdTFm+pwymSpKOUIS5JHWaIS1KHGeKS1GGGuCR1mCEuSR1miEtShxniktRhhrgkdZghLkkdZohL\nUocZ4pLUYYa4JHWYIS5JHWaIS1KHGeKS1GGGuCR1mCEuSR1miEtSh7UK8STrk+xKsjvJhRPqfizJ\ngST/dnZdlCQtZGqIJ1kFXAY8FTgL2JTkzAXq/jPwgVl3UpI0Xps98XXAnqraW1UHgG3AhjF1LwX+\nAviHGfZPkjRBmxBfA+wbmt4/mHe3JN8LnF9VbwAyu+5JkiY5fkbtvA4YHitfMMjn5ubuvt/r9ej1\nejPqgiQdG/r9Pv1+v1VtqmpyQXIOMFdV6wfTFwFVVZcO1dw8fxc4Gfga8JKq2j7SVk3bnqRjWxKg\nTQ4E86KRhKoau3PcJsSPAz4LnAfcBnwM2FRVOxeofwvwP6vq3WOWGeLSCmeIL96kEJ86nFJVB5Ns\nAa6iGUO/vKp2JtncLK6to6ssuceSpFam7onPdGPuiUsrnnviizdpT9x3bEpShxniktRhhrgkdZgh\nLkkdZohLUocZ4pLUYYa4JHWYIS5JHWaIS1KHGeKS1GGGuCR1mCEuSR1miEtShxniktRhhrgkdZgh\nLkkdZohLUocZ4pLUYYa4JHVYqxBPsj7JriS7k1w4ZvnPJLk+ySeTXJfkybPvqiRp1NQvSk6yCtgN\nnAfcCuwANlbVrqGa+1XVnYP7jwL+sqrOGNOWX5QsrXB+UfLiLfWLktcBe6pqb1UdALYBG4YL5gN8\n4AHA7YfbWUlSe21CfA2wb2h6/2Ded0hyfpKdwJXAr8yme5KkSY6fVUNV9R7gPUmeALwdeMS4urm5\nubvv93o9er3erLogSceEfr9Pv99vVdtmTPwcYK6q1g+mLwKqqi6dsM7ngXVV9eWR+Y6JSyucY+KL\nt9Qx8R3AGUlOT3ICsBHYPrKBhw7dPxtgNMAlSbM3dTilqg4m2QJcRRP6l1fVziSbm8W1FXhWkhcA\n3wS+Bjx3OTstSWpMHU6Z6cYcTpFWPIdTFm+pwymSpKOUIS5JHWaIS1KHGeKS1GGGuCR1mCEuSR1m\niEtShxniktRhhrgkdZghLkkdZohLUocZ4tIyWL16LUkm3lavXnuku6ljgB+AJS2Ddh/ytDI/4MkP\nwFo8PwBLko5RhrgkdZghLkkdZohLUocZ4pLUYYa4JHVYqxBPsj7JriS7k1w4Zvnzklw/uH0kyaNm\n31VJ0qipIZ5kFXAZ8FTgLGBTkjNHym4GnlhVPwT8FvCmWXdUknSoNnvi64A9VbW3qg4A24ANwwVV\ndW1V/b/B5LXAmtl2U5I0TpsQXwPsG5rez+SQfhHwvqV0SpLUzvGzbCzJucAvAk9YqGZubu7u+71e\nj16vN8suSFLn9ft9+v1+q9qpn52S5BxgrqrWD6YvAqqqLh2pezRwBbC+qj6/QFt+dopWBD87ZWF+\ndsriLfWzU3YAZyQ5PckJwEZg+8gGTqMJ8OcvFOCSpNmbOpxSVQeTbAGuogn9y6tqZ5LNzeLaCrwa\nOAn44zT/Zg9U1brl7LgkyY+ilZaFwykLczhl8fwoWkk6RhniktRhhrgkdZghLkkdZohLUocZ4pLU\nYYa4JHWYIS5JHWaIS1KHGeKS1GGGuCR1mCEuSR1miEtShxniktRhhrgkdZghLkkdZohLUocZ4pLU\nYYa4JHVYqxBPsj7JriS7k1w4Zvkjklyd5BtJXj77bkqSxpn6bfdJVgGXAecBtwI7kry3qnYNlX0Z\neClw/rL0UpI0Vps98XXAnqraW1UHgG3AhuGCqrq9qj4O3LUMfZQkLaBNiK8B9g1N7x/MkyQdYVOH\nU2Ztbm7u7vu9Xo9er3dPd0GSjmr9fp9+v9+qNlU1uSA5B5irqvWD6YuAqqpLx9ReDHy1qv5ggbZq\n2vakY0ESYNpzPazE10O7xwZW6uMzThKqKuOWtRlO2QGckeT0JCcAG4Htk7Z3GH2UJB2GqcMpVXUw\nyRbgKprQv7yqdibZ3CyurUlOAa4DHgh8K8kFwA9U1T8vZ+claaWbOpwy0405nKIVwuGUhTmcsnhL\nHU6RJB2lDHFJ6jBDXJI6zBCXpA4zxCWpwwxxSeowQ1ySOswQl6QOM8QlqcMMcUnqMENckjrMEJek\nDjPEJanDDHFJ6jBDXJI6zBCXpA4zxCWpwwzxI2T16rUkmXpbvXrtke6qpKNYqxBPsj7JriS7k1y4\nQM0fJdmT5FNJfni23Tz2fPGLe2m+omryramTpPGmhniSVcBlwFOBs4BNSc4cqXka8NCqehiwGfiT\nWXSu3++vqHpY3vatP7L1i/n7Hm19P5oem8Npv+v1k7TZE18H7KmqvVV1ANgGbBip2QC8DaCq/g44\nMckpS+3c0fZAHstP5HHDO+eee+6ihneOtsfzaKs3xCeusaztd71+kjYhvgbYNzS9fzBvUs0tY2p0\nFBs/vHPxIfMc3pGOLp7YlKQOS1VNLkjOAeaqav1g+iKgqurSoZo/AT5UVe8aTO8CnlRVXxxpa/LG\nJEljVVXGzT++xbo7gDOSnA7cBmwENo3UbAd+GXjXIPS/MhrgkzohSTo8U0O8qg4m2QJcRTP8cnlV\n7UyyuVlcW6vqyiRPT/I54GvALy5vtyVJ0GI4RZJ09DpmTmwmOT/Jt5I8vEXtwSSfGLwx6brBENCk\n+lOSvHPwZqYdSf5XkjOmtH1jkk8meXmSicNIQ+t8cvDzFYusP21C7b9K8o4knxv0/aNJRi8RHa7/\n6sj0zyf5r5P6M269NtqsM1wzONrbleT7ZtGXwfPlbUPTxyX5UpLtU9b5vaHpX0vymgn1a5K8Z/BG\nuT1J/jDJgkfAQ3/bG5K8K8l9pvwOh9v+p5NckeT+k9ofrPOqwfP5+sG6P7ZA3UlDz8nbkuwfmj6k\nT0lOT3LDyLyLk7x8gfb/JslTRuZdkOT1Y2r/IMmvDE2/P8nWoenXJnnZyDqnJrk5yYMH0981mJ70\n+vpwkvVD089OcuUCtecPPR7zr9+DSZ66UPutVNUxcaO5fn07cHGL2n8auv+TQH9K/dXAi4emHwU8\nvkXbJwP/m+bEcKv+tPxdW9eP6fv3Ab/ctm3g54E/mmWfFrPOfA1wHrAbWDurvgBfBT4B3HswvX4w\nvX3COl8HPg+cNJj+NeA1E+r/DnjB4H6APwV+t+Vz878DL5vyOyyl/f8GvHxK++cAHwWOH0yfBKxu\n8di+pkXbpwOfHpl38ULrAS8C3jwy75pxr0XgWcC2ocflOuCjQ8uvBtaNWe/XgTcO7r8ReMWU3+Es\n4DPACcAD2j5HB+u+mOaCkEW9bkZvR2RPPMm9ktxvgWUPPoz27g88lubk6sY2qwzdPxG4Y0Lb5wLf\nrKo3zc+rqhuq6qPTNlJVtwMvAbYsoj9ttKpP8mTgX0b6vq+qDtlzOYolyY/TvKB+qqr+z4zbvxL4\nqcH9TcA7p9TfBWwFxu4tDhs8/l+vqvk3whXwq8ALp+1hD3wYGHvEN6P2rwEeOqXmIcDtVXXXYBt3\nVNXft2h7OS5iuAJ4+vxefZqLLR6ywGvxauDfDO6fBdwIfDXJiUlOAM6k+Yc96nXAY5NcMFj/9yd1\nqKpuotl5vAh4NfDWNs/RNCMGrwF+blrtNPdoiCc5M8lrgV3AwxYouy7J2wfh2dYG4ANVtQ/4hySP\nmVJ/38HhzE6aF+R/nFD7g8DHF9GX71BVXwBWJfmeFv2ZP9R69pRmh+uvmFB3FuOfqJPcb/hwD7hk\nkevP2r2BvwTOr6o9M267aI7gNiW5N/Bomj3baeu8HvjZJA+cUnsWI8+dqvoqsJeFwzkAg6B6GnDD\nAnXT2l8onOfbPw54CnDTxN+guaDhtMEw1uuTPHFK/bKpqn8EPkbzuECzw/ZnC9TeBhxIcipNGF9N\n87d9HPCjwA3z/5hG1rsLeAXwh8AFVXWwRdd+E3gezZHc704rHvxt3wH8alXd0qL9iZY9xJPcL8kv\nJPkwTWDeBDy6qq5fYJWH0ewNbUlyU5KLkjxkymY28e0/5p/TPKCT3FlVZ1fVI2meEG9v9cscvml7\nJfP9eczg558vov5ZrTuRXJbmPMCkoJpv++yqegzN4e2RdIDmBfii5Wi8qm4E1tI8h/6KFnuQVfXP\nwFuBCw5zs5O2cd8kn6AJq73A5YfZ/r2mtH8bzdDaxM85qqqvAWfTHFF+CdiW5AWH0aexzS9yPjT/\ndOePtjcy+cjpauDxNCF+DXDt0PSkI+mnA7fSDJtOVVV3Au8C3l7NR5NM81vAjVX1F23an+ae2BO/\nDXgh8EtV9cSqesvgiTFWNa4chNOTaPYo9ib50XH1Sb4LeDJweZKbgf8ATNuTHd7etcDJSU5eoOQm\nmv/chyXJ9wN3VdWXDreNJbgJ+JH5iaraQjO2POmo4GhzEHgOsC7JK5dpG9uB32P6UMqw/wL8EjB2\nWHDgM4w8d5I8iCY8P7fAOsP/RC8Yt7fYov1TgYWOWu6sqrOB04BvcOjnIB1i8Jr826qaA15KM948\nC1+mGWMfdhJw+4R13gucNzjavm9VfXJC7fyQyg/SDKdcS7Mn/rjBskOk+QTW82jOBbw87T8D6luD\n20RJesAzaYZ+Z+KeCPFn0XyWyruT/MakM73zkjwoyUtoXlxn0Fx3/ukFyp8NvK2q/nVVfX9VnQ58\nIckTJm1iaFtn0jwOXx5XWFV/A5yQ5EVD6zwqyeNbtP09wBuAaVd3LMuY+KDv905zTf+8aVcjHO5Y\n5nK9kStV9Q2acevnJXnhDPsyX/dm4JLB+GardQaH9n/GhCOEqvogzZ7vz8HdQxivBd4y+J2W0vdJ\n7f+PCTtK8/3/Bs2RxG9P2kaSh+c7r8T6YZojhCUb9PHW+aHTJCfRfFrqR6as06f5m037p3s18Azg\njsE/on8EHsyEEAf+mGYYZT/N0MjEMfHFGOxwvpnmRPSds2p32UO8qv66qjYBPw78E/DeJFctFOZJ\n3k4zzrcWeH5VnVtV76iqby6wiefSjJkOezeHvqt02H2GxnzfSfOgTjqEeybwlDSX6d0A/A6w0Mmd\n+bZvpBlPfH9V/eaEtr+jP4OfvzOlfjEX958P9JJ8Psm1wFtoxvxm0fZhrzcInH9p2+7gBfg04FVJ\nnjFlnfsm+b9J9g1+vmyBuvm2b6mqy1p2ffj3/H3gu5n8uz8TeE6S3TTngr4OvKpl+208E3j2oP3b\ngQfRHI1Obb+qPgXsSfLcCfUPAN6a5hLDTwGPBOYW2cdJXgC8evBa/GuaK7m+MGWdd9Kcv5gW4jfQ\n/H2uGZn3lao65GKGJC8G9g52fqDZATtzcGJ9FjbTHAW/YZHnwCY6Im/2GQyN3DZuUH/wAr2yqqYe\nmqi7kvwQzaVcE6/RV3tp3u/wJuA5VbXzSPdH9wzfsal73GB456U0h60fPNL9kbrMEJekDjtm3nYv\nSSuRIS5JHWaIS1KHGeKS1GGGuCR1mCEuSR32/wGjEeZjJPLkfQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe7026a2e90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_decoded_dist(output_tokens[0,0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Reverse-lookup token index to decode sequences back to\n",
    "# something readable.\n",
    "reverse_input_char_index = dict(\n",
    "    (i, char) for char, i in input_token_index.items())\n",
    "reverse_target_char_index = dict(\n",
    "    (i, char) for char, i in target_token_index.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0, target_token_index['\\t']] = 1.\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict(\n",
    "            [target_seq] + states_value)\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if (sampled_char == '\\n' or\n",
    "           len(decoded_sentence) > max_decoder_seq_length):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "\n",
    "    start = 0 if decoded_sentence[0] != ' ' else 1\n",
    "    return decoded_sentence[start:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Guess         Correct = Phonemes\n",
      "        DECIFER        DECIPHER   d ɪ0 s aɪ1 f ɝ0\n",
      "       HAMLISCH        HAMLISCH + h æ1 m l ɪ0 ʃ\n",
      "        ENLARGE         ENLARGE + ɛ2 n l ɑ1 ɹ dʒ\n",
      "        BACUOUS          BOCUSE   b oʊ0 k j u1 z\n",
      "    DELAPIDATED     DILAPIDATED   d ɪ0 l æ1 p ʌ0 d eɪ2 t ɪ0 d\n",
      "       SHARPLES        SHARPLES + ʃ ɑ1 ɹ p ʌ0 l z\n",
      "          ABATE           ABBOT   æ1 b ʌ0 t\n",
      "       BRACKENS        BRACKENS + b ɹ æ1 k ʌ0 n z\n",
      "         MIKEXA         MIKESKA   m ɪ0 k ɛ1 s k ʌ0\n",
      "          BEAST          BEASTS   b i1 s t s\n",
      "       MAROONEY        MAROONEY + m ɝ0 u1 n i0\n",
      "     ESPINSHIDE     ESPENSCHIED   ɛ1 s p ɪ0 n ʃ i0 d\n",
      "          MOCHA           MOCHA + m oʊ1 k ʌ0\n",
      "      MELLOWING       MELLOWING + m ɛ1 l oʊ0 ɪ0 ŋ\n",
      "     CLEAVELAND      CLEAVELAND + k l i1 v l ʌ0 n d\n",
      "         ALFHEY           ALFIE   æ1 l f i0\n",
      "          PENJA           PENJA + p ɛ1 n dʒ ʌ0\n",
      "       WORTHLEY        WORTHLEY + w ɝ1 θ l i0\n",
      "        ZHIVKOV         ZHIVKOV + ʒ ɪ1 v k ɑ0 v\n",
      "    APPREHENDED     APPREHENDED + æ2 p ɹ ɪ0 h ɛ1 n d ʌ0 d\n",
      "        FISCHEL         FISCHEL + f ɪ1 ʃ ʌ0 l\n",
      "        DISGUST         DISGUST + d ɪ0 s ɡ ʌ1 s t\n",
      "     WETHINGTON     WEATHINGTON   w ɛ1 θ ɪ0 ŋ t ʌ0 n\n",
      "          LANGE           LANGE + l æ1 ŋ\n",
      "          FINED           FINED + f aɪ1 n d\n",
      "          SANKA           SANKA + s æ1 ŋ k ʌ0\n",
      "      RINFERATS      RAINFOREST   ɹ aɪ1 n f ɔ2 ɹ ʌ0 s t\n",
      "         LEIBER          LIEBER   l i1 b ɝ0\n",
      "CLASSIFICATIONS CLASSIFICATIONS + k l æ2 s ʌ0 f ʌ0 k eɪ1 ʃ ʌ0 n z\n",
      " CONSIDERATIONS   CONSIDERATION   k ʌ0 n s ɪ2 d ɝ0 eɪ1 ʃ ʌ0 n\n",
      "         WADING          WADING + w eɪ1 d ɪ0 ŋ\n",
      "       GRUMBLES        GRUMBLES + ɡ ɹ ʌ1 m b ʌ0 l z\n",
      "        LITHGOW         LITHGOW + l ɪ1 θ ɡ aʊ0\n",
      "         REIBER          REIBER + ɹ aɪ1 b ɝ0\n",
      "          CALES          CALLES   k eɪ1 l z\n",
      "         FORCOR         FORQUER   f ɔ1 ɹ k ɝ0\n",
      "          BIGGS           BIGGS + b ɪ1 ɡ z\n",
      "      VIBRATING       VIBRATING + v aɪ1 b ɹ eɪ0 t ɪ0 ŋ\n",
      "         KELLAR          KELLAR + k ɛ1 l ɝ0\n",
      "        EPITAPH         EPITAPH + ɛ1 p ʌ0 t æ2 f\n",
      "         BOLDON          BOLDON + b oʊ1 l d ʌ0 n\n",
      "          LOPSE           LOPES   l oʊ1 p s\n",
      "   KARZENIEWSKI    KORZENIEWSKI   k ɝ0 z ɪ2 n i0 ɛ1 f s k i0\n",
      "        FLORRIA          FLOREA   f l ɔ1 ɹ i0 ʌ0\n",
      "        COURANT         COURANT + k ʊ1 ɹ ʌ0 n t\n",
      "        CARDOSA         CARDOSA + k ɑ0 ɹ d oʊ1 s ʌ0\n",
      "        DOGGETT         DOGGETT + d ɑ1 ɡ ɪ0 t\n",
      "        TOUTENT         TOUTANT   t u0 t ɔ1 n t\n",
      "          RORKE          ROARKE   ɹ ɔ1 ɹ k\n",
      "          CABET           KABAT   k æ1 b ʌ0 t\n"
     ]
    }
   ],
   "source": [
    "print('%15s %15s = %s' % ('Guess', 'Correct', 'Phonemes'))\n",
    "for seq_index in range(50):\n",
    "    # Take one sequence (part of the training test)\n",
    "    # for trying out decoding.\n",
    "    input_seq = encoder_input_data[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    inputs = ' '.join(input_texts[seq_index])\n",
    "    correct = pdic[pdic['pronunciation']==inputs]['word'].iloc[0]\n",
    "    ok = '+' if decoded_sentence[:-1] == correct else ' '\n",
    "    ipa_inputs = ' '.join([get_ipa_symbol(c) for c in inputs.split(' ')])\n",
    "    print('%15s %15s %s %s'%(decoded_sentence[:-1], correct, ok, ipa_inputs))  \n",
    "    #print('%15s %15s %s %s'%(decoded_sentence[:-1], correct, ok, inputs))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mecanismos de Atenção"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Em nosso modelo, quando o decodificador processa suas entradas, toda a informação de contexto se concentra nos estados recebidos do codificador. Deste modo, a entrada do decodificador é um gargalo e ele não tem como relacionar diretamente aspectos sendo decodificados com os elementos fonte diretamente associados a estes aspectos. Este é um problema particularmente importante com sequências muito longas. Nestes casos, à medida que o decodificador processa a informação, ele tende a ficar menos preso à fonte e mais impressionado com o material que esta produzindo. Por exemplo, imagine que a frase a ser traduzida seja `You can't make an omelet without breaking a few legs`. Ao alcançar a tradução de `legs`, a rede pode achar que é mais provável a palavra `ovos` (`eggs`) porque ela é mais razoável no contexto da sentença sendo gerada (`Você não pode fazer um omelete sem quebrar alguns...`), _a despeito dela não ser a tradução correta_. Este problema se manifesta de várias formas. Em modelos de tradução, por exemplo, o decodificador 'esquece' a concordância de gênero, tempo, pessoa, etc.\n",
    "\n",
    "Para resolver este problema, Bahdanau et al., 2015 (https://arxiv.org/abs/1409.0473) e Luong et al., 2015 (https://arxiv.org/abs/1508.04025) introduziram a ideia de _mecanismos de atenção_. \n",
    "\n",
    "A ideia central é o estabelecimento de uma ligação direta entre as estimativas do codificador e do decodificador, de forma que o codificador preste atenção em informações relevantes do codificador enquanto ele traduz/decodifica. Como resultado, este mecanismo ajuda muito na tradução de sentenças longas e se tornou o padrão de fato em tradução, com aplicação também em diversos outros problemas (fala, sumarização, _captioning_, etc). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entre os vários mecanismos de atenção na literatura, vamos descrever o proposto por Luong et al., 2015. Ele é implmentado no tensorflow. A figura abaixo ilustra a computação da atenção no primeiro passo da decodificação:\n",
    "\n",
    "<img src=\"images/rnn_s2s_attention.png\" alt=\"Exemplo de RNN\" style=\"width: 600px;\"/>\n",
    "\n",
    "A computação consiste em 3 passos:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* __Passo 1__: O estado oculto atual é comparado com todos os estados no codificador (fonte) para derivar os _attention weights_. A figura abaixo ilustra esses pesos (exemplo de Bahdanau et al, 2015):\n",
    "\n",
    " <img src=\"images/attention_vis.jpg\" alt=\"Exemplo de RNN\" style=\"width: 400px;\"/>\n",
    "\n",
    " Note que os attention weights possibilitam uma fácil visualização do alinhamento entre as sentenças de entrada e saída, um subproduto interessante de adotá-los. No exemplo, fica claro que o conceito _european economic area_ está assoaciado (alinhado) a um conceito correspondente no codificador, _zone économique européenne_.\n",
    "\n",
    " Este passo pode ser descrito como:\n",
    "\n",
    " $$\\alpha_{ts} = \\frac{e^{score({\\bf h}_t, {\\bf {\\bar h}}_s)}}{\\sum_{s''}{e^{score({\\bf h}_t, {\\bf {\\bar h}}_s')}}}$$\n",
    "\n",
    " onde a comparação entre os vetores pode ser feita com diferentes funções _score_. É em geral na definição de _score_ que reside a diferença entre vários mecanismos de atenção. Por exemplo, em Bahdanau et al (2015), a função _score_ é dada por:\n",
    "\n",
    " $$score({\\bf h}_t, {\\bf {\\bar h}}_s) = {{\\bf h}_t}^\\intercal {\\bf W} {\\bf {\\bar h}}_s$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* __Passo 2__: os _attention weights_ são agregados para formar o _context vector_. A agregação é feita como uma média ponderada dos estados.\n",
    "\n",
    "$${\\bf c}_t = \\sum_s{\\alpha_{ts} {\\bf {\\bar h}}_s}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* __Passo 3__: o estado oculto atual é então combinado como o _context vector_ ($[{\\bf c}_t; {\\bf h}_t]$) para produzir o _attention vector_ final. Este vetor é fornecido como entrada para a previsão do próximo passo.\n",
    "\n",
    "$$\\alpha_t = f({\\bf c}_t, {\\bf h}_t) = tanh({\\bf W}_c [{\\bf c}_t; {\\bf h}_t])$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O tensorflow implementa diferentes mecanismos de atenção. Recomendo a leitura de Thang Luong, Eugene Brevdo, and Rui Zhao (https://github.com/tensorflow/nmt), para uma visão detalhada destas técnicas e suas melhores implementações em TF."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algumas observações finais\n",
    "\n",
    "* A Arquitetura de uma RNN pode ser bidirecional. Ou seja, cada célula pode se conectar a uma sucessora ($t+1$) e a uma antecessora ($t-1$). No caso de LSTMs, que têm memória com fluxo separado na sequ6encia, a estrutura bidirecional pode ser formada por duas seqências de células, uma da esquerda para direita e outra da direita para esquerda. Assim, cada célula na primeira sequência tem uma correspondente na segunda e elas compartilham entradas e saídas. Modelos bidirecionais tipicamente alcançam resultados melhores que os unidirecionais em muitas aplicações.\n",
    "\n",
    "<img src=\"images/bilstm.png\" alt=\"bi-directional LSTM\" style=\"width: 400px;\"/>\n",
    "\n",
    "* Embora em processamento de texto seja comum a modelagem de palavras de forma discreta, este não é o caso em redes neurais cuja entrada é formada por palavras. As razões para isso são eficiência e perda de informação semântica. Assim, nesses casos, é melhor usar os códigos gerados por redes neurais que modelam linguagens (embeddings). Estes códigos curtos e densos gerados por redes usadas exclusivamente como sistemas codificadores serão o foco das próximas aulas. E a ideia de redes auto-codificadoras também.\n",
    "\n",
    "<img src=\"images/bi-lstm-embeddings.png\" alt=\"bi-directional LSTM\" style=\"width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este curso é baseado em material da [Big Data University](https://bigdatauniversity.com/?utm_source=bducopyrightlink&utm_medium=dswb&utm_campaign=bdu). Assim, segue os termos da [licença do MIT](https://bigdatauniversity.com/mit-license/). Material adicional de François Chollet (https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html), Mikesj (https://github.com/mikesj-public/rnn_spelling_bee/blob/master/spelling_bee_RNN.ipynb), Thang Luong, Eugene Brevdo, and Rui Zhao (https://github.com/tensorflow/nmt)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
