{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Redes de Convolução"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# reinicia grafo do tensorflow\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convoluções"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import skimage.measure # scikit-image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para entendermos convolução e pooling, vamos inicialmente ver o que essas operações fazem em pequenos exemplos de imagens.\n",
    "\n",
    "Para isso, vamos criar uma função para exibir matrizes como imagens em tons de cinza."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_figs(lst):\n",
    "    if len(lst) == 1:\n",
    "        plt.matshow(lst[0], cmap = 'gray', interpolation='nearest')\n",
    "    else:\n",
    "        f, axes = plt.subplots(1, len(lst))\n",
    "        for i, a in enumerate(axes):\n",
    "            a.matshow(lst[i], cmap = 'gray', interpolation='nearest')\n",
    "            a.set(aspect='equal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E então exibir exemplos de imagens simples (ex0 e ex1) e de um kernel simples (k0):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ex0 = np.array([[0,0,0,1,0,0,0,0],\n",
    "                [0,0,1,0,0,0,0,0],\n",
    "                [0,1,0,0,0,0,0,0],\n",
    "                [1,0,0,0,0,0,0,0],\n",
    "                [0,0,0,0,1,0,0,0],\n",
    "                [0,0,0,0,0,1,0,0],\n",
    "                [0,0,0,0,0,0,1,0],\n",
    "                [0,0,0,0,0,0,0,1]], dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ex1 = np.array([[0,0,0,0,0,0,0,1],\n",
    "                [0,0,0,0,0,0,0,1],\n",
    "                [0,0,0,0,0,0,1,0],\n",
    "                [0,0,0,0,0,0,1,0],\n",
    "                [0,0,0,0,0,1,0,0],\n",
    "                [0,0,0,0,0,1,0,0],\n",
    "                [0,0,0,0,1,0,0,0],\n",
    "                [0,0,0,0,1,0,0,0]], dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "k0 = np.array([[0,0,0,1],\n",
    "               [0,0,1,1],\n",
    "               [0,0,1,0],\n",
    "               [0,1,0,0]], dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_figs([ex0, ex1, k0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O componente básico da operação de convolução é o produto (interno) entre a entrada ($\\bf{x}$) e o kernel ($\\bf{w}$), com saída opcionalmente transformada, por exemplo, para ter uma interpretação probabilística. Ou seja, ela pode ser representada como $\\sigma({\\bf x}^{T} {\\bf w})$, onde $\\sigma$ pode ser a função sigmoid:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv0(ilst, k):\n",
    "    sigmoid = lambda n: 1. / (1. + np.exp(-n))\n",
    "    kr = k.reshape((-1, 1))\n",
    "    s = 0\n",
    "    for i in ilst:\n",
    "        lstr = i.reshape((-1, 1))\n",
    "        s += np.sum(np.multiply(lstr, kr))\n",
    "    return sigmoid(s)\n",
    "    #return sigmoid(np.dot(lstr[i].T, kr))[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_figs([ex0[:4,:4], k0])\n",
    "conv0([ex0[:4,:4]], k0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ao aplicar conv0 para k0 e a parte superior esquerda de ex0, notamos que o padrão k0 é parcialmente observado em ex0 (probabilidade de 88%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_figs([ex0[4:,4:], k0])\n",
    "conv0([ex0[4:,4:]], k0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ao aplicar conv0 para k0 e a parte inferior direira de ex0, notamos que o padrão k0 é observado com certeza menor (probabilidade de 73%), umas vez que houve coincidência em um único ponto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex1p = skimage.measure.block_reduce(ex1, (2,2), np.max)\n",
    "plot_figs([ex1, ex1p, k0])\n",
    "conv0([ex1p], k0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ao aplicar conv0 para k0 e uma versão _reduzida_ de ex1, notamos que o padrão k0 é observado com certo grau de certeza (probabilidade de 95%).\n",
    "\n",
    "Destes exemplos, podemos interpretar a operação de convolução como um problema de dizer se um padrão pode ou não ser visto em uma imagem. Podemos ver também que reduzir a imagem (uma operação de _pooling_) pode tornar um padrão visível.\n",
    "\n",
    "Agora vamos ver a aplicação de um kernel sobre mais de uma entrada:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_figs([ex0[:4,:4], ex1p, k0])\n",
    "conv0([ex0[:4,:4], ex1p], k0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A seguir, vamos ver a aplicação da operação de convolução completa de um kernel sobre uma imagem.\n",
    "\n",
    "Para isso vamos usar o operador de convolução conv2D do tensorflow. Os parâmetros deste operador são a imagem de entrada (tensor 4D [número de batches, altura, largura, número de canais]), o conjunto de kernels a serem aplicados (tensor 4D [altura, largura, canais de entrada, canais de saída]), os delocamentos a serem aplicados (no conjunto de batches, nos pixels da altura, pixels da largura e canais de entrada) e o tipo de _padding_ a ser realizado ('VALID' = sem padding ou 'SAME' = tentar manter o mesmo padding em cada margem). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tfconv(imgs, kernels):\n",
    "    reset_graph()\n",
    "\n",
    "    # batch, alt, larg, canais entrada\n",
    "    X = tf.constant(imgs, dtype=tf.float32) \n",
    "    feature_maps = tf.constant(kernels)\n",
    "    convolution = tf.nn.conv2d(X, feature_maps, strides=stride, \n",
    "                               padding=\"SAME\")\n",
    "    \n",
    "    with tf.Session() as s:\n",
    "        output = convolution.eval(feed_dict={X: imgs})\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alt, larg, canais de entrada, canais de saida\n",
    "fmap = k0.reshape(k0.shape[0], k0.shape[1], 1, 1)\n",
    "output = tfconv(ex0.reshape(1, ex0.shape[0], ex0.shape[1], 1), fmap)\n",
    "plot_figs([ex0, k0, output[0, :, :, 0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ao aplicar convolução do kernel k0 em ex0, notamos que o padrão k0 é observado na parte superior esquerda da imagem, mas não muito claramente na parte inferior esquerda. Esta é a forma como pode ser interpretada a imagem resultante da convolução."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = tfconv(ex1.reshape(1, ex1.shape[0], ex1.shape[1], 1), fmap)\n",
    "plot_figs([ex1, k0, output[0, :, :, 0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ao aplicar convolução do kernel k0 em ex1, notamos que o padrão k0 parece ser observado ao longo de ex1, ao lado esquerdo (em particular, mais na parte superior que na parte inferior da imagem). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Na prática, contudo, vamos usar kernels não apenas como filtros, mas também como combinadores lineares. Para tanto, basta que a entrada tenha mais canais que a saída, como no exemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# alt, larg, canais de entrada, canais de saida\n",
    "fmap = np.zeros((k0.shape[0], k0.shape[1], 2, 1), dtype = np.float32)\n",
    "fmap[:,:,0,0] = k0\n",
    "fmap[:,:,1,0] = k0\n",
    "# batch, altura, largura, canais de entrada\n",
    "icanais = np.zeros((1, ex0.shape[0], ex0.shape[1], 2), dtype = np.float32)\n",
    "icanais[0,:,:,0] = ex0\n",
    "icanais[0,:,:,1] = ex1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "output = tfconv(icanais, fmap)\n",
    "plot_figs([ex0, ex1, k0, output[0, :, :, 0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outra observação interessante é que, embora um filtro 1x1 não tenha efeito nenhum quando aplicado sobre uma única entrada, ele se comporta como um combinador linear quando aplicado a múltiplas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# alt, larg, canais de entrada, canais de saida\n",
    "fmap = np.zeros((1, 1, 2, 1), dtype = np.float32)\n",
    "fmap[:,:,0,0] = np.array([1])\n",
    "fmap[:,:,1,0] = np.array([1])\n",
    "# batch, altura, largura, canais de entrada\n",
    "icanais = np.zeros((1, ex0.shape[0], ex0.shape[1], 2), dtype = np.float32)\n",
    "icanais[0,:,:,0] = ex0\n",
    "icanais[0,:,:,1] = ex1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = tfconv(icanais, fmap)\n",
    "plot_figs([ex0, ex1, k0, output[0, :, :, 0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente, vamos ver o efeito de padrões (kernels) mais complexos em uma imagem real."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "im = Image.open('images/building.jpeg') \n",
    "image = np.asarray(im) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "height, width, channels = image.shape\n",
    "image_grayscale = image.mean(axis=2).astype(np.float32)\n",
    "images = image_grayscale.reshape(1, height, width, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(images[0,:,:,0], cmap = 'gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para tanto, vamos usar dois padrões: uma linha verticar e outra horizontal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fmap = np.zeros(shape=(7, 7, 1, 2), dtype=np.float32)\n",
    "fmap[:, 3, 0, 0] = 1\n",
    "fmap[3, :, 0, 1] = 1\n",
    "plot_figs([fmap[:, :, 0, 0], fmap[:, :, 0, 1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = tfconv(images, fmap)\n",
    "plot_figs([output[0, :, :, 0], output[0, :, :, 1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Percebam que, na imagem resultante, são ressaltadas as linhas verticais e horizontais da imagem original, enfatizando _onde_ esses padrões são vistos.\n",
    "\n",
    "### Exercício ###\n",
    "\n",
    "Aplique o kernel com uma linha vertical sobre todos os canais da imagem original colorida do prédio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"right\">\n",
    "<a href=\"#modelkf1\" class=\"btn btn-default\" data-toggle=\"collapse\">Solução #1</a>\n",
    "</div>\n",
    "<div id=\"modelkf1\" class=\"collapse\">\n",
    "```\n",
    "\n",
    "fmap2 = np.zeros(shape=(7, 7, 3, 1), dtype=np.float32)\n",
    "fmap2[:, 3, 0, 0] = 1\n",
    "fmap2[:, 3, 1, 0] = 1\n",
    "fmap2[:, 3, 2, 0] = 1\n",
    "\n",
    "imager = image.reshape(1, height, width, 3)\n",
    "\n",
    "output = tfconv(imager, fmap2,stride=[1,1,1,1])\n",
    "plot_figs([output[0, :, :, 0]])\n",
    "```\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A seguir, vamos parametrizar nossa função de convolução para observar o efeito de diferentes strides e padding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tfconv(img, kernels, stride_val = 1, pad_val = 'SAME'):\n",
    "    reset_graph()\n",
    "\n",
    "    # batch, alt, larg, canais entrada\n",
    "    X = tf.constant(img, dtype=tf.float32) \n",
    "    feature_maps = tf.constant(kernels)\n",
    "    convolution = tf.nn.conv2d(X, feature_maps, \n",
    "                               strides=[1, stride_val, stride_val, 1], \n",
    "                               padding=pad_val)\n",
    "    \n",
    "    with tf.Session() as s:\n",
    "        output = convolution.eval(feed_dict={X: img})\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stride 4 --> imagem é reduzida 4 vezes\n",
    "output4 = tfconv(images, fmap, 4)\n",
    "plot_figs([output[0, :, :, 0], output4[0, :, :, 0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sem padding, quase não se nota mudança em uma imagem tão grande, exceto nas bordas.\n",
    "outputv = tfconv(images, fmap, pad_val = 'VALID')\n",
    "plot_figs([output[0, :, :, 0], outputv[0, :, :, 0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos agora introduzir pooling no processo. Pooling é o processo de se compactar várias entradas próximas em um só valor, por exemplo, no max pooling com 2x2, quadrados de 4 pixels são comprimidos em apenas uma saída, cujo valor será o maior destas 4. Outra alternativa é o average pooling, onde o valor é a média\n",
    "\n",
    "Note que o efeito do pooling é similar a striding na convolução, ou seja, redução da imagem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tfconv_p(img, kernels, stride_val = 1, pad_val = 'SAME', pool_val = 2):\n",
    "    reset_graph()\n",
    "\n",
    "    # batch, alt, larg, canais entrada\n",
    "    X = tf.constant(img, dtype=tf.float32) \n",
    "    feature_maps = tf.constant(kernels)\n",
    "    convolution = tf.nn.conv2d(X, feature_maps, \n",
    "                               strides=[1, stride_val, stride_val, 1], \n",
    "                               padding=pad_val)\n",
    "    # max pool!!!\n",
    "    max_pool = tf.nn.max_pool(convolution, \n",
    "                              ksize=[1, pool_val, pool_val, 1], \n",
    "                              strides=[1,pool_val,pool_val,1], \n",
    "                              padding=\"VALID\")\n",
    "    \n",
    "    with tf.Session() as s:\n",
    "        output = max_pool.eval(feed_dict={X: img})\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputmax = tfconv_p(images, fmap, pool_val = 4)\n",
    "plot_figs([output[0, :, :, 0], outputmax[0, :, :, 0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uma rede de convolução (CNN - Convolutional Neural Network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uma rede de convolução é basicamente uma rede neural com camadas em que se aplicam convoluções e pooling. Os pesos dos neurônios em camadas de convolução correspondem a kernels. Logo, os neurônios filtram as imagens que chegam em busca de padrões partculares. Os pesos nas camadas de pooling são sempre 1 e nunca se alteram. Ou seja, elas não fazem nada além de algum tipo de amostragem, reduzindo a quantidade de informação que vai seguir pela rede.\n",
    "\n",
    "Abaixo, vamos usar uma CNN para classificar MNIST.\n",
    "\n",
    "<img src=\"images/cnn0.png\" alt=\"CNN exemplo 0\" style=\"width: 600px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nossa CNN tem $28 \\times 28$ sinais de entrada. Estes sinais são entregues para um camada de convolução com 8 kernels $3 \\times 3$ e padding `valid`. As 8 imagens resultantes de $26 \\times 26$ pixels (redução devido ao padding não adicionar pixels) sao entrada de uma segunda rede de convolução, com 16 kernels, padding `same` e stride 2. Ou seja, ela reduz a entrada produzindo 16 imagens de $13 \\times 13$ pixels. Estas passam por uma camada de pooling que as reduz para 16 imagens de $7 \\times 7$ pixels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras import layers\n",
    "from keras import models\n",
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "img_input = layers.Input(shape = (28, 28, 1))\n",
    "x = layers.Conv2D(8, (3, 3), activation = 'relu')(img_input)\n",
    "x = layers.Conv2D(16, (3, 3), strides = (2, 2), padding = 'same', activation = 'relu')(x)\n",
    "x = layers.MaxPooling2D((2, 2), padding = 'same')(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Os $16 \\times 7 \\times 7$ sinais de saída são então dados como entrada para 64 neurônios (camada que chamamos FC1) que, por sua vez, se conectam com outros 10 neurônios. Cada neurônio deste, então, representa um digito diferente. Logo eles podem ser combinados via _softmax_ para determinar o dígito mais provável da rede."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = layers.Flatten()(x)\n",
    "x = layers.Dense(64, activation = 'relu', name = 'FC1')(x)\n",
    "yhat = layers.Dense(10, activation = 'softmax', name = 'output')(x)\n",
    "\n",
    "model = models.Model(inputs = img_input, outputs = yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"data/MNIST_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_images = mnist.train.images.reshape((55000, 28, 28, 1))\n",
    "test_images = mnist.test.images.reshape((10000, 28, 28, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_labels = to_categorical(mnist.train.labels)\n",
    "test_labels = to_categorical(mnist.test.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.fit(train_images, train_labels, epochs = 5, batch_size = 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = model.evaluate(test_images, test_labels, batch_size = 2000)\n",
    "test_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E a nossa primeira e simples CNN já atingiu uma alta taxa de classificação, da ordem de 98.2%. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Como seria o equivalente em tensorflow?\n",
    "\n",
    "```python\n",
    "#\n",
    "# modelo\n",
    "#\n",
    "\n",
    "height = 28\n",
    "width = 28\n",
    "channels = 1\n",
    "n_inputs = height * width\n",
    "\n",
    "reset_graph()\n",
    "\n",
    "with tf.name_scope(\"inputs\"):\n",
    "    X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "    X_reshaped = tf.reshape(X, shape=(-1, height, width, channels))\n",
    "    y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "\n",
    "# input: 28x28 image (batch images)\n",
    "conv1 = tf.layers.conv2d(X_reshaped, filters=8, kernel_size=3,\n",
    "                         strides=1, padding='SAME',\n",
    "                         activation=tf.nn.relu, name=\"conv1\")\n",
    "# input: 8 28x28\n",
    "conv2 = tf.layers.conv2d(conv1, filters=16, kernel_size=3,\n",
    "                         strides=2, padding='SAME',\n",
    "                         activation=tf.nn.relu, name=\"conv2\")\n",
    "\n",
    "with tf.name_scope(\"pool3\"):\n",
    "    # input: 16 14x14 \n",
    "    pool3 = tf.nn.max_pool(conv2, ksize=[1, 2, 2, 1], \n",
    "                           strides=[1, 2, 2, 1], padding=\"VALID\")\n",
    "    # output: 16 7x7 \n",
    "    pool3_flat = tf.reshape(pool3, shape=(-1, 16 * 7 * 7))\n",
    "\n",
    "with tf.name_scope(\"fc1\"):\n",
    "    # input: 16x7x7\n",
    "    fc1 = tf.layers.dense(pool3_flat, 64, activation=tf.nn.relu, name=\"fc1\")\n",
    "\n",
    "with tf.name_scope(\"output\"):\n",
    "    logits = tf.layers.dense(fc1, 10, name=\"output\")\n",
    "    Y_proba = tf.nn.softmax(logits, name=\"Y_proba\")\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=y)\n",
    "    loss = tf.reduce_mean(xentropy)\n",
    "    optimizer = tf.train.AdamOptimizer()\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "with tf.name_scope(\"init_and_save\"):\n",
    "    init = tf.global_variables_initializer()\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "#\n",
    "# treino & avaliacao\n",
    "#\n",
    "n_epochs = 5\n",
    "batch_size = 64\n",
    "\n",
    "with tf.Session() as s:\n",
    "    init.run()\n",
    "    for e in range(n_epochs):\n",
    "        for i in range(mnist.train.num_examples // batch_size):\n",
    "            X_b, y_b = mnist.train.next_batch(batch_size)\n",
    "            s.run(training_op, feed_dict={X: X_b, y: y_b})\n",
    "        acc_tr = accuracy.eval(feed_dict={X: X_b, y: y_b})\n",
    "        acc_test = accuracy.eval(feed_dict={X: mnist.test.images, \n",
    "                                            y: mnist.test.labels})\n",
    "        print '%d: acc tr = %.5f test = %.5f' % (e, acc_tr, acc_test)\n",
    "\n",
    "        saver.save(s, \"/tmp/my_mnist_model\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A seguir, vamos tentar melhorar nossa arquitetura introduzindo droput nas camadas de pooling e FC1, além de permitir que ela treine por mais épocas (o treino será finalizado usando _early stopping_)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# com dropout \n",
    "img_input = layers.Input(shape = (28, 28, 1))\n",
    "x = layers.Conv2D(8, (3, 3), activation = 'relu')(img_input)\n",
    "x = layers.Conv2D(16, (3, 3), strides = (2, 2), padding = 'same', activation = 'relu')(x)\n",
    "x = layers.MaxPooling2D((2, 2), padding = 'same')(x)\n",
    "x = layers.Flatten()(x)\n",
    "x = layers.Dropout(0.25)(x)\n",
    "x = layers.Dense(64, activation = 'relu', name = 'FC1')(x)\n",
    "x = layers.Dropout(0.50)(x)\n",
    "yhat = layers.Dense(10, activation = 'softmax', name = 'output')(x)\n",
    "\n",
    "model = models.Model(inputs = img_input, outputs = yhat)\n",
    "\n",
    "model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(train_images, train_labels, epochs = 10, batch_size = 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = model.evaluate(test_images, test_labels, batch_size = len(test_labels))\n",
    "test_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para implementar _early stopping_, vamos usar um _callback_ do Keras. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# com dropout \n",
    "img_input = layers.Input(shape = (28, 28, 1))\n",
    "x = layers.Conv2D(8, (3, 3), activation = 'relu')(img_input)\n",
    "x = layers.Conv2D(16, (3, 3), strides = (2, 2), padding = 'same', activation = 'relu')(x)\n",
    "x = layers.MaxPooling2D((2, 2), padding = 'same')(x)\n",
    "x = layers.Flatten()(x)\n",
    "x = layers.Dropout(0.25)(x)\n",
    "x = layers.Dense(64, activation = 'relu', name = 'FC1')(x)\n",
    "x = layers.Dropout(0.50)(x)\n",
    "yhat = layers.Dense(10, activation = 'softmax', name = 'output')(x)\n",
    "\n",
    "model = models.Model(inputs = img_input, outputs = yhat)\n",
    "\n",
    "model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "earlystopping = EarlyStopping(monitor='val_loss', patience = 2, min_delta = 0, verbose=0)\n",
    "\n",
    "model.fit(train_images, train_labels, epochs = 20, validation_split = 0.05,\n",
    "          batch_size = 64, callbacks = [earlystopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = model.evaluate(test_images, test_labels, batch_size = len(test_labels))\n",
    "test_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta rede, mesmo usando poucos kernels já é capaz de alcançar taxas de acerto superiores a 99%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como isso poderia ser feito em tensorflow?\n",
    "\n",
    "```python\n",
    "# save and restore net in memory\n",
    "def get_model_params():\n",
    "    gvars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)\n",
    "    return {gvar.op.name: value for gvar, value in zip(gvars, tf.get_default_session().run(gvars))}\n",
    "\n",
    "def restore_model_params(model_params):\n",
    "    gvar_names = list(model_params.keys())\n",
    "    assign_ops = {gvar_name: tf.get_default_graph().get_operation_by_name(gvar_name + \"/Assign\")\n",
    "                  for gvar_name in gvar_names}\n",
    "    init_values = {gvar_name: assign_op.inputs[1] for gvar_name, assign_op in assign_ops.items()}\n",
    "    feed_dict = {init_values[gvar_name]: model_params[gvar_name] for gvar_name in gvar_names}\n",
    "    tf.get_default_session().run(assign_ops, feed_dict=feed_dict)\n",
    "    \n",
    "# treino com early stopping\n",
    "n_epochs = 1000\n",
    "batch_size = 50\n",
    "\n",
    "best_loss_val = np.infty\n",
    "check_interval = 500\n",
    "no_progress = 0\n",
    "max_no_progress = 20\n",
    "best_model_params = None \n",
    "\n",
    "with tf.Session() as s:\n",
    "    init.run()\n",
    "    for e in range(n_epochs):\n",
    "        for i in range(mnist.train.num_examples // batch_size):\n",
    "            X_b, y_b = mnist.train.next_batch(batch_size)\n",
    "            s.run(training_op, feed_dict={X: X_b, y: y_b, training: True}) # dropout\n",
    "            if i % check_interval == 0:\n",
    "                loss_val = loss.eval(feed_dict={X: mnist.validation.images,\n",
    "                                                y: mnist.validation.labels})\n",
    "                if loss_val < best_loss_val:\n",
    "                    best_loss_val = loss_val\n",
    "                    no_progress = 0\n",
    "                    best_model_params = get_model_params()\n",
    "                else:\n",
    "                    no_progress += 1\n",
    "        acc_train = accuracy.eval(feed_dict={X: X_b, y: y_b})\n",
    "        acc_val = accuracy.eval(feed_dict={X: mnist.validation.images,\n",
    "                                           y: mnist.validation.labels})\n",
    "        print(\"%2d: train acc: %.5f, val acc: %.5f, curr best loss: %.5f\" % (\n",
    "                  e, acc_train, acc_val, best_loss_val))\n",
    "        if no_progress > max_no_progress:\n",
    "            print(\"Early stopping!\")\n",
    "            break\n",
    "\n",
    "    if best_model_params:\n",
    "        restore_model_params(best_model_params)\n",
    "    acc_test = accuracy.eval(feed_dict={X: mnist.test.images,\n",
    "                                        y: mnist.test.labels})\n",
    "    print \"acc test:\", acc_test\n",
    "    saver.save(s, \"/tmp/my_mnist_model\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observando pesos de camadas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uma forma de compreender melhor redes neurais é observando os pesos que foram aprendidos. No caso de processamento de imagens, onde os pesos representam os padrões visuais que os neurônios reconhecem, eles podem ser facilmente interpretáveis.\n",
    "\n",
    "Abaixo listamos os parâmetros observados no melhor modelo aprendido anteriormente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weights = model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wnames = []\n",
    "for l in model.layers:\n",
    "    for ws in l.weights:\n",
    "        wnames += [ws.name]\n",
    "wnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dweights = dict(zip(wnames, weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dweights['conv2d_1/kernel:0'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dweights['conv2d_1/kernel:0'][:,:,0,0].reshape(3,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_figs([dweights['conv2d_1/kernel:0'][:,:,0,i] for i in range(8)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note que como nesta rede usamos kernels de tamanho 3x3, não há padrões muito significativos a observar. Este não é o caso de redes onde são usados kernels maiores, em que padrões muito característicos são observados. Por exemplo, abaixo, podemos ver kernels de diferentes camadas na rede AlexNet (imagem de http://vision03.csail.mit.edu/cnn_art/index.html):\n",
    "\n",
    "![](images/alexnet_weights.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Claramente, camadas mais profundas apresentam padrões mais complexos. Assim, vemos bordas e manchas coloridas na primeira camada, texturas na terceira, partes de objetos na quinta e padrões complexos na oitava."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rede Neural como codificador distribuído"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outra característica interessante de redes neurais é que a saída dela para uma instância em qualquer camada, pode ser tomada como uma representação _comprimida_ e _distribuída_ daquela instância (um _embedding_). Como tais representações são baseadas nos mesmos conjuntos de características, espera-se que elas sejam similares para instâncias que compartilham as mesmas características. Logo, duas instâncias do número 0 devem ter representação similar, porém diferente do número 1, por exemplo. \n",
    "\n",
    "Para observarmos isso, vamos tomar a representação correspondente à camada totalmente conectada da nossa rede, a fc1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dweights['FC1/kernel:0'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_at_layer = Model(inputs=model.input, \n",
    "                       outputs=model.get_layer('FC1').output)\n",
    "Xs = mnist.test.images[:100].reshape(100,28,28,1)\n",
    "fc1_vals = model_at_layer.predict(Xs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note que fc1_vals tem as saídas dos neurônios da camada fc1 para as 100 primeiras imagens da coleção de teste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neigh = NearestNeighbors(2)\n",
    "neigh.fit(fc1_vals) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se tomarmos aleatoriamente uma imagem em fc1_vals e seus 4 vizinhos mais próximos, de acordo com a representação feita pelos neurônios, observe o que obtemos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rquery = random.randint(0, 99)\n",
    "indices = neigh.kneighbors([fc1_vals[rquery]], 4, return_distance=False)\n",
    "plot_figs([mnist.test.images[i].reshape(28,28) for i in indices[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Camadas Inception"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As primeiras redes de convolução, como a AlexNet, usavam grandes kernels (ex: 14x14). Enquanto isso é interessante para visualizarmos o que a rede aprendeu, por outro lado, implica em fazer muitas operações, a saber, número de mapas na entrada $\\times$ (altura da saída $\\times$ largura da saída) $\\times$ (tamanho do kernel)$^2$ $\\times$ número de mapas na saída. \n",
    "\n",
    "Por exemplo, se a camada de convolução vai processar uma entrada com 192 mapas de 14x14 para entregar imagens de 28x28 para uma camada seguinte que possui 32 mapas, ela vai consumir $14^2 * 28^2 * 192 * 32 = 944.111.616$ operações. Muito!\n",
    "\n",
    "Logo, parece mais aconselhável usar kernels menores. Mas que tamanho? 3x3? 5x5? Uma solução é usar ambos e, então, deixar para a camada da frente da rede neural decidir o que fazer. Esta ideia foi inspirada em uma arquitetura chamada NiN (2014) e deu origem à Inception v1.\n",
    "\n",
    "Embora esta nova abordagem permitisse à rede explorar padrões maiores e menores no mesmo campo de visão, ela continuava a requerer muitas operações. Em particular, $(5^2 * 28^2 * 192 * 32 + 3^2 * 28^2 * 192 * 32) = 163.774.464$. Embora bem menos que nas primeiras arquiteturas, ainda é muito. \n",
    "\n",
    "Uma solução proposta foi usar um número menor de kernels intermediários de tamanho 1x1, funcionando como combinadores lineares. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por que kernels 1x1 são melhores? Porque se usarmos um número menor de kernels 1x1 (ex: 16) entre os 192 e os 32 kernels usados antes, o número global de operações será menor. Ou seja, em lugar de 192 kernels 3x3 e 5x5 para gerar imagens de 28x28 para 32 kernels de saída, teremos agora 192 kernels 3x3 e 5x5 gerando imagens 28x28 para 16 kernels 1x1. Estes, por sua vez, serão entrada de 32 kernels de saída. O número de operações é, então:\n",
    "\n",
    "$(1^2 * 28^2 * 192 * 16 + 5^2 * 28^2 * 16 * 32) + (1^2 * 28^2 * 192 * 16 + 3^2 * 28^2 * 16 * 32) = 18.464.768$\n",
    "\n",
    "Muito menos!!!\n",
    "\n",
    "Redes de convolução Inception v2 basicamente vão incorporar essa ideia. Uma camada inception, em particular, é composta por 4 operações paralelas: (1) convolução 1x1, (2) convolução 1x1 em série com 3x3, (3) convolução 1x1 em série com 5x5 e (4) uma maxpooling (pq? bom, pq todo mundo usa maxpooling) em série com convolução 1x1. Veja abaixo:\n",
    "\n",
    "![](images/inception_implement.png)\n",
    "\n",
    "Ao reduzir o número de operações & parâmetros, foi possível conseguir redes mais profundas. Com redes mais profundas, contudo, o problema de perda de gradientes se agravou, o que levou à ideia de fazer supervisão em várias camadas intermediárias da rede (em lugar de apenas no fim). Gradientes intermediários são então somados aos gradientes globais da rede durante a propagação retrógrada de forma a minimizar o problema de perda de gradientes.\n",
    "\n",
    "Abaixo você pode ver 3 camadas softmax na rede GoogLeNet representadas por blocos laranja, além de nove camadas inception (fonte: http://joelouismarino.github.io/images/blog_images/blog_googlenet_keras/googlenet_diagram.png).  \n",
    "\n",
    "![](images/googlenet_diagram.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A evolução das redes Inception seguiu com a introdução de BN (ainda na Inception v2) e uso de kernels de no máximo tamanho 3 (kernels 3x3). Para cobrir áreas tão amplas quanto a dos kernels 5x5 e 7x7 adotou-se o empilhamento de  kernels 3x3 e/ou séries de kernels 1x$n$ seguidos de $n$x1 ($n = 7$ em Inception v3). \n",
    "\n",
    "![](images/inceptionv3layer.png)\n",
    "\n",
    "A arquitetura baseada nesta nova camada é a Inception v3 (ver https://arxiv.org/pdf/1512.00567.pdf para uma descrição de todas elas):\n",
    "\n",
    "![](images/inceptionv3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arquiteturas Inception seguintes (v4) incluem camadas residuais, discutidas a seguir."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Redes de Convolução Residuais"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe as operações a seguir:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "k = np.array([[0,0,0,1],\n",
    "              [0,0,1,1],\n",
    "              [0,0,1,0],\n",
    "              [0,1,0,0]], dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fmap = k.reshape(k.shape[0], k.shape[1], 1, 1)\n",
    "input0 = ex0.reshape(1, ex0.shape[0], ex0.shape[1], 1)\n",
    "output0 = tfconv(input0, fmap)\n",
    "plot_figs([ex0, k])\n",
    "plot_figs([output0[0, :, :, 0], ex0 + output0[0, :, :, 0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como antes, o kernel 3x3 foi aplicado à imagem ex0, produzindo uma interpretação onde o kernel parece observado na imagem. A quarta imagem corresponde à soma da entrada com a saída -- dizemos que a entrada foi 'copiada' para a saída. Ou seja, temos uma mistura entre onde o kernel foi observado e sua entrada original :/. Como interpretar isso? \n",
    "\n",
    "Note que uma pilha de camadas de uma rede neural aprende uma representação/mapeamento $\\mathcal{F}({\\bf x})$ para a sua entrada ${\\bf x}$. Ao somarmos ${\\bf x}$ à saída $\\mathcal{F}$ da camada, o código aprendido passa a ser $\\mathcal{H}({\\bf x}) = \\mathcal{F}({\\bf x}) + {\\bf x}$, ou seja, a rede aprende o resíduo da representaçao $\\mathcal{F}({\\bf x}) = \\mathcal{H}({\\bf x}) - {\\bf x}$.  \n",
    "\n",
    "![](images/resnet_block.png)\n",
    "\n",
    "Mas por que isso seria útil? Em termos teóricos, se um mapeamento existente ${\\bf x}$ já é ótimo, é mais simples uma pilha de camadas aprender um resíduo $\\mathcal{F}({\\bf x})$ próximo a zero que o mapeamento da função  identidade através de uma pilha de camadas não lineares. Generalizando esta ideia, podemos dizer que é mais simples otimizar resíduos.  \n",
    "\n",
    "Em termos práticos, os neurônios que ainda não aprenderam nada útil e estão disparando próximo a zero, agora disparam uma cópia do valor da sua entrada (ou de uma entrada anterior, dependendo de onde é feita a 'cópia'). Assim, eles não bloqueiam mais o fluxo do sinal pela rede através das camadas, o que nos permite usar mais camadas!!! O resultado é a criação de redes com centenas de camadas, sem a necessidade de treinamento intermediário. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Redes de Convolução Densas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tanto redes inception quanto residuais sugerem que camadas podem tirar proveito de atributos menos abstratos (obtidos via kernels menores em Inception ou de camadas anteriores em ResNets). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Várias arquiteturas exploraram esta ideia de que atributos aprendidos em camadas anteriores podem ser úteis à frente, como as redes Fractais (2016). A ideia, contudo, foi generalizada pelas redes de convolução densamente conectadas (https://arxiv.org/pdf/1608.06993.pdf). Nestas, qualquer camada tem acesso a todos os atributos aprendidos em todas as camadas anteriores que seguem uma camada de pooling (uma vez que camadas de pooling mudam o tamanho do atributo -- kernel, o que impediria a sua utilização).\n",
    "\n",
    "![](images/dense_convolve.jpeg)\n",
    "\n",
    "O estudo destas arquiteturas demonstra que camadas posteriores tem alta probabilidade de usar atributos de camadas anteriores próximas. Isso é mais relevante quanto mais perto do início da rede está a camada, como visto na figura a seguir do paper sobre _densely connected convolution nets_ (onde cores indicam a influência das camadas). \n",
    "\n",
    "![](images/weight_impact.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
