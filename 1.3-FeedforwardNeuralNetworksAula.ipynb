{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rede Neural Feedforward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nesta aula, vamos usar a coleção MNIST. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-3-37872fb765e6>:1: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From E:\\Anaconda\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From E:\\Anaconda\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting data/MNIST_data\\train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From E:\\Anaconda\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting data/MNIST_data\\train-labels-idx1-ubyte.gz\n",
      "Extracting data/MNIST_data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST_data\\t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From E:\\Anaconda\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets('data/MNIST_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55000\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "print(mnist.train.num_examples)\n",
    "print(mnist.test.num_examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta é uma coleção com 55 mil casos de treino e 10 mil casos de teste. Cada instância é a imagem de 28x28 pixels de um número de 0 a 9 (em escala de cinza). Nosso objetivo é reconhecer o número. O atual estado-da-arte para este problema tem acurácia de 99.79 (Novembro de 2016, http://paper.researchbib.com/view/paper/104069). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQQAAAECCAYAAAAYUakXAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x29776150b00>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import random\n",
    "idx = random.randint(0, mnist.train.num_examples)\n",
    "plt.matshow(mnist.train.images[idx].reshape((28,28)), cmap = 'gray')\n",
    "print(mnist.train.labels[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Activation(object):\n",
    "    \"\"\"Funcao de ativacao\"\"\"\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        \n",
    "    def init(self, n_inputs, n_outputs):\n",
    "        return tf.random_uniform([n_inputs, n_outputs], -1.0, 1.0)\n",
    "        \n",
    "    def fire(self, ypred):\n",
    "        if self.name == 'sigmoid':\n",
    "            return tf.nn.sigmoid(ypred) \n",
    "        else:\n",
    "            return ypred\n",
    "            \n",
    "class Layer(object):\n",
    "    \"\"\"Camada de rede neural sequencial\"\"\"\n",
    "    def __init__(self, units, activation = None, name = None):\n",
    "        self.units = units\n",
    "        self.name = name \n",
    "        self.activation = activation if activation != None else Activation('')\n",
    "        \n",
    "    def output(self, X):\n",
    "        n_inputs = int(X.get_shape()[1])\n",
    "        with tf.name_scope(self.name):\n",
    "            self.W = tf.Variable(self.activation.init(n_inputs, self.units), name = 'W')\n",
    "            self.b = tf.Variable(tf.zeros([self.units]), name = 'b')\n",
    "            ypred = self.activation.fire(tf.matmul(X, self.W) + self.b)\n",
    "        return ypred\n",
    "\n",
    "class LossFunction(object):\n",
    "    def __init__(self, name = 'sigmoid'):\n",
    "        self.name = name\n",
    "\n",
    "    def get(self, yreal, ypred):\n",
    "        if self.name == 'sigmoid':\n",
    "            loss = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "                labels = yreal, logits = ypred) \n",
    "        return tf.reduce_mean(loss, name = 'lossf')\n",
    "    \n",
    "class Optimizer(object):\n",
    "    def __init__(self, name = 'sgd'):\n",
    "        self.name = name\n",
    "        self.lrate = 0.1\n",
    "\n",
    "    def get(self, lossf):\n",
    "        if self.name == 'sgd':\n",
    "            opt = tf.train.GradientDescentOptimizer(learning_rate = self.lrate) \n",
    "        return opt.minimize(lossf)\n",
    "    \n",
    "class FeedforwardNeuralNet(object):\n",
    "    \"\"\"Rede neural sequencial\"\"\"\n",
    "    def __init__(self, input_dim):\n",
    "        self.input_dim = input_dim\n",
    "        self.learning_rate = 0.1\n",
    "        self.layers = []\n",
    "        \n",
    "    def add(self, units, activation = None, name = None):\n",
    "        \"\"\"Adiciona camadas para rede neural\"\"\"\n",
    "        self.layers += [Layer(units, activation, name)]\n",
    "    \n",
    "    def compile(self, loss = 'sigmoid', optimizer = 'sgd'):\n",
    "        \"\"\"Cria grafo da rede neural\"\"\"\n",
    "        self.X = tf.placeholder(tf.float32, \n",
    "                           shape = (None, self.input_dim), \n",
    "                           name = 'X')\n",
    "        self.y = tf.placeholder(tf.int64, shape = (None), name = 'y')\n",
    "        \n",
    "        # cria layers\n",
    "        with tf.name_scope('layers'):\n",
    "            layer_in = self.X\n",
    "            for layer in self.layers:\n",
    "                layer_out = layer.output(layer_in)\n",
    "                layer_in = layer_out\n",
    "                    \n",
    "        # loss function\n",
    "        with tf.name_scope('loss'):\n",
    "            self.lossf = LossFunction(loss).get(self.y, layer_out)\n",
    "    \n",
    "        # optimizer\n",
    "        with tf.name_scope('train'):\n",
    "            self.train_op = Optimizer(optimizer).get(self.lossf)\n",
    "            \n",
    "        # evalution metrics\n",
    "        with tf.name_scope('eval'):\n",
    "            correct = tf.nn.in_top_k(layer_out, self.y, 1)\n",
    "            self.acc = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "            \n",
    "        self.init_op = tf.global_variables_initializer()\n",
    "        self.saver = tf.train.Saver()\n",
    "    \n",
    "    def fit(self, train_data, n_epochs, batch_size):\n",
    "        \"\"\"Executa treino da rede neural\"\"\"\n",
    "        num_batches = train_data.num_examples // batch_size\n",
    "        with tf.Session() as s:\n",
    "            s.run(self.init_op)\n",
    "            for e in range(n_epochs):\n",
    "                tloss = 0.\n",
    "                for i in range(num_batches):\n",
    "                    X_b, y_b = train_data.next_batch(batch_size)\n",
    "                    _, loss_e = s.run([self.train_op, self.lossf], \n",
    "                                      feed_dict = {self.X: X_b, self.y: y_b})\n",
    "                    tloss += loss_e\n",
    "                acc_train = s.run(self.acc, \n",
    "                                  feed_dict = {self.X: X_b, self.y: y_b})\n",
    "                print('%2d loss: %.8f acc: %.2f' % (e, tloss/num_batches, acc_train))\n",
    "            self.saver.save(s, '/tmp/model.ckpt') \n",
    "    \n",
    "    def evaluate(self, X_test, y_test):\n",
    "        \"\"\"Avalia rede neural em colecao de teste\"\"\"\n",
    "        with tf.Session() as s:\n",
    "            self.saver.restore(s, '/tmp/model.ckpt')\n",
    "            acc_test = s.run(self.acc, \n",
    "                             feed_dict = {self.X: X_test, self.y: y_test})\n",
    "        return acc_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usando nossa rede neural"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "model = FeedforwardNeuralNet(input_dim = 28*28)\n",
    "\n",
    "model.add(units=300, activation = Activation('sigmoid'), name = 'h1')\n",
    "model.add(units=100, activation = Activation('sigmoid'), name = 'h2')\n",
    "model.add(units=10, name = 'out')\n",
    "\n",
    "model.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0 loss: 0.76269900 acc: 0.86\n",
      " 1 loss: 0.37637324 acc: 0.94\n",
      " 2 loss: 0.30266735 acc: 0.90\n",
      " 3 loss: 0.26168132 acc: 0.96\n",
      " 4 loss: 0.23361137 acc: 0.98\n",
      " 5 loss: 0.21224959 acc: 0.98\n",
      " 6 loss: 0.19536860 acc: 0.90\n",
      " 7 loss: 0.18075725 acc: 0.94\n",
      " 8 loss: 0.16841592 acc: 0.96\n",
      " 9 loss: 0.15768284 acc: 0.92\n"
     ]
    }
   ],
   "source": [
    "model.fit(mnist.train, n_epochs = 10, batch_size = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/model.ckpt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9416"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(mnist.test.images, mnist.test.labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hiper parâmetros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Número de camadas: dependendo da complexidade do problema, mais é melhor. Em geral, se ganha mais com mais camadas que com mais neurônios. Além disso, elas permitem a transferência de aprendizado, já que você não precisa treinar a rede toda. Pode sempre aproveitar camadas anteriores pré-treinadas. Uma estratégia comum é usar muitas camadas e checar _overfitting_. Se ele ocorre, começamos a fazer regularização."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* _Número de neurônios por camada_: quanto mais melhor. Contudo, dado a natureza hierárquica do aprendizado, é comum uma estrutura geral de funil, pois é mais provável que usemos mais atributos de baixo nível que de alto nível. Uma estratégia comum é usar muitos neurônios e verificar _overfitting_. Se ele ocorre, então regularizamos para diminui-lo. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* _Funções de ativação_: vamos usar quase sempre ReLU e suas variantes. Em classificação, a última camada é normalmente _softmax_. Em regressão, nenhuma ativação é usada na última camada. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treinando FNNs profundas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vanishing and Exploding Gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Métodos dependem de gradientes. Contudo, gradientes podem desaparecer ou explodir, inviabilizando o aprendizado. Uma das razões para saturação são o uso combinado de funções de ativação que saturam (como sigmoids e variantes) e métodos de normalização de média 0, variância 1. \n",
    "\n",
    "O efeito dessa combinação é que a variância da saída de cada camada é bem mais alta que da entrada. Logo, após várias camadas, esta variância leva a ocorrência de valores críticos.\n",
    "\n",
    "Uma solução para este problema é o uso de diferentes funções de ativação (ReLU e família) e diferentes técnicas de inicialização (Xavier e He)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inicialização de Xavier (ou Glorot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variância da saída deveria ser igual da entrada; o mesmo vale para variância dos gradientes antes e depois de passar por uma camada na volta. Embora não seja possível garantir isso, um bom compromisso é alcançado se os pesos forem inicializados assim:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* normalização média 0, variância $\\sigma$: $\\sigma = \\alpha \\sqrt{\\frac{2}{n_{entradas}+n_{saidas}}}$\n",
    "* normalização uniforme entre $-r$ e $+r$: $r = \\alpha \\sqrt{\\frac{6}{n_{entradas}+n_{saidas}}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Onde $\\alpha$ é 1 para ativação sigmoid, 4 para tangente hiperbólica e $\\sqrt{2}$ para ReLU. Quando o número de entradas e saídas é basicamente o mesmo, vc pode usar:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* normalização média 0, variância $\\sigma$: $\\sigma = \\alpha \\frac{1}{\\sqrt{n_{entradas}}}$\n",
    "* normalização uniforme entre $-r$ e $+r$: $r = \\alpha \\frac{\\sqrt{3}}{\\sqrt{n_{entradas}}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Activation(object):\n",
    "    \"\"\"Funcao de ativacao\"\"\"\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "                \n",
    "    def init(self, n_inputs, n_outputs):\n",
    "        \"\"\"Xavier initialization\"\"\"\n",
    "        if self.name == 'sigmoid':\n",
    "            stddev = np.sqrt(2. / (n_inputs + n_outputs))\n",
    "        else:\n",
    "            stddev = 2. / np.sqrt(n_inputs + n_outputs)\n",
    "        return tf.truncated_normal((n_inputs, n_outputs), stddev = stddev)\n",
    "        \n",
    "    def fire(self, ypred):\n",
    "        if self.name == 'sigmoid':\n",
    "            return tf.nn.sigmoid(ypred) \n",
    "        elif self.name == 'relu':\n",
    "            return tf.nn.relu(ypred)\n",
    "        else:\n",
    "            return ypred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "model = FeedforwardNeuralNet(input_dim = 28*28)\n",
    "\n",
    "model.add(units=300, activation = Activation('sigmoid'), name = 'h1')\n",
    "model.add(units=100, activation = Activation('sigmoid'), name = 'h2')\n",
    "model.add(units=10, name = 'out')\n",
    "\n",
    "model.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0 loss: 1.05048586 acc: 0.80\n",
      " 1 loss: 0.39163126 acc: 0.84\n",
      " 2 loss: 0.32634528 acc: 0.96\n",
      " 3 loss: 0.29699917 acc: 0.90\n",
      " 4 loss: 0.27530011 acc: 0.90\n",
      " 5 loss: 0.25641846 acc: 0.88\n",
      " 6 loss: 0.24033329 acc: 0.96\n",
      " 7 loss: 0.22454761 acc: 0.92\n",
      " 8 loss: 0.21074877 acc: 0.96\n",
      " 9 loss: 0.19779807 acc: 0.98\n"
     ]
    }
   ],
   "source": [
    "model.fit(mnist.train, n_epochs = 10, batch_size = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/model.ckpt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9441"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(mnist.test.images, mnist.test.labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Função de ativação não saturáveis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-5, 5, -0.2, 1.2]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX8AAAELCAYAAAAx94awAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x297003b9a90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "z = np.linspace(-5, 5, 200)\n",
    "sz = sigmoid(z)\n",
    "dsz = np.gradient(sz, z)\n",
    "\n",
    "plt.plot([-5, 5], [0, 0], 'k--')\n",
    "plt.plot([-5, 5], [1, 1], 'r--')\n",
    "plt.plot([0, 0], [-0.2, 1.2], 'k--')\n",
    "plt.plot(z, sz, \"b-\", linewidth=2)\n",
    "plt.plot(z, dsz, \"g-\", linewidth=2)\n",
    "props = dict(facecolor='black', shrink=0.1)\n",
    "plt.annotate('Saturacao', xytext=(3.5, 0.7), xy=(5, 1), arrowprops=props, fontsize=14, ha=\"center\")\n",
    "plt.annotate('Saturacao', xytext=(-3.5, 0.3), xy=(-5, 0), arrowprops=props, fontsize=14, ha=\"center\")\n",
    "plt.annotate(r\"$\\sigma'$\", xytext=(2, 0.2), xy=(0, 0.5), fontsize=20, ha=\"center\")\n",
    "plt.title(r\"Funcao de ativacao Sigmoid $\\sigma$\", fontsize=14)\n",
    "plt.axis([-5, 5, -0.2, 1.2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O grande problema de sigmoids (incluindo a tangente hiperbólica) é a existência de regiões de saturação. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para lidar com este problema, foram introduzidas as Rectifier Linear Units -- ReLUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def relu(z):\n",
    "    return np.maximum(0, z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAEJCAYAAAC0U81tAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x297002aba58>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rz = relu(z)\n",
    "drz = np.gradient(rz, z)\n",
    "\n",
    "plt.plot(z, rz, \"b-\", linewidth=2)\n",
    "plt.plot(z, drz, \"g-\", linewidth=2)\n",
    "plt.plot([-5, 5], [0, 0], 'k--')\n",
    "plt.plot([0, 0], [-0.5, 4.2], 'k--')\n",
    "props = dict(facecolor='black', shrink=0.1)\n",
    "plt.title(\"Funcao de ativacao ReLU\", fontsize=14)\n",
    "plt.axis([-5, 5, -0.5, 4.2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "model = FeedforwardNeuralNet(input_dim = 28*28)\n",
    "\n",
    "model.add(units=300, activation = Activation('relu'), name = 'h1')\n",
    "model.add(units=100, activation = Activation('relu'), name = 'h2')\n",
    "model.add(units=10, name = 'out')\n",
    "\n",
    "model.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0 loss: 0.28673112 acc: 1.00\n",
      " 1 loss: 0.12325674 acc: 1.00\n",
      " 2 loss: 0.08538376 acc: 1.00\n",
      " 3 loss: 0.06277581 acc: 1.00\n",
      " 4 loss: 0.04825924 acc: 1.00\n",
      " 5 loss: 0.03707605 acc: 1.00\n",
      " 6 loss: 0.02926872 acc: 1.00\n",
      " 7 loss: 0.02138758 acc: 1.00\n",
      " 8 loss: 0.01664940 acc: 1.00\n",
      " 9 loss: 0.01198216 acc: 1.00\n"
     ]
    }
   ],
   "source": [
    "model.fit(mnist.train, n_epochs = 10, batch_size = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.1+0.2-0.3 ==0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/model.ckpt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9788"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(mnist.test.images, mnist.test.labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um problema com ReLUs é que elas podem ainda saturar em 0. Assim, novas variantes foram criadas como Leaky ReLU, ELU (Exponential Linear Unit) e SELU (Self-normlizing Exponential Linear Unit).\n",
    "\n",
    "A ideia da leakyReLU e eLU é que não haja saturação em 0. A leakyReLU usa uma estratégia linear e a eLU, uma estratégia exponencial. Como a eLU sempre é melhor que a leakyReLU, vamos mostrar o seu efeito a seguir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def elu(z, alpha=1):\n",
    "    return np.where(z < 0, alpha * (np.exp(z) - 1), z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXYAAAELCAYAAADN4q16AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAIABJREFUeJzt3Xl8VNX9//HXJ2EHlU1RIbIoUFyp\nUiqolQouIIiKwldFS62CotZYqC0Uf9+2ILaKNlYFoaipQBUKioCgsshXK7iABi0qi6iAsoVN9mzn\n98e5gZCFBGYyN5l5P32MuTPnzr2fuUw+OXPuZ8415xwiIhI/ksIOQEREokuJXUQkziixi4jEGSV2\nEZE4o8QuIhJnlNhFROKMEruISJxRYhcRiTNK7BJ1ZjbLzNJD2ne6mc0KY9/xxMzqmdkmMzs97FiK\nY2ZTzew3YcdRUSmxx0CQbFwxt7Zhx1ZZmdlCM3u6mKb7gb6xjifajvCeeb/QOsX+ESvp+JhZPzPb\nXYYQhgKznXNfHfurODZm9jMzm2Fm3wWvuV8xq/0JGGZmJ8Q4vEpBiT125gGnFLr9N9SI4pBzbqdz\nbkfYcURJce+ZbuW9UzOrBdwBPFfe+ypBHfzvxv3AvuJWcM59BqwhDv6Ilwcl9tg54JzbWOiWA8X3\nrgr2xoL20WY20swyzWyzmY0ys6QC65uZDTKzVWZ2wMzWm9kjBdqvMrN3zWy7mW0zszfNrE2hfVY3\ns7TgI/h+M3vfzC4+0osys1pBrLuD5w0t1G5m9qCZfWVm+8zsMzMr9ZfxSPEGwzyXAvcU6Mk2K3jc\nzGxAEE+VQtv9l5m9VtbjEtZxDRT3ntlWhudFqhuQB7xX8EEzu8DM5gf/jquDnnVvM3uv+M0cG+fc\nbOfcUOfc1CCOkswAbormvuOFEnvlcQuQA3QE7gVSgT4F2kcCDwGPAGcBNwLrCrTXBtKA9kAnYCcw\n08yqFVjn0WCbtwM/Bj4D3jCzU44Q1yjgcqAX0Dl43s8KtI8AfgXcA5wZxDfWzK4u5fUeKd77gcXA\nCxzqya4r9PwpQF2gS/4DZlYb6AlMLON+ILzjGqZLgKWuwAyBZvYT4F3gbeBc4H38cMgf8MfnMGY2\nNPhjf6TbJRHG+SHQ3sxqRrid+OOc062cb0A6PinvLnCbU6B9IfB0Mc+ZVaB9caH2ucD4YLkOsB+4\n6yhiqg3kAhcXuJ8F3FZgnWTgK2BECduoAxwAbin02I4g/tr4j9KXFHpeGn789miOYeF4ixyzYo7b\nq8CEAm198Ym3Rln2E9ZxPcJ7Zjfw1+JeazHPL+n49AN2l/IapgP/LPTYO8DkAve7Ba/z7RK2UR84\no5RbzTIcz91AvxLazgUccPrRvJcS4XbYx1QpV+8A/QvcL3bs8Ag+LXT/e+CkYPlMoDowv6Qnm69u\nGA78FDgR/2ktCTgtWOV0oCoFPn4753LNbHGw/eKcDlTD957zn7PbzD4rEFcNfO+04PzQVYFvSoq1\njPGWxUQg3cxqOef24j/1THXO7S/jfrII57jmK/yeAf9Hs7zVBDbl3zGzk/G9+J8XWCcL/zqL9NYB\nnB8yKu9ho/zfIfXYC1Fij529zrnVJbTlAVbosaqF7mcXuu84NJRW+LnFmQl8BwwIfuYAn+MTc8Ft\nFDdBf0mT9pe23/z4egBrC7UVfj2FlRZvWcwKntfTzObjh2WuOIr9hHVc8x3pPVOaH4DiKkbq4j+1\nHEkmUK/A/fxzBh8VeKw1sMI595/iNhCcaxlaXFsBXZ1z75ayzpHUD35uiWAbcUmJvWLYgh8nLug8\nSunVFvA5fkikM7CqcKOZNcD/ct7jnHs7eOx8Dv/3X43vhV2MrzbAzJKBDsC/StjvanyCvrDAc2oD\nZ+OHGvLjauqcW1DG11LWeLPwQxolcs4dMLOp+J56Q2Aj8H9HsZ+wjms0rAC6mZm5YNwicH7QdiSf\n4Ids8tXF/xHKAzCz4/Bj6xuPsI1n8ec5juS7UtpLczbwvXNuU6lrJhgl9ophAZBmZtfgf+kGACmU\nMbE753aZ2ZPAI2Z2AP8RvgFwgXNuDLAd3wu708zWAY2Bx/C9y/xt7DGzMcBfzCwT+Bp4AGgEjC5h\nv7vN7Dngr2a2BT889P8IEm4Q1yhglJlZEFcd/B+CPOfcuBJeUqnxBsemfVANsxvY5pwrroJiIr5s\nsDnwr0LrHHE/YR3XAqoHwyAF5TrnCvZQj7ei34fYAYzBn2R/ysz+gT9X0A1fRdKzlP2+if83beCc\n2wpk4D95DDGzScFr3ACcYWYtnXNF/uhFMhRjZnXwY/AQDGsFr3Gbc67gJ79LgDeOZR9xL+xB/kS4\ncYSTXEF7VeAZfJLIBP5M0ZOnJZ5cDe4nAb/H9wqz8JUbDxdovwxfG7w/+HklhU5M4ceT0/Djqwfw\nlQ8Xl/LaagMvBtvajB9znQWkB+0G3Meh3u8W/Infy0vZ7hHjBVrhx/b34nuTzUo4Lob/I+CAc45h\nP2Ed1/Qg5sK39WVYZ2rQ/hN8kt6EH375ALi2jO/ZxfhPIvn3hwb/dvuBSfihmveALeXw+9KphNeV\nXmCdGsFrujDs3++KeLPgIImIHGRmVwFPAmc653LDjqcwM7sH6OmcK3zORFAdu4gUwzn3Bv5TZJOw\nYylBNv6ToBRDPXYRkTijHruISJxRYhcRiTOhlDs2bNjQNWvWLIxdi5RoxQpf3t26deuQI5EjycmB\nL7+EAwegXj1o0SLsiGJn6dKlmc65E0tbL5TE3qxZM5YsWRLGrkVK1KlTJwAWLlwYahxSsqwsuPJK\nn9TPPx/efRdq1Qo7qtgxs2/Lsp6GYkSkUnAO7rsPFi6EU06B115LrKR+NPTNU5HAsGHDwg5BjuCp\np2DcOKhRA6ZPhyYVtRCzAlBiFwl06dKl9JUkFG++CQ884Jeffx7atw83noou4qEYM6thZh+a2TIz\nW25mf4pGYCKxlpGRQUZGRthhSCFffgl9+kBeHgwbBjfpmkmlikaP/QBwmfMTQlUF/mNmc5xz75f2\nRJGKJDU1FdDJ04pk2zbo0QN27oTrr4c/qdtYJhEndue/upp/1fOqwU1fZxWRiGRnw403wurV0LYt\nvPgiJKnco0yicpjMLNnMMvCz+811zn1QzDr9zWyJmS3ZskXz4ovIkd1/PyxYAI0awYwZULt22BFV\nHlFJ7M65XOdcW/yEQe3N7Oxi1hnnnGvnnGt34oml1teLSAJ75hkYMwaqV/cVMCkpYUdUuUT1g41z\nbgd+7vCrorldEUkcc+f63jrAc8/BhReGG09lFPEYu5mdCGQ753aYWU38dSX/GnFkIjE2cuTIsENI\neCtXQu/ekJsLQ4bALbeEHVHlFI2qmFOAfwbXcUwCpjjnZkVhuyIx1bFjx7BDSGjbt/sKmB074Npr\nYcSIsCOqvKJRFfMp8OMoxCISqkWLFgFK8GHIyfE99ZUr4dxzYcIEVcBEQt88FQkMHToUUB17GB54\nAObNg5NO8hUwdeqEHVHlpr+JIhKqZ5+Fp5+GatXg1VehadOwI6r8lNhFJDQLFsC99/rlf/wDNAoW\nHUrsIhKKVavghht8BcyDD8Jtt4UdUfxQYheRmNuxw1fA5FfCqNI0unTyVCSQlpYWdggJISfHz9a4\nYgWccw5MmgTJyWFHFV+U2EUCbdu2DTuEhDB4MLz1FjRs6Ctgjjsu7Ijij4ZiRALz5s1j3rx5YYcR\n1/7xD3jySaha1VfA6Jr25UM9dpHAiOCrjrqSUvlYuBAGDvTLY8fCxReHGk5cU49dRMrdV19Br15+\nfH3QIPjlL8OOKL4psYtIudq501e+bNsGV18Nf9UUgeVOiV1Eyk1urr9G6RdfwFlnwb/+pQqYWFBi\nF5Fy89vfwpw50KCBr4A5/viwI0oMOnkqEhg7dmzYIcSV556Dv/0NqlSBV16BFi3CjihxKLGLBFq3\nbh12CHHjnXfg7rv98pgx8LOfhRtPotFQjEhg5syZzJw5M+wwKr2vv/YVMNnZkJoKd9wRdkSJRz12\nkcDjjz8OQI8ePUKOpPL64QdfAZOZCVddBY89FnZEiUk9dhGJitxcuPlmWL4c2rSBl1/24+sSe0rs\nIhIVv/89vP461K8PM2fCCSeEHVHiUmIXkYilp8OoUb6HPm0anH562BElNiV2EYnIe+/BgAF++Zln\noFOnUMMRdPJU5KAJEyaEHUKl8803cN11kJUF990H/fuHHZGAErvIQSkpKWGHUKns2gXXXANbtsAV\nV8ATT4QdkeTTUIxIYPLkyUyePDnsMCqFvDzo2xc++wxat4bJk1UBU5Hon0IkMGbMGAD69OkTciQV\n39Chfu6XevV8BUzdumFHJAWpxy4iR+XFF/3Uu8nJMHUqtGwZdkRSmBK7iJTZ4sVw551++amn4LLL\nwo1HiqfELiJlsnYtXHutr4C5555Dk3xJxaPELiKl2r3bV8Bs3gydO/vpeKXi0slTkcDUqVPDDqFC\nysuDW2+FZcv8ePq//w1Vq4YdlRyJErtIoGHDhmGHUCE99BBMn+4rX2bO9JUwUrFFPBRjZilm9raZ\nfWFmy83s/mgEJhJr6enppKenhx1GhTJpEowc6StgpkzxNetS8UWjx54DDHLOfWxmxwFLzWyuc+7z\nKGxbJGbyk3q/fv1CjaOi+OAD+NWv/HJaGlx+ebjxSNlF3GN3zm1wzn0cLO8CvgAaR7pdEQnPunXQ\nsyccOAB33eWrYKTyiGpVjJk1A34MfBDN7YpI7OzZ45P6pk3w85/D3/8OZmFHJUcjaondzOoA04BU\n59wPxbT3N7MlZrZky5Yt0dqtiERRXh784hfwySd+TnVVwFROUUnsZlYVn9QnOedeKW4d59w451w7\n51y7E088MRq7FZEo++Mf/YUyjj/eV8A0aBB2RHIsIj55amYGPAd84ZzTxJ1Sac2ePTvsEEL18ssw\nfDgkJfnZGtu0CTsiOVbR6LFfBNwKXGZmGcGtWxS2KxJTtWrVolatWmGHEYoPP4Rf/tIvP/EEXHVV\nuPFIZCLusTvn/gPo1IpUeqNHjwZg4MCBIUcSW9995+eA2b/fT/D161+HHZFESnPFiASmTJnClClT\nwg4jpvbu9RUwGzbApZfC00+rAiYeKLGLJCjn/PDL0qXQooWfW71atbCjkmhQYhdJUH/+s58m4Ljj\n/NWQNFVO/FBiF0lA//63L21MSvLVMGedFXZEEk1K7CIJZulS/yUkgMceg26qYYs7mrZXJLBw4cKw\nQyh333/vL5ixbx/cfjs88EDYEUl5UI9dJEHs2+fLGr//Hi65BMaMUQVMvFJiFwmMGjWKUaNGhR1G\nuXDO99A/+giaNfPTBqgCJn4psYsEZs2axaxZs8IOo1w8/LA/SVqnjp8DRtM1xTcldpE4N22av7yd\nGbz0Epx9dtgRSXlTYheJY598Arfd5pf/+lfo3j3ceCQ2lNhF4tSGDb4CZu9eX944eHDYEUmsqNxR\nJFCzZs2wQ4ia/fvhuutg/Xq46CIYO1YVMIlEiV0kMGfOnLBDiArn4I47/MWomzaFV16B6tXDjkpi\nSUMxInHmL3+BSZOgdm0/B8xJJ4UdkcSaErtIYPjw4QwfPjzsMCIyfToMHeqHXSZNgnPPDTsiCYMS\nu0hg/vz5zJ8/P+wwjtmyZdC3r18eOdLPsy6JSYldJA5s2gQ9esCePXDrrfC734UdkYRJiV2kksuv\ngFm3Di68EMaNUwVMolNiF6nEnIP+/WHxYkhJ8WPsNWqEHZWETeWOIoEGDRqEHcJRe+wxmDABatXy\nFTCNGoUdkVQESuwigWnTpoUdwlGZMQN+/3u/PHEitG0bbjxScWgoRqQS+uwzuOUWPxQzYoQfYxfJ\np8QuEhgyZAhDhgwJO4xSbd7sK2B274abb/Z16yIFaShGJLB48eKwQyjVgQNw/fXw7bfQvj2MH68K\nGClKPXaRSsI5uOsueO89aNLEV8DE0bxlEkVK7CKVxOOPQ3q6T+avvQannBJ2RFJRKbGLVAKvvw4P\nPuiXJ0yA888PNx6p2DTGLhJo0qRJ2CEUa/lyuOkmPxTz5z9Dr15hRyQVnRK7SGDixIlhh1BEZqav\ngNm1C/r0gWHDwo5IKgMNxYhUUFlZvnf+9dfQrh288IIqYKRsopLYzex5M9tsZv+NxvZEwpCamkpq\namrYYQB+2GXgQHjnHTj1VH+yVBUwUlbRGopJB54GXozS9kRiLiMjI+wQDkpLg+eeO1QBc+qpYUck\nlUlUErtz7h0zaxaNbYlUBPtz9pOVmxXKvufOhUFDgeow+jlodQ78cCCUUKSS0slTkcD+nP18tf0r\n6oysw57sPeEGE0zu9cuV8Mu/hBuKVD4xS+xm1h/oD3DaaafFarciZfLyf1/mw+8+xDkH2VA1qSo1\nqsR2YnPn/BWQnIMqVTSmLkXtYleZ1otZYnfOjQPGAbRr187Far8ipZmxYgZ9X+mLq+84o/4ZLEhd\nQJPjm2AxLEHJyoIrr4SFC/2Xj95918+xLlKQDS3be1LljpLQFny9gN7/7k2uy2XIo0NYNXcVKSek\nxDSpOwf33eeT+imn+JOlSuoSiWiVO74ELAZam9l6M/tVNLYrUp4+WP8B17x0DQdyDzCw3UAevuzh\nUOJ46il/ndIaNfzEXhX0C7BSiUSrKuamaGxHJFY+3fQpXSd1ZU/2Hvqe25enuj3FgAEDABg3blzM\n4njzTXjgAb/8/PN+Kl6RSKkqRhLOqq2ruGLCFWzfv52erXvyQs8XSLIkVq5cGdM4vvzSTxOQl+en\nCrhJ3SOJEo2xS0L5fMvnXJp+KZv2bKJz8868fMPLVEmKff9m2zY/B8zOnf7CGX/6U8xDkDimHrsk\njE82fMIVE68gc28mP2/2c6b/z/SYlzQCZGfDjTfC6tX+AtQvvghJ6mJJFOntJAnhg/UfcNmLl5G5\nN5NuLbvx+s2vU6danVBiuf9+WLAAGjWCGTOgdu1QwpA4ph67xL1ZK2fRZ2of9mbv5fo21/NSr5eo\nllytyHpt27Yt91ieeQbGjIHq1X0FTEpKue9SEpASu8S10R+N5r4595Hn8vjFeb9g/DXjSxxTT0tL\nK9dY5s71vXXwE3xdeGG57k4SmIZiJC7l5uUy+K3B3DP7HvJcHn+89I+80POFUE6UAqxcCb17Q24u\nDBkCt9wSShiSINRjl7izde9WbnnlFt786k2qJFVhfI/x/KLtL0p9Xt++fYHoX0lp+3ZfAbNjB1x7\nLYwYEdXNixShxC5x5eMNH9NrSi++2fENDWs1ZMoNU/h585+X6bnr16+PejzZ2b6nvnIlnHuuvxC1\nKmCkvCmxS1xwzjH+4/H8+o1fsz9nPz859SdM7T2V004IdybR3/wG5s2Dk07yFTB1winEkQSjxC6V\nXubeTO6ceSfTv5wOwJ3n38nfu/49lBr1gp59Fp5+GqpVg1dfhaZNQw1HEogSu1Rqb331Fv2m92PD\n7g0cX/14RncbzS3nhn9mcsECuPdev/yPf0DHjuHGI4lFiV0qpW37tjH4rcG8kPECABelXMTE6yfS\nrG6zY95mhw4dohLbqlVwww2+AubBB+G226KyWZEyM+dif82Ldu3auSVLlsR8v1L5OeeY+vlU7ptz\nH5v2bKJacjX+99L/5cGLHgytlLGgHTt8ffqKFb4S5tVXITk57KgkXpjZUudcu9LWC/83QaSMPt/y\nOalvpDJ3zVwALjntEsb1GMePGv4o5Mi8nBw/W+OKFXDOOTBpkpK6hEOJXSq87fu288eFf+SZj54h\n1+VSt0ZdHun8CP0v6E+SRa92sFevXgBMmzbtmJ4/aBC89RaceKKvgDnuuKiFJnJUlNilwtqTtYcn\nP3iSR997lJ0HdpJkSdx1wV0Mv2w4DWs1jPr+tm7deszPHTcO/v53qFoVXnkFmjWLXlwiR0uJXSqc\nfdn7GP/xeB5+92E27dkEwGXNL+OJK57gvJPPCzm6ot5+G+65xy+PGwcXXxxuPCJK7FJh7Ny/k9Ef\njSbtgzQ279kMQPvG7Rl52Ug6t+gccnTFW73aV8Dk5MDgwdCvX9gRiSixSwWwausqRn80mucznueH\nAz8AcMEpFzDsZ8Po2bonZhZyhMXbuROuucZfDal7d/jLX8KOSMRTYpdQ5OblMnvVbJ756Bne/OrN\ng493ataJoRcPpUuLLjFP6J07l/1TQU4O/M//wBdfwFlnqQJGKhYldomp1dtWM+nTSaQvS+ebHd8A\nUKNKDW4++2buaX8P559yfmixPfTQQ2Ve97e/hTfegIYNYeZMOP74cgxM5CgpsUu5y9ybyZTlU5jw\n6QTeX//+wcdb1GvB3e3u5vYf3079mvVDjPDojB8PaWmHKmCaNw87IpHDKbFLuVi3cx0zVszgtRWv\n8fY3b5OTlwNA7aq16XVmL/qe05fOLTpHtQ49Ul27dgVgzpw5Ja7zf/8Hd9/tl599Fi65JBaRiRwd\nJXaJity8XD7Z+AlzVs3htRWvsXTD0oNtyZZM1zO60vfcvvRs3ZPa1Srm1Zv37dt3xPY1a6BXLz++\n/pvfwO23xygwkaOkxC7HxDnHmu1rmLdmHnPXzGXB1wvYvn/7wfZaVWtx5elXcu2PruXqllfToFaD\nEKON3A8/+Llftm6Frl3h0UfDjkikZErsUib7c/bz8YaPWbxuMYvXL2bRukVs2L3hsHWa1W3G5S0u\n55rW19C5eWdqVq0ZUrTRlZsLN90En38ObdrASy+pAkYqNiV2KSIrN4vPt3zOso3LyNiYwfvfvc/H\nGz4mKzfrsPXq16zPZc0vo0vzLlx++uW0qNcipIjL14MPwuzZUL++r4A54YSwIxI5MiX2BJabl8va\nnWtZuXUln23+jGWblrFs4zK+yPzi4MnOfIZx9kln06FJBzo06UDHlI60atCqwn556Fh07969yGPP\nPQdPPAFVqsC0aXD66SEEJnKUlNjjXHZuNt/t+o5vd3zLmu1rWLl1JSu3rWRF5gpWb1vNgdwDRZ5j\nGC3rt+S8k8/jvEbn0b5xe37a+KecUCO+u6qDBw8+7P477xyqgBk9Gjp1in1MIsdCib0Sy8rNYvOe\nzWzavYmNuzey7od1fLvjW9b+sJZvd3zLtzu/5ftd35Pn8krcxqnHnUqrBq04s+GZBxP52SedXWEr\nV2Ll6699BUx2Ntx/P9x5Z9gRiZSdEnsF4Zxjd9Zutu/fzvZ92w/+3LZvG5v3bGbj7o1s2rPJ34JE\nXrAKpSSG0fi4xjSt25RmdZvRqn4rWjdsTasGrWhZvyXHVdek4fk6BV3yGTMW0qMHZGbClVfCqFHh\nxiVytKKS2M3sKuBJIBkY75yL++mQnHNk5WaxP2c/e7P3sjtrd5HbrqxdxT6e31YwgW/fv73IuHZp\nki2ZE2ufyMl1TqZR7UY0Ob4JTU9oymknnEbTuk1pekJTGh/fmGrJ1crpKMQf5+Dmm2H5cvjRj2Dy\nZD++LlKZRPyWNbNk4BngcmA98JGZzXDOfV7Sc3Lycti8ZzN5Lo/cvFz/0+UevF9wuaS2nLwcsnOz\nyc7LPmw5Oze4X8xy4XULLucn6ZJuB3IPFHks2mpVrUW9GvWoV7Me9WvWP7h8Uq2TaFSnEY1qN/JJ\nPFhuUKtBhfrmZjxYswbWr1cFjFRu0eiLtAdWO+fWAJjZy0BPoMTEvmz5Mhqd1ejwB88KtpQFTCrm\nSW2BHwN7gCnFtP8EOBvYCbxSTHtHoDWQCcwspv1nwOnABuCNYto7A6cBa4H5/iEzI8mSSLIkTr7h\nZBq0aEDOqhy+e/07kpOSSbZkkpOSqWJVuP7B62l+RnNWL17N/Enz/eNJVQ7eXnzxRc5ofgaTJ09m\nzJgxAOwI/vuar5k6dSoNGzYkPT2d4enDi4Q3e/ZsatWqxejRo5kypegBWrhwIQCjRo1i1qxZh7XV\nrFnz4Nfohw8fzvz58w9rb9CgwcHLxQ0ZMoTFixcf1t6kSRMmTpwIQGpqKhkZGYe1t2rVinHjxgHQ\nv39/Vq5ceVh727ZtSUtLA6Bv376sX7/+sPYOHTrwyCOPAP7ydYWvdNS5c+eDE3h17dq1yDdIu3fv\nfvDEaKdizoD27t2bgQMH8v33eaxf/ynQiZQUuOMO396vXz/69etHZmYmN9xwQ5Hn33333fTp04d1\n69Zx6623FmkfNGgQPXr0YMWKFQwYMKBI+7Bhw+jSpQsZGRmkpqYWaR85ciQdO3Zk0aJFDB06tEh7\nWloabdu2Zd68eYwYMaJI+9ixY2ndujUzZ87k8ccfL9I+YcIEUlJSDnvvFVTwvZeenl6kXe+9yN97\ne/fupVu3bkXaS3vvlSQaib0xsK7A/fXATwuvZGb9gf4AVIMqyVWw4D8MatesTd26dSEbNlbdePBx\nw5fTNazbkJNOPYm8PXmsqrHKP9P885MsiRantqD5Wc3J2pbF4jqLfdIl6eA6P23zU9pc1IYd63fw\n+n9eP/T8oL1nx55ccPEFbFi1gRc+eeFgws6/PXjjg1x00UUsW7KMkatHFukpp91U4JdrSdFfrrva\n3eV/uTbP5NPanxZpr16l+jH/A0jk3n0XVq3yyy1bQt264cYjEglzzkW2AbMbgSudc3cE928F2jvn\n7ivpOe3atXNLliyJaL8i0bJ8ub+c3Y4do7n0Uli4cGDYIYkUy8yWOufalbZeNHrs64GUAvebAN9H\nYbsi5W79erjqKtixA669diBTp4YdkUjkonHm7SOgpZk1N7NqwP8AM6KwXZFytX27T+rr18NFF8H4\n8Xs5cGBv2GGJRCziHrtzLsfM7gXexJc7Pu+cWx5xZCLlaPduP1vj8uV+Yq8ZM+D66/3Jq/yTfSKV\nVVQqdJ1zs4HZ0diWSHnbsweuvhreew+aNPGXuKtfeS7gJFIqFUFLQtmzB7p39/PANG4Mb78Np50W\ndlQi0aXELgkjf/hl4UI45RSf1M84I+yoRKJPX5aWhJCZ6YdfPvwQTj7ZJ/WWLcOOSqR8KLFL3Fu7\nFq64AlasgKZN4a23oFWrouv169cv5rGJlAcldolry5cfKmk85xx/ovTUU4tfV4ld4oXG2CVuzZoF\nHTr4pH7xxf6EaUlJHSAzM5PMzMzYBShSTpTYJe44B48+CtdcA7t2QZ8+fviltPlfbrjhhqOaaEmk\nolJil7iyezfceiv87nc+wY+BmBSnAAALZklEQVQYAS+9BDVrhh2ZSOxojF3ixmefQe/e8OWXULs2\nTJgA110XdlQisaceu1R6zsH48dC+vU/qZ57pyxqV1CVRKbFLpbZxo0/gd94J+/fD7bfDRx/55C6S\nqDQUI5XW5Mlwzz2wdSscfzw8/bQfXz9Wd999d/SCEwmRErtUOmvXwv33w/Tp/v7ll/uhmEjnfOnT\np0/kwYlUABqKkUojO9uXMbZp45N6nTrw7LPw5pvRmchr3bp1rFu3rvQVRSo49dilwnMOZs+G3/4W\nvvjCP3bjjfDEE37a3WjJvxC15mOXyk6JXSq0jz+GwYP9pF0Ap5/ux9KvuircuEQqMg3FSIX02We+\nV37BBT6p16vne+j5c7+ISMnUY5cKJSMDHn6YgxeVrl4d7r0X/vAHn9xFpHRK7BK63Fx4/XX429/8\nRTDAJ/QBA/zUAEeauEtEilJil9Ds2gXp6fDkk/DVV/6x446DO+7w4+qxTuiDBg2K7Q5FyokSu8SU\nc3763Bde8MMte/b4x5s1g1//Gn71K/9lozD06NEjnB2LRJkSu8TE11/DxIm+h75mzaHHL7nEf9no\n2mshOTm08ABYsWIFAK1btw43EJEIKbFLuVm50vfKp03zZYv5mjSB226Dfv0q1nVHBwwYAKiOXSo/\nJXaJmpwc+OADf/m56dPhv/891FanDvTo4ZN5587h985F4pkSu0Rk7VqYO9cn87lzYefOQ20nnOCv\nYtSrl7+YtC52IRIbSuxSZs7BihXw7rv+BOg77/jEXlDLlv4LRF27+p55tWrhxCqSyJTYpVjO+YtA\nL1ly6LZ0qZ8it6C6deFnP/PJ/MoroUWLcOIVkUOU2IW9e/2Vh5Yvh88/h2XLfBLfvLnouo0a+USe\nfzv7bEiKk4kphg0bFnYIIlGhxJ4gsrNh3TpfarhmDaxe7WdKXL4cvvnG99ALq1cP2rU7/JaSAmYx\nDz8munTpEnYIIlGhxB4ndu+G776D77/3t7VrDyXxNWt8Us/NLf65VapAq1b+cnJnneVvF1wAzZvH\nbxIvTkZGBgBt27YNORKRyCixV1B5ebBjhx/Tzsw8/LZlC2zY4BN4fjLftevI2zPzve0WLfyteXN/\nwYqzzoIzzoCqVWPzuiqy1NRUQHXsUvlFlNjN7Ebgj0AboL1zbkk0gqrscnNh3z4/dr1rF/zwgy8D\nLO3nzp2HEvnWrT65l1WNGn5ulcaN/c8mTQ4l8RYtoGlTP7GWiMS/SHvs/wWuB8ZGIZajkpfnE2j+\nLSen6P3sbMjKOvKtLOvs23coUe/dW/pyVlZ0XuMJJ0DDhkVvDRrAKaccnsjr1k2sYRMRKVlEid05\n9wWAHWVG2fvJCj6q0wmcP2nngNdr9ebFOgOpmr2Xf2Z28yfz8tsdTEjuxz+tH3VzMpmcd0ORbY7h\nbqbQhyasYwJFL1X/OIOYRQ9asYKxDKAqULtA+wiGMZ8unEcGaaQWef5QRrKYjnRgESMZWqQ9lTSW\n0ZbOzGMYI0hO8tUiyVWgSjKktRnLzpNb02nXTHqsepwqyYfaqlSBZYMnUP2MFJp/NJmTXxlDlaqQ\nVPCwpk/1WT093d8Kmz0batWC0aNhypSi7fnDC6NGwaxZh7fVrAlz5vjl4cNh/vzD2xs08PMCAAwZ\nAosXH97epImfCAYgNdVPql5Qq1Ywbpxf7t/fzzVQUNu2kJbml/v29XWWBXXoAI884pd79Spac9m5\nMzz0kF/u2tX/hS2oe3c/XSRAp04U0bs3DBzoewufflp0nX79/C0zE24o+t7j7ruhTx9/IuPWou89\nBg3yX7tdscLPRVzYsGHQpYs/bqlF33uMHAkdO8KiRTC06HuPtDR/DOfNgxEjiraPHQutW8PMmfD4\n40XbJ0zw43STJ8OYMUXbp+q9B5Tve2/vXujWrWh7ae+9EsRsjN3M+gP9AVpR5+Csfvky98G3W6Em\nUFyHNyvPP17w/J8F/zODmtWgbg2oB1Td4x9LMrAk//P0k+HCk+D0HKi3+tDj+T+v+jGc0Qya74Qm\n/zm8LSkZfnsd/HA2pKyDcyb7x5KT/M+kJHhrOFT9CdReDFUfDWIrYOJYoDUwEyjmd+uKK4AUYCOg\nL/WISATMFVfnVnAFs3nAycU0/cE591qwzkJgcFnH2M88s53717+WkJzMEW9VqpTcFi+101JxLFq0\nCICOHTuGHIlI8cxsqXOuXWnrldpjd85Fvbi3Vi3/CUikIlFCl3ihfq9IYNGiRQd77SKVWaTljtcB\nTwEnAq+bWYZz7sqoRCYSY0ODE5OqY5fKLtKqmFeBV6MUi4iIRIGGYkRE4owSu4hInFFiFxGJM5oE\nTCSQlv8NRJFKToldJKDpeiVeaChGJDBv3jzmzZsXdhgiEVOPXSQwIphAS1dSkspOPXYRkTijxC4i\nEmeU2EVE4owSu4hInNHJU5HA2LExv8KjSLlQYhcJtG7dOuwQRKJCQzEigZkzZzJz5sywwxCJmHrs\nIoHHgws99+jRI+RIRCKjHruISJxRYhcRiTNK7CIicUaJXUQkzujkqUhgwoQJYYcgEhVK7CKBlJSU\nsEMQiQoNxYgEJk+ezOTJk8MOQyRi6rGLBMaMGQNAnz59Qo5EJDLqsYuIxBkldhGROKPELiISZ5TY\nRUTijE6eigSmTp0adggiUaHELhJo2LBh2CGIRIWGYkQC6enppKenhx2GSMSU2EUCSuwSLyJK7Gb2\nmJl9aWafmtmrZlY3WoGJiMixibTHPhc42zl3LrASGBJ5SCIiEomIErtz7i3nXE5w932gSeQhiYhI\nJKI5xn47MCeK2xMRkWNQarmjmc0DTi6m6Q/OudeCdf4A5ACTjrCd/kB/gNNOO+2YghUpT7Nnzw47\nBJGoMOdcZBsw+wVwF9DZObe3LM9p166dW7JkSUT7FRFJNGa21DnXrrT1IvqCkpldBfwOuLSsSV2k\noho9ejQAAwcODDkSkchEOsb+NHAcMNfMMszs2SjEJBKKKVOmMGXKlLDDEIlYRD1259wZ0QpERESi\nQ988FRGJM0rsIiJxRoldRCTORFzueEw7NdsCfBvzHR+uIZAZcgwVhY7FIToWh+hYHFJRjkVT59yJ\npa0USmKvCMxsSVnqQROBjsUhOhaH6FgcUtmOhYZiRETijBK7iEicSeTEPi7sACoQHYtDdCwO0bE4\npFIdi4QdYxcRiVeJ3GMXEYlLSuyAmQ02M2dmCXuZel3m0E9qZ2YrzGy1mf0+7HjCYmYpZva2mX1h\nZsvN7P6wYwqbmSWb2SdmNivsWMoi4RO7maUAlwNrw44lZAl9mUMzSwaeAboCZwI3mdmZ4UYVmhxg\nkHOuDXAhcE8CH4t89wNfhB1EWSV8Ygf+BjwIJPTJBl3mkPbAaufcGudcFvAy0DPkmELhnNvgnPs4\nWN6FT2iNw40qPGbWBLgaGB92LGWV0IndzK4BvnPOLQs7lgomES9z2BhYV+D+ehI4meUzs2bAj4EP\nwo0kVGn4zl9e2IGUVUTT9lYGR7q0HzAUuCK2EYUnWpc5jFNWzGMJ/SnOzOoA04BU59wPYccTBjPr\nDmx2zi01s05hx1NWcZ/YnXNdinvczM4BmgPLzAz80MPHZtbeObcxhiHGTEnHIl9wmcPu+MscJlpS\nWw+kFLjfBPg+pFhCZ2ZV8Ul9knPulbDjCdFFwDVm1g2oARxvZhOdc31DjuuIVMceMLNvgHbOuYow\n0U/MBZc5fAJ/mcMtYccTa2ZWBX/SuDPwHfARcLNzbnmogYXAfE/nn8A251xq2PFUFEGPfbBzrnvY\nsZQmocfY5TAJfZnD4MTxvcCb+JOFUxIxqQcuAm4FLgveCxlBj1UqCfXYRUTijHrsIiJxRoldRCTO\nKLGLiMQZJXYRkTijxC4iEmeU2EVE4owSu4hInFFiFxGJM/8fVcHfYljiCVUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x297006c64a8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ez = elu(z)\n",
    "dez = np.gradient(ez, z)\n",
    "\n",
    "plt.plot(z, ez, \"b-\", linewidth=2)\n",
    "plt.plot(z, dez, \"g-\", linewidth=2)\n",
    "plt.plot([-5, 5], [0, 0], 'k--')\n",
    "plt.plot([-5, 5], [-1, -1], 'r--')\n",
    "plt.plot([0, 0], [-2.2, 3.2], 'k--')\n",
    "plt.title(r\"Funcao de ativacao ELU ($\\alpha=1$)\", fontsize=14)\n",
    "plt.axis([-5, 5, -2.2, 3.2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fire(self, ypred):\n",
    "    if self.name == 'sigmoid':\n",
    "        return tf.nn.sigmoid(ypred) \n",
    "    elif self.name == 'relu':\n",
    "        return tf.nn.relu(ypred)\n",
    "    elif self.name == 'elu':\n",
    "        return tf.nn.elu(ypred)\n",
    "    else:\n",
    "        return ypred\n",
    "        \n",
    "Activation.fire = fire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "model = FeedforwardNeuralNet(input_dim = 28*28)\n",
    "\n",
    "model.add(units=300, activation = Activation('elu'), name = 'h1')\n",
    "model.add(units=100, activation = Activation('elu'), name = 'h2')\n",
    "model.add(units=10, name = 'out')\n",
    "\n",
    "model.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0 loss: 0.29459507 acc: 0.96\n",
      " 1 loss: 0.14510611 acc: 0.96\n",
      " 2 loss: 0.10564223 acc: 1.00\n",
      " 3 loss: 0.08409390 acc: 0.98\n",
      " 4 loss: 0.06919516 acc: 1.00\n",
      " 5 loss: 0.05796448 acc: 1.00\n",
      " 6 loss: 0.04923700 acc: 1.00\n",
      " 7 loss: 0.04204264 acc: 1.00\n",
      " 8 loss: 0.03683003 acc: 0.98\n",
      " 9 loss: 0.03107884 acc: 1.00\n"
     ]
    }
   ],
   "source": [
    "model.fit(mnist.train, n_epochs = 10, batch_size = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/model.ckpt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9783"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(mnist.test.images, mnist.test.labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outro problema é que, na prática, mesmo que a entrada seja normalizada em média 0, variância 1, as camadas seguintes vão perdendo essa normalização (esse problema é conhecido como _internal covariate shift_, ver https://github.com/aleju/papers/blob/master/neural-nets/Batch_Normalization.md).\n",
    "\n",
    "Uma forma de lidar com esse problema é aprender, durante o treino, a média e a variância de cada camada. Assim, é possível re-normalizar a entrada das camadas seguintes. Esses dois novos parâmetros pode ser estimados nos batches, através de uma estratégia que que ficou conhecida como _batch normalization_ -- BN (ver https://github.com/aleju/papers/blob/master/neural-nets/Batch_Normalization.md). O uso de BN virtualmente acaba com os problemas de perda e estouro de gradientes.\n",
    "\n",
    "Mais recentemente (junho de 2017), uma alternativa à BN, a função SELU, foi proposta em um artigo (https://arxiv.org/pdf/1706.02515.pdf) de Günter Klambauer, Thomas Unterthiner e Andreas Mayr. A ideia é que a própria função de ativação já garanta saídas normalizadas, sem necessidade de parâmetros adicionais. Embora muito recente, esta tem se mostrado a melhor solução para o problema em redes profundas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# valores de SELU para media 0, desvio 1\n",
    "# ver paper para detalhes\n",
    "def selu(z,\n",
    "         scale = 1.0507009873554804934193349852946,\n",
    "         alpha = 1.6732632423543772848170429916717):\n",
    "    return scale * elu(z, alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXYAAAELCAYAAADN4q16AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAIABJREFUeJzt3XmczfX+wPHXe8ZYhuxEKFIU6qeb\nvZJQIktZW7gppdJCaUXaRIrS7UZcNyMUEylcUlNJN1LjNi3UCImRmLGMZcz++f3xOcPsi/nO+Z7l\n/Xw8vo/5nvPd3uc757zP53y+n+/nI8YYlFJKBY4QtwNQSinlLE3sSikVYDSxK6VUgNHErpRSAUYT\nu1JKBRhN7EopFWA0sSulVIDRxK6UUgFGE7vKQ0RWiUiES8eOEJFVbhw72IhIDRHZLyJN3Y7Fm0Rk\nqYg84nYcZUkTezF4ko3JZ2rtdmz+SkTWicg/81k0Ghjq7XicJiJ1RGSmiOwSkRRPAv1MRK7Ntk5B\n76tvsi0v8EuuoHMoIsNF5HgxwhwHrDbG7DiT11gaItJZRFaIyF7Pax5ezO3qi8h8EYkXkWQR2Soi\nV2dbvquAc/qfbLt5DpggItUcflk+o5zbAfiRKGBYrucS3AgkkBljEt2OwSHLgHBgBLAdqAtcDdTK\ntV5+76vUsg5ORMKBu4A+ZX2sAlQBfgbe8UxFEpHqwNfAf4EbgHjgfOBAttXaAqHZHtcHNgORWU8Y\nY34SkZ3YAsSbZ/4SfJgxRqciJiACWFXI8nXAPwvaxrN8JjAZ+2VwAJgGhGRbX4CxwG9AChAHTMm2\n/HrgK+AwcAhYC1yc65gVgBnAfiAZ+Aa4sojXFu6J9bhnu3HAKiAiW1yPAzuAk8BPwNBinLMC4/Uc\nz+SaGmc/b8A9nnjK5drvu8BHxT0vbpxXoLrnNXUv5fuqxO87z/PDgeNFHHsgcBCQXM9fDnzm+V9v\nBzoDg4Gvz/CzI8VY5zgwvBjrTS5pHMB44AgQnuv5icB/z+Q1+cOkVTHecxuQDnQCHgDGAEOyLZ8M\nPA1MAVoCg4A92ZZXxiaXdkAXIBFYKSLls63zsmefdwKXYZPwxyJSv5C4pgHXAgOAbp7tOmdbPglb\n6rwfaOGJb7aI3FDE6y0s3tHARmAetkRVP9drBVvCqg50z3pCRCoD/YCFxTwOuHNej3umviJSsYB1\n3HYVsNl4shyAiLTFfsl9AVyK/QJ7Dpscn869AxE5S0SmiMj3IrLTU3V0nYhUEZFmIjLDcxyn3Ahs\nEpElInJARGJE5AERkfxW9jw/AlhojEnKtfhboJ2IVHIwPt/h9jeLP0zYklM6pz+wx4E12Zavo+gS\n+8Zcyz8F5nrmq2BLgveWIKbKQAaekqPncSrw92zrhGJL2pMK2EcVbCn2tlzPHfHEXxlbcrsq13Yz\nsHWzJTmHuePNc87yOW/LgQXZlg3FJt6KxTmOW+fVs84A7C+AZOyX2DSgfTHeV8eBqbnPRQHHKOgc\nDqfoEvuHwPxcz60HlmR73MtzLr4oYB/PA/OBHkBf7K/SeOyvlZPAv4CzinHOi1tiT/ZMU7BfsHd4\ntn2ggPWv88TSOp9ll3qWNS3J+9hfJq1jL771wMhsj0+WcPsfcz3+E1vvCrYkXAH7EzhfnpYLLwDt\ngTrYC98hwLmeVZoCYdg6SACMMRkistGz//w0BcpjE0/WNsdF5KdscVXElk6z9+8cBuwqKNZixlsc\nC4EIEQk3tsR1G7DUGJNczOOk4s55xRizzHPB7iqgI7bKZ6yIjDfGTM62au73Fdgv1rJWCVu1BICI\n1MPGek22dVKx5yJPad3jNWPMYRFpAIQZY1aIyANAPWx1owGqOhhzCBBtjHnK8/h7EbkQ+2syvwvx\ndwPfGWNi8lmW9fkNyBK7JvbiSzLGbC9gWSa2Lje7sFyP03I9NpxulZTvT8lcVgJ7sXXPe7Elva3Y\nxJx9H/l1sF9Qp/tFHTcrvj7A7lzLcr+e3IqKtzhWebbrJyKfYatlrivBcdw6r3ah/QL61DM9LyJz\ngWdFZJoxJusCaWHvq6IcBfJr2VEd+8umMAlAjWyPL/b8/S7bc82BWGPMfwvYR3URWYbny0BEYoF/\nYy8ItwQmAM9gf1k4YR/2f5PdL9iqvRxEpC622u7+AvZV0/M33qHYfIrWsTsjHltPnN3/lWD7rdgq\nkW75LRSRWtgP3mRjTJQx5hfgLHJ+MW/HlrCuzLZdKLa0mPvDkH2bNKBDtm0qA61yxXWeMWZ7rumP\ngl5MMeNNJWfrhTyMMSnAUmxJfQjwF/BlCY7j1nktyFbPvp2qd48F/pZPHfPfPMsK8z05f3FkXfDN\nBFt/jq1bz103nd2t2F9DF2DfM3Ox1zC+Bv4BrCHbLx0HfI39ssmuGZDfe/EO7P9+cQH7agX8aYzZ\nX8Byv6Yldmd8DswQkb7YD9Q9QCOKqK7IYow5JiKvA1NEJAX787wWcLkxZha2xUYCcLeI7AEaAK9g\nS5dZ+zghIrOAl0QkAfgdeBg4G1v3md9xj4vIv4GpIhKPrR6aiCfheuKaBkzzJI/12HrrDkCmMWZO\nAS+pyHg956adiDTG1pMeMsZk5rOvhdgSYBPg3VzrFHoct86r5wvjfeBtbBXcMaANtnXRZ8aYo9lW\nr+CpBskuwxiTVZKsms/9EkeMMbuAWdgL8W+IyL+w9c+9gFuwpdXCrMX+32sZYw4CMdhfJ0+JyCLP\nedgHXCAiFxpjfstnH1ONMdn/p1uw1xKKJCJVsF8I4Kn68rzOQ8aY3Z51HsDWn1/kWe81YIOIjAeW\nYOvZH8K25Mq+b8E25VxsjDlWQAhXAR8XJ1a/5HYlvz9MFH0RKwzbHjbBMz1P3ounBV5c9TwOAZ4E\ndmJLiHuAF7Mt74pt95vs+duDXBedyNksL4XiNXesjG1HfBxbL/o0eZs7Psjp0m88tmrh2iL2W2i8\n2JLWRmyJME9zx2z7EeyXgAEuOYPjeP28etafjK3WOOx5jb8BrwI1c70Hcjf7NEBcEcuXZttHW2yS\n3o+tftkE3FjM9/VG4P5sj8d5/r/JwCJsVc3XQHwZfKa6FPDaIrKt8yxgcm13A/CDJ8Zt2MSeu8nm\nNZ59tSvg2BU956qDt3OJtybxvFClVJARkeuB14EWxpgMt+PxFhG5H+hnjMl9vSZgaB27UkHKGPMx\n9pdmQ7dj8bI07K/QgKUldqWUCjBaYldKqQCjiV0ppQKMK80da9eubRo3buzGoZUqUGysbfrdvHnu\nptLKLSkp8MsvkJEB55wD9Qvr9SgIbN68OcEYU6eo9VxJ7I0bNyY6OtqNQytVoC5dugCwbt06V+NQ\n1vHj0LGjTeo33gjLlkFIkNcxiEiBNwZmF+SnSSnli4yBu+6Cn3+G5s1h/nxN6iWhd54q5TFhwgS3\nQ1Aer74KS5ZAlSqwfDlUdbIrsSCgiV0pj+7duxe9kipzn38Ojz9u5995By6+uPD1VV6l/nEjIhVF\n5FsR+UFEtojIc04EppS3xcTEEBOTXw+vylt274YhQyAzE556Cm66ye2I/JMTJfYUoKuxHUqFAf8V\nkTXGmG8c2LdSXjNmzBhAL566JTkZBgyAhATo0QNeeMHtiPxXqRO7sbeuZo2IHuaZ9HZWpVSxGQOj\nRkF0NDRpAu++C6GFduqsCuPIdWYRCRWRGGzvgJ8aYzbls85IEYkWkej4+IDs214pdYZmz4Z586BS\nJfjgA6hZs+htVMEcSezGmAxjTGtsZ0LtRKRVPuvMMca0Mca0qVOnyPb1SqkgsXEjPPSQnZ8zB1rn\n7n1elZijLUONMUewfY9f7+R+lVKB6a+/YOBASEuzyX3oULcjCgylrmMXkTpAmjHmiIhUwo5LObXU\nkSnlZZMnTy56JeWYtDQYNAj+/BOuugqmFWvsJVUcTrSKqQ/M94wDGQJEGmNWObBfpbyqU6dObocQ\nVMaOhf/+1/YBExkJYbmHf1dnzIlWMT9ixx5Uyq9t2LAB0ATvDQsWwBtv2GS+bBnUyz3qqyoVvfNU\nKY9x4+yYyNqOvWx9/z2MHGnn//lP6NDB3XgCkXaro5TymoMHoX9/ezPSiBFw991uRxSYNLErpbwi\nIwNuuQV27YK2bW1pXcTtqAKTJnallFdMmACffgp16th69YoV3Y4ocGliV0qVuWXL4KWXbDcBkZHQ\nqJHbEQU2vXiqlMeMGTPcDiEgbd0Kw4fb+VdeAc9AVaoMaWJXyqO13svuuMRE2/Xu8eNw883g6UBT\nlTGtilHKIyoqiqioKLfDCBiZmXD77bBtG1xyCcydqxdLvUVL7Ep5TJo0CdCRlJwyeTJ89BFUr26H\nt6tc2e2IgoeW2JVSjluzBiZOtCX0d9+Fpk3djii4aIldKeWoHTvg1lvt4BnPPw89e7odUfDRErtS\nyjEnTtiLpUeOQN++MH682xEFJ03sSilHGGO7CPjpJ2jWDN55B0I0w7hCq2KU8pg9e7bbIfi1GTPg\nvfegShV7sbRaNbcjCl6a2JXyaN68udsh+K116+Cxx+x8RAS0aOFmNEp/KCnlsXLlSlauXOl2GH5n\nzx4YPNh28vXEEzBggNsRKS2xK+Uxffp0APr06eNyJP4jOdkm8vh4uPZaePFFtyNSoCV2pVQpPPgg\nfPcdnHeerV8PDXU7IgWa2JVSZ2jOHNtNQMWK9mJprVpuR6SyaGJXSpXYN9/AAw/Y+Tlz4DId9din\naGJXSpXI/v0wcCCkpdnkPmyY2xGp3PTiqVIeCxYscDsEn5eWZlvA7N0LV14JnuvNysdoYlfKo5EO\n61Okxx6D9euhfn14/30oX97tiFR+tCpGKY8lS5awZMkSt8PwWYsWweuvQ1iYHequXj23I1IF0RK7\nUh6zZs0CYMiQIS5H4ntiYmw/MAD/+Ad07OhuPKpwWmJXShXq0CHo3x9OnoQ77oB77nE7IlUUTexK\nqQJlZNi+1X//Hdq0gZkzdXg7f6CJXSlVoIkTYe1aqF3b1qtXrOh2RKo4NLErpfK1fLkdtzQkBJYs\ngXPPdTsiVVx68VQpj6VLl7odgs/49Ve4/XY7P3UqdO3qbjyqZDSxK+VRu3Ztt0PwCUePwo03wrFj\nMGQIjB3rdkSqpEpdFSMijUTkCxH5RUS2iMhoJwJTytsiIiKIiIhwOwxXZWbaknpsLLRqBf/+t14s\n9UdOlNjTgbHGmP+JyFnAZhH51Biz1YF9K+U1WUl9+PDhrsbhppdegg8/tMPaLV8OlSu7HZE6E6Uu\nsRtj9hlj/ueZPwb8AjQo7X6VUt61di1MmGDnFy2CCy5wNx515hxtFSMijYHLgE1O7lcpVbZ27oRb\nbgFj4Nln4YYb3I5IlYZjiV1EqgDLgDHGmKP5LB8pItEiEh0fH+/UYZVSpZSUZO8sPXwY+vSBp592\nOyJVWo4kdhEJwyb1RcaYD/JbxxgzxxjTxhjTpk6dOk4cVilVSsbAyJHwww9w4YWwYIFtt678W6kv\nnoqIAP8GfjHGvFr6kJRyx+rVq90Owev+8Q9bn165sr1YWq2a2xEpJzjx3XwFMAzoKiIxnqmXA/tV\nyqvCw8MJDw93Owyv+fLL023U582Dli3djUc5p9QldmPMfwFt6ar83syZMwEYNWqUy5GUvbg4OxJS\nRoYdPGPQILcjUk7S2jSlPCIjI4mMjHQ7jDKXkmLHLD1wALp1s/3BqMCiiV2pIPPQQ7Bpk+3Ua/Fi\nKKcdiwQcTexKBZG5c2HOHKhQAT74wHbHqwKPJnalgsS338L999v52bPh8svdjUeVHU3sSgWBAwdg\nwABITYVRo053yasCk9auKeWxbt06t0MoE+nptgVMXBx06gSvveZ2RKqsaYldqQD3+OO2zXq9evD+\n+1C+vNsRqbKmiV0pj2nTpjFt2jS3w3DUe+/ZEnq5crB0KZxzjtsRKW/QxK6C3srYlWyN38qqVatY\ntWqV2+E45scfYcQIO//663DFFe7Go7xH69hVUFv922r6Lu7LudXOpTGNkQC5ifrwYbjpJjh50l4o\nve8+tyNS3qQldhW0MjIzeDLqSQB2J+4mISnB5YickZEBt91m+1j/299g1iwd3i7YaGJXQevdn97l\npwM/nSql7z261+WInPHss7BmDdSqZW9CqlTJ7YiUt2liV0EpJT2Fp7+wI0q8fv3rVClfhcSMRNJD\n0l2OrHQ++ggmTbJ9qi9eDOed53ZEyg2a2FVQmhU9iz8S/6BV3VaMajuKYZcOg6FwzYRr3A7tjMXG\nwrBhdn7KFOje3d14lHs0saugk5icyKT1kwCY0m0KoSGhdGjYAYDth7e7GdoZO3bMXiw9dsx2wfvY\nY25HpNykiV0FnalfT+XgyYNcee6V3HChHbX5/Brnw5fw1YKvXI6u5IyBO+6AX36BFi3g7bf1Ymmw\n0+aOKqhsP7Sd6RunA/DKta8gngzYtEZT2An7Q/e7Gd4ZmToVli2DqlXt8HZVqrgdkXKblthVUBnz\n8RhSM1IZ3nr4qeoXgHpV6iEipGekk5ic6GKEJfPJJzB+vJ1fuBCaNXM3HuUbNLGroLFq2yr+89t/\nqFqhKi91eynHMhGhUphtF7jz8E43wiux33+HW26BzEyYOBH69HE7IuUrNLGroJCcnszoj0cD8FyX\n5zi7ytl51qlUzn8Se1IS9O8Phw5Br17wzDNuR6R8iSZ2FRSmb5jOzsM7aVGnBfe3vT/fdarXrA7h\nsOPwDi9HVzLGwL33QkwMNG1qq2BC9JOsstG3gwp4vx38jUlf2eaNb/R8g7DQsHzXGz19NAzx/RL7\nm2/CggUQHg4ffgg1argdkfI1mthVQMs0mYxYMYLk9GSGXTqMrk26Frju+TXOB3w7sX/1FTz8sJ1/\n+21o1crdeJRv0sSuAtpb0W/x1e6vOLvy2cy4fkah63705kcQ5btVMXv32puP0tNh7FgYMsTtiJSv\n0sSuAtauI7t4/NPHAZh5w0xqVqpZ6Pq/xvwKe+CPI3+QnulbfcakpMDAgbB/P1xzDbz0UtHbqOCl\niV0FJGMMI1eO5ETaCQa1GET/i/sXuU2IhFC+XHkyTAZ/HPnDC1EW35gx8M030KgRLFliR0RSqiCa\n2FVAeiv6LT7d+Sm1KtXijZ5vFHu7ymGVAYj+M7qsQiuxt9+Gt96CChVsN7x16rgdkfJ1mthVwPn5\nwM888skjAMy6YVa+bdYLUr1idQC+/OPLMomtpKKjYdQoOz9rFrRp4248yj/oDzoVUE6mneTmpTeT\nnJ7MiMtGMKjloGJv27BhQ8rXKM/v/M76P9aXYZTFEx9vb0JKSbHt1u+4w+2IlL/QxK4CythPxrIl\nfgvNazXn9etfL9G2CxcuJCU9hepTq7MlfgsJSQnUDq9dRpEWLj0dbr4Z9uyBDh3sYNRKFZdWxaiA\nsfyX5cyKnkX50PK8N+A9KpevXOJ9VChX4VTnYF/94V4Xvk8+CZ9/DmefbXtuLF/etVCUH3KkxC4i\nbwO9gQPGGL1lQnndrwm/cvuHtwPwUreXuKz+ZSXex5gxYwC4+sarWbdrHWt3rKVjo46OxlkcH30E\n02dDaDV4ayGEVIW/jns9DOXHxBhT+p2IdAaOA+8UJ7G3adPGREf7TqsD5d8SkxNpP7c9sQdjGdRi\nEEsGLjnVz3pJdOnSBYCJ8ybS7Z1uDkeplAOeZbMxpshL6I6U2I0x60WksRP7UqokMk0mw5YPI/Zg\nLJfUvYS3+719Rkk9uysaXUG3Jt34+cDPDkVZPMbAwYOQkQEVK0K1al49vPID+yneQDB68VT5tee/\nfJ6V21ZSo2INlg9ZTpXypR8+qEK5CkT9PcqB6IovM9P2p756NbRuDRs2QKVKXg1B+QF5rHiFFq9d\nPBWRkSISLSLR8fHx3jqsCmALfljAc18+R4iE8N6A92has6nbIZ2x556zSb1mTTu8nSZ1VRpeK7Eb\nY+YAc8DWsXvruCowRe2M4s4VdwLwWo/X6HFBj1Lvs5lL48qtXAnPP2/7VF+8GBo3diUMFUC0Kkb5\nnZi/Yui/pD/pmek82vFRHmr/kCP7nTNnjiP7KYlt22DoUDv/4otw7bVeD0EFIEeqYkTkPWAj0FxE\n4kRkhBP7VSq3XUd20WtRL46lHuPmVjcz9dqpbod0xo4fh5tugqNHYcAAeOIJtyNSgcKpVjG3OLEf\npQqzO3E318y/hn3H93H1eVcT0S+CEHHuMtHIkSMB75TcjbFdBGzdChdfDPPmQSkb8yh1ilbFKL+w\n9+heus7vyq4ju2jXoB0rbllBhXIVHD3Gtm3bHN1fYaZNg6VLoWpVe7H0rLO8dmgVBLRLAeXz9h3b\nR9d3urLj8A4ur385a4eupWqFqm6HdcaiomyXAQDvvAPNm7sbjwo8mtiVT9t1ZBedIzqz7eA2Wtdr\nzSfDPjnVta4/2rXLdu6VmQkTJkC/fm5HpAKRVsUon7XlwBauW3gdfx77k8vqXcYnwz4pcng7X3by\npL1IevAg9OwJzz7rdkQqUGliVz7pm7hv6LWoF4eTD9P5vM6suHkF1SqW7T32rVu3LrN9GwP33Qf/\n+x+cfz4sWgShoWV2OBXkNLErn/PBLx8wbPkwktKS6Nu8L4sHLKZSWNnfijljxowy2/esWTB/PoSH\n24ulNWqU2aGU0jp25TuMMUz+ajIDIgeQlJbEHa3vYNngZV5J6mXp669h9Gg7P3cuXHqpu/GowKcl\nduUTktOTuXvl3Sz8cSGC8FL3l3is02Ol7qmxJIZ6bgFduHChY/vctw8GDrQjIj38MNyid3woL9DE\nrly3/dB2Br0/iJi/YqgcVpl3B7xL3+Z9vR5HXFyco/tLTbVJ/a+/oEsXePllR3evVIE0sStXvb/l\nfUasGMGx1GM0rdGUZYOX8X/1/s/tsBzx8MO2+92GDWHJEiinnzblJfpWU644kXqCxz59jFnRswAY\n2GIgc/vMLfOWL94SEQEzZ9qxSpctg7p13Y5IBRNN7MrrNuzZwO0f3s72Q9sJCwlj+nXTeaDdA16t\nTy9LmzfDvffa+TffhHbt3I1HBR9N7MprTqad5Lkvn+OVDa+QaTK5pO4lvHPTO7SuV3btx0uiY8fS\nD1ydkAD9+0NKCowcCXfd5UBgSpWQJnblFWu3r2XU6lHsPLyTEAnhySue5NkuzzrekVdpTJkypVTb\np6fb7gJ274b27eEf/3AoMKVKSBO7KlNxR+MY+8lYIrdEAnBJ3UuY3Xs2HRuVvnTsa8aPh88+s/Xp\nS5dCBd/5zlJBRhO7KhPHUo7x8tcvM33jdE6mnyQ8LJznujzH6PajCQsNczu8fA0YMACAZcuWlXjb\n99+3zRlDQ+18w4ZOR6dU8WliV45KzUhl3vfzeGbdM+w/sR+AQS0G8cq1r3Be9fNcjq5wBw8ePKPt\nfv7ZDpoBMH06dO7sYFBKnQFN7MoRaRlpzP9hPpPWT+KPxD8AaN+gPa/2eJVOjTq5HF3ZOXLEXiw9\ncQJuuw0ecmb4VaVKRRO7KpXjqceJiIng1Y2v8vuR3wG4uPbFPNvlWQa1GBQwTRjzk5kJw4bBb7/B\n//0fzJmjw9sp36CJXZ2RuKNx/PPbfzJ782yOJB8BoHmt5jxz9TMMbjmY0JDA75N20iRYtcr21PjB\nB7bnRqV8gSZ2VWzGGL7a/RWzN88mcksk6ZnpAFzR6Aoe6fgI/Zr38+uE3q1bt2Kv+5//2IEyROC9\n92wf60r5Ck3sqkh7j+5l/g/zmRczj+2HtgMQIiEMbjmYRzo8QvuG7V2O0BlPP/10sdb77Tdbn24M\nvPgi9OhRxoEpVUKa2FW+jiQfYUXsChb/vJi1O9aSaTIBaHBWA4a3Hs5df7uLxtUbuxukC44ftxdL\nExPhppvgqafcjkipvDSxq1MOnTzER79+xPtb3ydqZxRpmWkAhIWEMeDiAdx52Z1ce/61fl3dUpie\nPXsCsGbNmnyXGwMjRtjmjRddZDv60oulyhdpYg9ixhhi/orh4+0fs2b7Gjbs2UCGyQBsVUvXJl0Z\nePFABrUcRO3w2i5HW/ZOnjxZ6PJXX4XISDjrLDu8XdWqXgpMqRLSxB5kdh3Zxfo/1vPFri/4ePvH\n/HX8r1PLQiWU7ud3Z1CLQdx40Y3Urax9zWb5/HN4/HE7P3++LbEr5as0sQew9Mx0tsZvZVPcJtbv\nXs/6P9azO3F3jnUanNWA6y+4np4X9KTb+d2oXrG6S9H6rt27YcgQ22593Dhbt66UL9PEHiAyTSbb\nDm4j+s9oov+M5rs/v+P7fd9zMj1n9UKNijW46ryr6HxuZ65reh2t6rYK6JuISis5GQYMsN3x9ugB\nzz/vdkRKFU0Tu58xxrDn6B62HNjC1vitbInfwpZ4O3889Xie9ZtUb0LbBm25stGVXN34alrVbUWI\nhLgQue/r3bt3jsfGwKhREB0NTZrAu+/aTr6U8nWa2H1Qpsnkz2N/suPQDnYc3nH67+EdxCbEciz1\nWL7bNTirAW3OaUPbc9rS5pw2tDmnDbXCa3k5ev/16KOP5ng8ezbMmweVKtmLpTVruhSYUiWkid3L\njDHEJ8UTdzSuwGnP0T0kpycXuI/a4bVpWaelneq2pEWdFrSs05I6let48ZUEto0bT3fo9a9/2b5g\nlPIXmthLwRjDyfSTJCYnciT5CAlJCcQnxRN/Ip74pHgOnDiQ53FCUsKpW/ELUzu8Nk1rNKVpzab2\nr2e+Wa1m2lqljHTp0gWAxYvXMWAApKXB6NH2LlOl/IkjiV1ErgdeB0KBucaYl5zYr9OMMaRkpHAi\n9QRJaUmcSDuRYz4pLSnH42MpxziSfITElEQSU2zyzkriiSmJJCYnnrqJpyRqVKxBw6oNC52qVtBG\n0m4wBgYNgn37bL/qr7zidkRKlVypE7uIhAJvAtcCccB3IrLCGLO1oG0SUxJZEbuC9Mz0Aqe0jLRT\n86kZqaRkpJCSnnJ6PvfjXPMpGZ7HnvmktCSS0pJO3RrvlAqhFahWsRrVKlSjdnht6lSuQ53wOtSt\nXJc64XVyPq5ch9rhtalYrqKjMSjn7NwJcXHQoIG9GSnMNwd7UqpQTpTY2wHbjTE7AURkMdAPKDCx\nb9+2nX49+uV8sqVnT6nAonw2ag1cBpwAIvNZ3hZoBSQCH+SzvBPQHMIOhWFWGUIkhFAJJVRCCQkJ\n4cIbL+Tcy84lOS6ZmIgYQiRBR8NwAAAQrElEQVSEciHlCA0JpVxIOf7+8N9p16Edu3/ezZyX55x6\nPquFyYwZM2jdujVRUVFMmjSJ7WzPcfjZs2fTsH5DVq5cyfTp0/OEt2DBAho1asSSJUuYNWtWnuVL\nly6ldu3aREREEBERkWf56tWrCQ8PZ+bMmURG5j1B69atA2DatGmsWrUqx7JKlSqduo3+hRde4LPP\nPsuxvFatWqeGi3vqqafYuHFjjuUNGzZk4cKFAIwZM4aYmJgcy5s1a8acOXMAGDlyJNu2bcuxvHXr\n1syYMQOAoUOHEhcXl2N5x44dTw00PWDAgDwjHXXr1u1UB149e/bMcwdp7969T10YzapuyW7w4MGM\nGjWK48cziYv7EehC3bq27TrA8OHDGT58OAkJCQwcODDP9vfddx9Dhgxhz549DBs2LM/ysWPH0qdP\nH2JjY7nnnnvyLJ8wYQLdu3cnJiaGMWPG5Fk+efJkOnXqxIYNGxg3blye5bnfe7nNnj2b5s2b63vP\nh997SUlJ9OrVK8/yot57BXEisTcA9mR7HAfk6e5PREYCIwFCyodQI7wGgiAiCELjBo1p2bolpMHa\nFWtPPS9ip8svupwO3TuQdiyN+Z/NJ0RCEBFCJIQQQuhzZR+u63cdR/Yf4YVvXjj1fNZ6YwaOof+N\n/dnx2w7u+SGfD1f3bB+ulXk/XNc0uYZOTTuxYf8GKoVVcuC0KV+za5f9W7++dheg/JsYY0q3A5FB\nQA9jzF2ex8OAdsaYBwvapk2bNiY6OrpUx1XKSd9+C+3bz6RcOdi1axQNGrgdkVJ5ichmY0ybotZz\nosQeBzTK9rgh8KcD+1XKK4yBJ58EGMXYsWhSV37PiVsQvwMuFJEmIlIeuBlY4cB+lfKKTz6BL76A\n6tWTePDBJLfDUarUSp3YjTHpwAPAWuAXINIYs6W0+1XKGzIzs0rrULNmL267Le8FLKX8jSPt2I0x\nq4HVTuxLKW9avBhiYqBhQ62CUYFDe4NSQSs1FbKGOX32WQjRT4MKEPpWVkFrzhx7Q9JFF8Htt7sd\njVLO0cSuglJi4um+1adMgXLaa5IKIPp2VkFpyhSIj4crroB+npughw8f7mpMSjml1DconQm9QUm5\n6fffbfVLaqq9MaltW7cjUqp4inuDklbFqKDz5JM2qQ8dmjOpJyQkkJCQ4F5gSjlEq2JUUNmwwfba\nWLEiTJ6cc1lWJ0tZnVYp5a+0xK6CRmYmPPywnX/0UWjUqPD1lfJXmthV0Jg/39ap16sHTzzhdjRK\nlR1N7CooJCTAY4/Z+ZdfhipV3I1HqbKkiV0FhUcfhYMHoVs3e9FUqUCmF09VwFu3zlbDVKgAs2aB\nSP7r3XfffV6NS6myooldBbSUFMgajW78eLjwwoLXHZI1Fp5Sfk6rYlRAmzwZtm2zNyQ9/njh6+7Z\ns4c9e/YUvpJSfkBL7CpgffstvPiirXqZPdtWxRQmayBqbceu/J2W2FVASkqCv/8dMjJs2/XOnd2O\nSCnv0cSuAtKTT0JsLLRoYUvtSgUTTewq4ERFwRtv2K54Fyyw3QcoFUw0sauAEh8PWb3vPvMM/O1v\nroajlCv04qkKGBkZ9uajvXuhU6fTg1QX19ixY8smMKW8TBO7ChgvvgiffAK1a8OSJSUfFalPnz5l\nE5hSXqZVMSogREXZAalFYNEiaNiw5PuIjY0lNjbW8diU8jYtsSu/FxcHt94KxsDEiXDddWe2n3s8\nt6hqO3bl77TErvzaiRPQt6+9aNq9u03sSgU7TezKb2Vm2oul338PF1wAixdDaKjbUSnlPk3sym+N\nGwcffgjVq8OqVVCrltsRKeUbNLErv/T22zB1qi2hL10KzZu7HZFSvkMvniq/8+GHcPfddv7NN+3g\nGU6YMGGCMztSymWa2JVf+ewzGDLE1q9PnHi6r3UndO/e3bmdKeUirYpRfmPTJujXD1JT4cEHbbt1\nJ8XExBATE+PsTpVygZbYlV/YvBl69rTNG4cNgxkzCh7i7kyNGTMG0Hbsyv+VqsQuIoNEZIuIZIpI\nG6eCUiq7jRuha1c4fBhuvBH+/W8I0d+aShWotB+Pn4H+wHoHYlEqj/Xr7Z2kR4/CoEEQGQlhYW5H\npZRvK1VVjDHmFwBx+jexUsCaNTBgAJw8aW9Emjev5B17KRWMvPaDVkRGiki0iETHx8d767DKT/3r\nX9Cnj03qd94JERGa1JUqriI/KiISBdTLZ9F4Y8xHxT2QMWYOMAegTZs2ptgRqqCS1ZHXpEn28fjx\n8MILzl8ozc/kyZPL/iBKeUGRid0Yo417lVckJdkbj959195ROnMmjBzpveN36tTJewdTqgzpj1vl\nE37/Hfr3h5gYqFzZDpRxww3ejWHDhg2AJnjl/0qV2EXkJuANoA7wHxGJMcb0cCQyFTQ+/RRuvhkO\nHbK9NC5fDq1aeT+OcePGAdqOXfm/Ul08NcYsN8Y0NMZUMMacrUldlURamu2hsUcPm9R79YLvvnMn\nqSsVSLQqRrli+3a47Tb49lt7s9HEiXbSG4+UKj1N7MqrjIG5c+GRR+D4cWjUyI5RetVVbkemVODQ\nxK68ZscO2+rliy/s40GDYPZsqFHD3biUCjSa2FWZS02F11+HZ56xNxzVqQNvvAGDB3unfXpxzZgx\nw+0QlHKEJnZVpj7+GEaPhm3b7OOhQ+G116B2bXfjyk/r1q3dDkEpR+ilKlUmtm6Fvn1tV7vbtkGz\nZjbJL1jgm0kdICoqiqioKLfDUKrUtMSuHLV7tx0AY/58O8pRlSq2Cuahh6B8ebejK9wkTz8GOpKS\n8nea2JUjdu6E6dNti5fUVNth1z33wNNPQ/36bkenVHDRxK5K5YcfYOpU2wVAZqZ97pZb4Pnn7V2k\nSinv08SuSiwz0w4qPWMGrF5tnytXzg5Z9/jj0KKFu/EpFew0satiO3DADnYxZ46tegEID7dt0x95\nBM491934lFKWJnZVqLQ0+Pxzm9A/+MA+BpvE774b7r3Xd1u5lNTs2bPdDkEpR2hiV3lkZsLXX8N7\n78H770NCgn0+JMQ2YbznHttxV2iou3E6rXnz5m6HoJQjNLErAFJS7MDRq1bZknlc3OllF10Et94K\nw4fbvl0C1cqVKwHo06ePy5EoVTqa2IPYvn2wdq1N5mvX2k65spx3nu0j/ZZb4NJLfevW/7Iyffp0\nQBO78n+a2INIQgKsW2c74fr8c/j115zLL70Ueve2g0i3bx8cyVypQKSJPUClp8PPP8OmTaenrVtz\nrlO5MnTubJN5797aqkWpQKGJPQCkptrS908/2RuGNm2C6Gg7OHR2FSrAFVdA165wzTXQti2EhbkT\ns1Kq7Ghi9yMpKXbQ523bbGn8p5/sFBtrS+i5nX8+dOhgq1Xat4fWrW1yV0oFNk3sPsQYO/bnnj22\nM62dO+G3305Pu3efvm0/OxF7+/4ll9h68rZtoV072++5Kr4FCxa4HYJSjtDE7iUpKfbOzf377fTX\nX7ZJ4e7dpxP5nj15q0+yCwmBJk3gwgvtbfuXXGKnFi1sfbkqnUaB3JZTBRVN7CVkDJw4AYcP29L1\n4cP5zycknE7iBw7AkSPF2/9ZZ9mLmI0aQePGNolnTU2aaFVKWVqyZAkAQ4YMcTkSpUonIBN7ejok\nJ9vp5Mmcf3PPJyXBsWO2DXf2v/k9d/w4JCaevq2+JEJDoW5dOPvs038bNbJTViI/91yoVs3586GK\nZ9asWYAmduX/XEns+/bBxIk2QToxpabmTNj5XUh0UqVKdgDmmjXt3/zma9a0yTtrqlnTVqUopVRZ\ncyWxV/kzlq4vdMnxXCSDmcsoKpHEanrl2SaC4cxnOLVIYCkD8yyfxX1EMoSG7GEBwwgJsYk01PN3\nQd2xbKrbhwszY3ni93tOLQ8JgXKh8GXnCexv1Z0miTFcs2IMoaH2+VDPlPjEZMKu7kS1LRuo8Ny4\nnAdPBJ6bYZudREWBZySeHGbPhubNYeVKOyJFbgsW2GL7kiXgKTnmsHSp7W0rIsJOua1ebbtanDkT\nIiPzLl+3zv6dNs3eappdpUqwZo2df+EF2ydvdrVqwbJldv6pp2DjxpzLGzaEhQvt/JgxEBOTc3mz\nZrZLSICRI08PgJqldWvbBzDYQVGz92cA0LEjTJli5wcMgIMHcy7v1s2O6AF2LL6TJ3Mu790bHn3U\nznfpQh6DB8OoUfbK9I8/5l1n+HA7JSTAwLzvPe67D4YMsRdJhg3Lu3zsWHvXV2ys7WgntwkToHt3\ne97GjMm7fPJk6NQJNmyAcePyLp+h7z3Av997SUnQK2/eK/K9VwBXEnv58tC4PkiIbdERIjDkcmjd\nDSpmwsWv2+ezL3/8erizN1Q6ARdNyLv8sjvhXzdDxXgIGwG5b5psPxboA8QC+Xy2LhwBdAdigG/z\nLq/SAKgLbHf0VCillOPEGOP1g7Zp08ZER0d7/bhKFaaLp0S1LquEqZSPEZHNxpg2Ra0XkBdPlToT\nS5cudTsEpRyhiV0pj9qBMmKICnraTkMpj4iICCLyuziolJ/RxK6UhyZ2FShKldhF5BUR+VVEfhSR\n5SJS3anAlFJKnZnSltg/BVoZYy4FtgFPlT4kpZRSpVGqxG6M+cQYk3Wf5zdAw9KHpJRSqjScrGO/\nE1jj4P6UUkqdgSKbO4pIFFAvn0XjjTEfedYZD6QDiwrZz0hgJMC5Ogab8kGrV692OwSlHFHqO09F\n5HbgXqCbMaaQ3sRP0ztPlVKq5Lxy56mIXA88AVxd3KSulK+aOXMmAKNGjXI5EqVKp7R17P8EzgI+\nFZEYEXnLgZiUckVkZCSR+fVOqJSfKVWJ3RhzgVOBKKWUcobeeaqUUgFGE7tSSgUYTexKKRVgXBlo\nQ0TigT+8fuCcagMJLsfgK/RcnKbn4jQ9F6f5yrk4zxhTp6iVXEnsvkBEoovTHjQY6Lk4Tc/FaXou\nTvO3c6FVMUopFWA0sSulVIAJ5sQ+x+0AfIiei9P0XJym5+I0vzoXQVvHrpRSgSqYS+xKKRWQNLED\nIvKoiBgRCdph6nWYQ9upnYjEish2EXnS7XjcIiKNROQLEflFRLaIyGi3Y3KbiISKyPcissrtWIoj\n6BO7iDQCrgV2ux2Ly4J6mEMRCQXeBHoCLYBbRKSFu1G5Jh0Ya4y5GOgA3B/E5yLLaOAXt4MorqBP\n7MBrwONAUF9s0GEOaQdsN8bsNMakAouBfi7H5ApjzD5jzP8888ewCa2Bu1G5R0QaAjcAc92OpbiC\nOrGLSF9grzHmB7dj8THBOMxhA2BPtsdxBHEyyyIijYHLgE3uRuKqGdjCX6bbgRRXqbrt9QeFDe0H\njAOu825E7nFqmMMAJfk8F9S/4kSkCrAMGGOMOep2PG4Qkd7AAWPMZhHp4nY8xRXwid0Y0z2/50Xk\nEqAJ8IOIgK16+J+ItDPG/OXFEL2moHORxTPMYW/sMIfBltTigEbZHjcE/nQpFteJSBg2qS8yxnzg\ndjwuugLoKyK9gIpAVRFZaIwZ6nJchdJ27B4isgtoY4zxhY5+vM4zzOGr2GEO492Ox9tEpBz2onE3\nYC/wHXCrMWaLq4G5QGxJZz5wyBgzxu14fIWnxP6oMaa327EUJajr2FUOQT3MoefC8QPAWuzFwshg\nTOoeVwDDgK6e90KMp8Sq/ISW2JVSKsBoiV0ppQKMJnallAowmtiVUirAaGJXSqkAo4ldKaUCjCZ2\npZQKMJrYlVIqwGhiV0qpAPP/UDEM0Rh8+NwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x297003bb780>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sz = selu(z)\n",
    "dsz = np.gradient(sz, z)\n",
    "\n",
    "plt.plot(z, sz, \"b-\", linewidth=2)\n",
    "plt.plot(z, dsz, \"g-\", linewidth=2)\n",
    "plt.plot([-5, 5], [0, 0], 'k--')\n",
    "plt.plot([-5, 5], [-1.758, -1.758], 'r--')\n",
    "plt.plot([0, 0], [-2.2, 3.2], 'k--')\n",
    "plt.title(r\"Funcao de ativacao SELU ($\\alpha \\approx 1.67$)\", fontsize=14)\n",
    "plt.axis([-5, 5, -2.2, 3.2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A SeLU tem a propriedade de preservar a variância em torno de 1 (média 0), para redes com muitas camadas (centenas delas), evitando assim o problema de desaparecimento e explosão de gradientes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 0: -0.26 < media < 0.27, 0.74 < stddev < 1.27\n",
      "Layer 20: -0.17 < media < 0.18, 0.74 < stddev < 1.24\n",
      "Layer 40: -0.38 < media < 0.39, 0.74 < stddev < 1.25\n",
      "Layer 60: -0.26 < media < 0.43, 0.74 < stddev < 1.35\n",
      "Layer 80: -0.18 < media < 0.16, 0.72 < stddev < 1.19\n",
      "Layer 100: -0.26 < media < 0.32, 0.74 < stddev < 1.26\n",
      "Layer 120: -0.19 < media < 0.20, 0.78 < stddev < 1.36\n",
      "Layer 140: -0.24 < media < 0.34, 0.74 < stddev < 1.14\n",
      "Layer 160: -0.27 < media < 0.21, 0.72 < stddev < 1.22\n",
      "Layer 180: -0.35 < media < 0.35, 0.77 < stddev < 1.41\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "Z = np.random.normal(size=(500, 100))\n",
    "for layer in range(200):\n",
    "    W = np.random.normal(size=(100, 100), scale = np.sqrt(1./100))\n",
    "    Z = selu(np.dot(Z, W))\n",
    "    means = np.mean(Z, axis=1)\n",
    "    stds = np.std(Z, axis=1)\n",
    "    if layer % 20 == 0:\n",
    "        print(\"Layer {}: {:.2f} < media < {:.2f}, {:.2f} < stddev < {:.2f}\".format(\n",
    "            layer, means.min(), means.max(), stds.min(), stds.max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Activation(object):\n",
    "    \"\"\"Funcao de ativacao\"\"\"\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "                \n",
    "    def init(self, n_inputs, n_outputs):        \n",
    "        if self.name == 'selu':\n",
    "            stddev = np.sqrt(1./n_inputs)\n",
    "        elif self.name == 'sigmoid':\n",
    "            stddev = np.sqrt(2. / (n_inputs + n_outputs))\n",
    "        else:\n",
    "            stddev = 2. / np.sqrt(n_inputs + n_outputs)\n",
    "        return tf.truncated_normal((n_inputs, n_outputs), stddev = stddev)\n",
    "        \n",
    "    def fire(self, ypred):\n",
    "        def selu(z, scale=1.0507009873554804934193349852946,\n",
    "                 alpha=1.6732632423543772848170429916717):\n",
    "            return scale * tf.where(z >= 0.0, z, alpha * tf.nn.elu(z))        \n",
    "        \n",
    "        if self.name == 'sigmoid':\n",
    "            return tf.nn.sigmoid(ypred) \n",
    "        elif self.name == 'relu':\n",
    "            return tf.nn.relu(ypred)\n",
    "        elif self.name == 'elu':\n",
    "            return tf.nn.elu(ypred)\n",
    "        elif self.name == 'selu':\n",
    "            return selu(ypred)\n",
    "        else:\n",
    "            return ypred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SNN(FeedforwardNeuralNet):\n",
    "    \"\"\"Rede neural sequencial auto-normalizavel\"\"\"\n",
    "    def __init__(self, input_dim):\n",
    "        FeedforwardNeuralNet.__init__(self, input_dim)\n",
    "        \n",
    "    def fit(self, train_data, n_epochs, batch_size):\n",
    "        \"\"\"Executa treino da rede neural\"\"\"\n",
    "        self.means = train_data.images.mean(axis=0, keepdims=True)\n",
    "        self.stds = train_data.images.std(axis=0, keepdims=True) + 1e-10\n",
    "\n",
    "        num_batches = train_data.num_examples // batch_size\n",
    "        with tf.Session() as s:\n",
    "            s.run(self.init_op)\n",
    "            for e in range(n_epochs):\n",
    "                tloss = 0.\n",
    "                for i in range(num_batches):\n",
    "                    X_b, y_b = train_data.next_batch(batch_size)\n",
    "                    # scale data!\n",
    "                    X_b_scaled = (X_b - self.means) / self.stds\n",
    "                    _, loss_e = s.run([self.train_op, self.lossf], \n",
    "                                      feed_dict = {self.X: X_b_scaled, self.y: y_b})\n",
    "                    tloss += loss_e\n",
    "                acc_train = s.run(self.acc, \n",
    "                                  feed_dict = {self.X: X_b_scaled, self.y: y_b})\n",
    "                print('%2d loss: %.8f acc: %.2f' % (e, tloss/num_batches, acc_train))\n",
    "            self.saver.save(s, '/tmp/model.ckpt')\n",
    "            \n",
    "    \n",
    "    def evaluate(self, X_test, y_test):\n",
    "        \"\"\"Avalia rede neural em colecao de teste\"\"\"\n",
    "        with tf.Session() as s:\n",
    "            self.saver.restore(s, '/tmp/model.ckpt')\n",
    "            X_test_scaled = (X_test - self.means) / self.stds\n",
    "            acc_test = s.run(self.acc, \n",
    "                             feed_dict = {self.X: X_test_scaled, self.y: y_test})\n",
    "        return acc_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "model = SNN(input_dim = 28*28)\n",
    "\n",
    "model.add(units=300, activation = Activation('selu'), name = 'h1')\n",
    "model.add(units=100, activation = Activation('selu'), name = 'h2')\n",
    "model.add(units=10, name = 'out')\n",
    "\n",
    "model.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0 loss: 0.28533542 acc: 0.98\n",
      " 1 loss: 0.13552079 acc: 1.00\n",
      " 2 loss: 0.09023499 acc: 1.00\n",
      " 3 loss: 0.06528229 acc: 1.00\n",
      " 4 loss: 0.04601010 acc: 1.00\n",
      " 5 loss: 0.03279021 acc: 1.00\n",
      " 6 loss: 0.02714882 acc: 1.00\n",
      " 7 loss: 0.01862058 acc: 1.00\n",
      " 8 loss: 0.01313842 acc: 1.00\n",
      " 9 loss: 0.00903148 acc: 1.00\n"
     ]
    }
   ],
   "source": [
    "model.fit(mnist.train, n_epochs = 10, batch_size = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/model.ckpt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9753"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(mnist.test.images, mnist.test.labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Antes da introdução das SELUs, BN era a técnica preferível para lidar com o problema de deriva de co-variância. Atualmente, há uma tendência para preferir a solução com SELUs, pois:\n",
    "\n",
    "* A propriedade de auto-normalização de SELUs é baseada em um arcabouço matemático sólido;\n",
    "* Elas são bem mais simples que BN, uma vez que a função de ativação já produz saídas normalizadas;\n",
    "* Há razoável comprovação empiríca (na medida em que é possível dado o pouco tempo de existência das SELUs) de que elas atingem maior acuidade em menos tempo, quando aplicadas a redes com várias camadas, em comparação a ReLUs, ELUs e BN. \n",
    "\n",
    "_Note que BNs ainda podem ser úteis onde SELUs não puderem ser usadas, como qualquer rede em que uma função muito particular de perda tiver que ser usada que não case bem com SELUs (embora este seja o caso de LSTMs, há atualmente um esforço grande em adaptar SELUs para LSTMs). Além disso, BN também opera como regularizador, o que pode resultar em melhores desempenhos em arquiteturas como as CNNs. A seguir, temos uma explicação um pouco melhor sobre BNs. Vamos ver exemplos de uso mais à frente, com Keras._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batch normalization\n",
    "\n",
    "Este método foi descrito por [Ioffe and Szegedy](https://arxiv.org/abs/1502.03167) para acelerar o treinamento de redes neurais. Ele é baseada num simples modo de falha que atrapalha o treinamento de redes neurais: _à medida que o sinal se propaga pela rede, mesmo se normalizado na entrada, ele pode acabar completamente enviesado em alguma camada oculta, tanto em termos de variância quanto média_ (efeito conhecido como *deriva interna da covariância*). Isto resulta em grandes discrepâncias entre as atualizações de gradientes ao longo de diferentes camadas. Como resultado, somos mais conservadores com a taxa de aprendizado e aplicamos regularizadores mais fortes, o que desacelera o treino.\n",
    "\n",
    "Batch normalization propõe a normalização das ativações de uma camada para média zero e variância 1, através do batch de dados passando pela rede (ou seja, no treino, nós normalizamos pelos `batch_size` exemplos e, no teste, normalizamos considerando estatísticas derivadas do _treino todo_---uma vez que não conhecemos os dados de teste com antecedência). Nós calculamos a média e a variância para um batch particular de ativações $\\mathcal{B} = \\{x_1, \\dots, x_m\\}$ assim:\n",
    "\n",
    "$$\\begin{align*}\\mu_{\\mathcal{B}} &= \\frac{1}{m}\\sum_{i=1}^{m}x_i \\\\ \\sigma_\\mathcal{B}^2 &= \\frac{1}{m}\\sum_{i=1}^{m}\\left(x_i - \\mu_\\mathcal{B}\\right)^2\\end{align*}$$\n",
    "\n",
    "Nós então usamos estas estatísticas para transformar as ativações de forma que elas tenham média zero e variância um:\n",
    "\n",
    "$$\\hat{x}_i = \\frac{x_i - \\mu_\\mathcal{B}}{\\sqrt{\\sigma_\\mathcal{B}^2 + \\varepsilon}}$$\n",
    "\n",
    "onde $\\varepsilon > 0$ é um pequeno valor que nos protege de divisão por zero (no caso do desvio padrão do batch ser muito pequeno ou mesmo zero). Finalmente, para obter a ativação final $y$, precisamos estar certos que nenhuma propriedade de generalizaçao foi perdida ao executar a normalização---e desde que as operações feitas foram um deslocamento (média) e um escalonamento (desvio), nós permitimos um deslocamento e um escalonamento arbitrários dos valores normalizados para obter o valor final (isso permite a rede, por exemplo, voltar aos valores originais se ela os considerar mais úteis):\n",
    "\n",
    "$$y_i = \\gamma\\hat{x}_i + \\beta$$\n",
    "\n",
    "onde $\\beta$ e $\\gamma$ são parâmetros *treináveis* da operação de batch normalization (otimizáveis via gradiente descendente nos dados de treino). Esta generalização significa que batch normalisation pode se aplicada diretamente às _entradas_ da rede neural (dado que a presença desses parâmetros permite à rede assumir uma estatística de entrada diferente da que nós selecionamos através do processamento manual dos dados).\n",
    "\n",
    "Este método, quando aplicado às camadas de uma rede convolutiva quase sempre levam a maior velocidade do aprendizado. Eles também agem como ótimos regularizadores, nos permitindo um cuidado maior na escolha da taxa de aprendizado, na importância do $L_2$ e uso do dropout (tornando-o muitas vezes completamente desnecessário). Esta regularização ocorre como consequência do fato de que a saída de um único exemplo *não é mais determinística* (já que ela depende do batch inteiro a qual ela pertence), ajudando a rede a generalizar melhor.\n",
    "\n",
    "Note que no artigo original, os autores usam batch normalization *antes* de aplicar a função de ativação do neurônio (nas combinações lineares computadas dos dados de entrada). Contudo, [em resultados recentes](http://arxiv.org/abs/1511.06422) observou-se que poderia ser mais benéfico (e, no mínimo, tao bom quanto) aplicá-la *depois*.\n",
    "\n",
    "Em Keras, batch normalisation corresponde a uma camada: `BatchNormalization`, para a qual podemos fornecer alguns parâmetros. O mais importante deles é o `axis` (sobre que eixo dos dados as estatísticas deveriam ser computadas). Em particular, quando trabalhando com camadas de convolução, queremos normalizar através dos canais individuais, logo `axis = 1`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Otimizadores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Os otimizadores a serem usados com redes neurais são os mesmos que vimos antes, uma vez que o gradiente descendente é um caso particular da propagação retrógrada. \n",
    "\n",
    "Dos vários métodos que vimos antes, a opção em geral será pelo Adam, que combina momento com taxas de aprendizado pora cada dimensão (adaptativo). Eventualmente, quando se quiser um modelo muito esparso (e rápido no teste), deve-se optar pelo FTRLOpimizer (junto com uma regularização $L_1$ -- a ser visto mais tarde)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Optimizer(object):\n",
    "    def __init__(self, name = 'sgd'):\n",
    "        self.name = name\n",
    "        self.lrate = 0.1\n",
    "\n",
    "    def get(self, lossf):\n",
    "        if self.name == 'sgd':\n",
    "            opt = tf.train.GradientDescentOptimizer(learning_rate = self.lrate) \n",
    "        elif self.name == 'adam':\n",
    "            opt = tf.train.AdamOptimizer() \n",
    "        return opt.minimize(lossf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "model = FeedforwardNeuralNet(input_dim = 28*28)\n",
    "\n",
    "model.add(units=300, activation = Activation('relu'), name = 'h1')\n",
    "model.add(units=100, activation = Activation('relu'), name = 'h2')\n",
    "model.add(units=10, name = 'out')\n",
    "\n",
    "model.compile(optimizer = 'adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0 loss: 0.22149987 acc: 0.96\n",
      " 1 loss: 0.09076463 acc: 0.96\n",
      " 2 loss: 0.05697486 acc: 1.00\n",
      " 3 loss: 0.04194708 acc: 0.98\n",
      " 4 loss: 0.03391871 acc: 1.00\n",
      " 5 loss: 0.02644770 acc: 1.00\n",
      " 6 loss: 0.02029505 acc: 1.00\n",
      " 7 loss: 0.02090722 acc: 1.00\n",
      " 8 loss: 0.01641303 acc: 1.00\n",
      " 9 loss: 0.01407519 acc: 1.00\n"
     ]
    }
   ],
   "source": [
    "model.fit(mnist.train, n_epochs = 10, batch_size = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/model.ckpt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9764"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(mnist.test.images, mnist.test.labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient clipping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Antes de BNs e SNNs, era comum o uso de corte de gradientes para evitar estouro de gradientes. Atualmente, esquemas de normalização são preferidos. Contudo, é importante entender esta técnica, especialmente para o caso onde a normalização não puder ser trivialmente aplicada, como LSTMs.\n",
    "\n",
    "A ideia de clipping é cortar todos os gradientes que estão abaixo ou acima de valores críticos, de forma que eles assumam os valores críticos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Optimizer(object):\n",
    "    def __init__(self, name = 'sgd'):\n",
    "        self.name = name\n",
    "        self.lrate = 0.1\n",
    "        self.threshold = 1\n",
    "\n",
    "    def get(self, lossf):\n",
    "        if self.name == 'sgd':\n",
    "            opt = tf.train.GradientDescentOptimizer(learning_rate = self.lrate) \n",
    "            return opt.minimize(lossf)\n",
    "        \n",
    "        elif self.name == 'clipped_sgd':\n",
    "            opt = tf.train.GradientDescentOptimizer(learning_rate = self.lrate) \n",
    "            grads_vars = opt.compute_gradients(lossf)\n",
    "            clipped_vals = [(tf.clip_by_value(grad, -self.threshold, self.threshold), var) \n",
    "                            for grad, var in grads_vars]\n",
    "            return opt.apply_gradients(clipped_vals)\n",
    "        \n",
    "        elif self.name == 'adam':\n",
    "            opt = tf.train.AdamOptimizer() \n",
    "            return opt.minimize(lossf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "model = FeedforwardNeuralNet(input_dim = 28*28)\n",
    "\n",
    "model.add(units=300, activation = Activation('relu'), name = 'h1')\n",
    "model.add(units=100, activation = Activation('relu'), name = 'h2')\n",
    "model.add(units=10, name = 'out')\n",
    "\n",
    "model.compile(optimizer = 'clipped_sgd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0 loss: 0.27929616 acc: 1.00\n",
      " 1 loss: 0.12198854 acc: 1.00\n",
      " 2 loss: 0.08302177 acc: 1.00\n",
      " 3 loss: 0.06140562 acc: 1.00\n",
      " 4 loss: 0.04680037 acc: 1.00\n",
      " 5 loss: 0.03567845 acc: 1.00\n",
      " 6 loss: 0.02822019 acc: 1.00\n",
      " 7 loss: 0.02090556 acc: 1.00\n",
      " 8 loss: 0.01595843 acc: 1.00\n",
      " 9 loss: 0.01254436 acc: 1.00\n"
     ]
    }
   ],
   "source": [
    "model.fit(mnist.train, n_epochs = 10, batch_size = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/model.ckpt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9781"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(mnist.test.images, mnist.test.labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Política de evolução de taxa de aprendizado (_learning rate schedulling_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A ideia aqui é modificar a taxa de aprendizado de acordo com uma política de evolução em lugar de usar uma taxa fixa. Naturalmente, esta é uma possibilidade apenas para os métodos não adaptativos, uma vez que estes modificam a taxa de aprendizado de acordo com o processo de otimização.\n",
    "\n",
    "Para os métodos em que tais políticas fazem sentido, as principais estratégias são:\n",
    "\n",
    "* _Modificação em intervalos pre-determinados_: por exemplo, inicie com 0.1 e, então, após 50 épocas, mude para 0.001. Complicado aqui é definir que valores usar e quando mudar.\n",
    "\n",
    "* _Modificação de acordo com desempenho_: a cada N passos, avalie o erro no conjunto de validação e, se este parou de cair, reduza a taxa por um fator $\\lambda$.\n",
    "\n",
    "* _Escala exponencial_: diminua a taxa de aprendizado por um fator de 10 a cada $r$ passos. Assm, dado o passo $t$, um valor inicial $\\eta_0$, $\\eta(t) = \\eta_0 10^{\\frac{-t}{r}}$\n",
    "\n",
    "* _Escala de potência_: equivalente ao anterior, mas bem mais lento. Nesse caso, $\\eta(t) = \\eta_0 (1 + \\frac{t}{r})^{-c}$, onde $c$ é tipicamente definido como 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Optimizer(object):\n",
    "    def __init__(self, name = 'sgd'):\n",
    "        self.name = name\n",
    "        self.lrate = 0.1\n",
    "        # clipping\n",
    "        self.threshold = 1\n",
    "        # scheduling\n",
    "        self.initial_lrate = 0.1 # eta0\n",
    "        self.decay_steps = 10000 # r\n",
    "        self.decay_rate = 1./10 \n",
    "        self.global_step = tf.Variable(0, trainable = False) ##\n",
    "\n",
    "    def get(self, lossf):\n",
    "        if self.name == 'sgd':\n",
    "            opt = tf.train.GradientDescentOptimizer(learning_rate = self.lrate) \n",
    "            return opt.minimize(lossf)\n",
    "        \n",
    "        elif self.name == 'clipped_sgd':\n",
    "            opt = tf.train.GradientDescentOptimizer(learning_rate = self.lrate) \n",
    "            grads_vars = opt.compute_gradients(lossf)\n",
    "            clipped_vals = [(tf.clip_by_value(grad, -self.threshold, self.threshold), var) \n",
    "                            for grad, var in grads_vars]\n",
    "            return opt.apply_gradients(clipped_vals)\n",
    "        \n",
    "        elif self.name == 'exp_decay_momentum':\n",
    "            lrate = tf.train.exponential_decay(self.initial_lrate,\n",
    "                                              self.global_step,\n",
    "                                              self.decay_steps,\n",
    "                                              self.decay_rate)\n",
    "            opt = tf.train.MomentumOptimizer(learning_rate = lrate, momentum = 0.9)  ##\n",
    "            return opt.minimize(lossf, global_step = self.global_step) ##\n",
    "        \n",
    "        elif self.name == 'adam':\n",
    "            opt = tf.train.AdamOptimizer() \n",
    "            return opt.minimize(lossf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "model = FeedforwardNeuralNet(input_dim = 28*28)\n",
    "\n",
    "model.add(units=300, activation = Activation('relu'), name = 'h1')\n",
    "model.add(units=100, activation = Activation('relu'), name = 'h2')\n",
    "model.add(units=10, name = 'out')\n",
    "\n",
    "model.compile(optimizer = 'exp_decay_momentum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0 loss: 0.23740909 acc: 0.98\n",
      " 1 loss: 0.09769055 acc: 0.98\n",
      " 2 loss: 0.05691636 acc: 0.98\n",
      " 3 loss: 0.03441181 acc: 0.98\n",
      " 4 loss: 0.01844564 acc: 1.00\n",
      " 5 loss: 0.01017675 acc: 1.00\n",
      " 6 loss: 0.00636753 acc: 1.00\n",
      " 7 loss: 0.00395000 acc: 1.00\n",
      " 8 loss: 0.00298685 acc: 1.00\n",
      " 9 loss: 0.00240513 acc: 1.00\n"
     ]
    }
   ],
   "source": [
    "model.fit(mnist.train, n_epochs = 10, batch_size = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/model.ckpt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9836"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(mnist.test.images, mnist.test.labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combatendo _overfitting_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dada a complexidade de modelos profundos, é comum que os modelos se super-especializem no treino. Para verificar se isso está acontecendo, é recomendado durante o treino avaliar o modelo tanto na coleção de treino quanto em uma coleção de validação (separada da coleção de treino e diferente da de teste). Se as taxas de erro diferem, com o erro do treino menor que na validação, está acontecendo _overfitting_.\n",
    "\n",
    "Para lidar com esse problema, as seguintes técnicas podem ser aplicadas:\n",
    "\n",
    "* _early stopping_: parar o processo de treino quando nenhum novo modelo supera o melhor observado após N épocas;\n",
    "\n",
    "* uso de regularizadores (como $L_1$ e $L_2$): reguladorizadores penalizam certos conjuntos de pesos, obrigando o modelo a encontrar soluções \"melhores\". Por exemplo, regularizadores $L_1$ tendem a gerar um conjunto esparso de pesos enquanto $L_2$ penaliza outliers.\n",
    "\n",
    "* _dropout_: neurônios são retirados aleatoriamente do modelo (também atributos de entrada), de forma que nenhum neurônio pode se co-adaptar a outro em particular. De certa forma, isso simula a natureza estocástica de redes neurais biológicas.\n",
    "\n",
    "* introdução de ruído/mais casos no treino: aumento dos casos de treino com casos ruidosos (por exemplo, uma versão embaçada de uma imagem), ou variantes prováveis (ex: a imagem de ponta a cabeça, um fragmento de imagem, a imagem rotacionada, etc)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regularizador $L_1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A função de perda $\\ell(\\hat{y}({\\bf w}), {\\bf y})$ pode ser minimizada com _qualquer_ conjunto de pesos ${\\bf w}$. Contudo, certos conjuntos de pesos podem envolver pesos muito mais importantes que outros (_outliers_), configurações onde todos os pesos são relevantes (não esparsos), etc. Uma função regularizadora $R$ pode ser usada para penalizar esses conjuntos, melhorando a possibiliade de generalização do modelo seja por simplificá-lo ou remover _outliers_, por exemplo. \n",
    "\n",
    "Incorporando $R$, temos a nova função de perda:\n",
    "\n",
    "$$L(\\hat{y}({\\bf w}), {\\bf y}) = \\ell(\\hat{y}({\\bf w}), {\\bf y}) + \\lambda R({\\bf w})$$\n",
    "\n",
    "onde $\\lambda$ é o fator que indica a importância do regularizador para o custo final. Dois regularizadores comuns são:\n",
    "\n",
    "* $L_1 = R({\\bf w}) = \\sum_i{|{\\bf w}_i|}$ -- prefere conjuntos de pesos esparsos. Este conjunto tem certa motivação biológica, uma vez que redes neurais biológicas são esparsas. Do ponto de vista estatístico, pesos esparsos levam a um modelo mais simples e, portanto, mais fácil de generalizar.\n",
    "\n",
    "* $L_2 = R({\\bf w}) = \\sum_i{{\\bf w}_i^2}$ -- prefere conjuntos de pesos densos, sem _outliers_. A ideia desta penalização é que confiar em conjuntos de pesos esparsos, baseado em amostras (provavelmente pequenas, como as vindas de pequenos lotes de instâncias, normalmente usadas em algoritmos da família do gradiente descendente), não é seguro, uma vez que os pesos relevantes podem ser _outliers_.\n",
    "\n",
    "A seguir, mudamos nossa implementação para incorporar um regularizador $L_1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LossFunction(object):\n",
    "    def __init__(self, name = 'sigmoid', reg_lambda = 0.001):\n",
    "        self.name = name\n",
    "        self.reg_lambda = reg_lambda # regularizer importance factor\n",
    "\n",
    "    # get needs to access layers' weights\n",
    "    def get(self, yreal, ypred, layers = None):\n",
    "        reg_losses = 0. # reg loss component\n",
    "        if self.name == 'sigmoid' or self.name == 'sigmoid_l1':\n",
    "            loss = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "                labels = yreal, logits = ypred) \n",
    "            # L1 regularization\n",
    "            if layers != None and self.reg_lambda > 0:\n",
    "                for layer in layers:\n",
    "                    reg_losses += tf.reduce_sum(tf.abs(layer.W))\n",
    "                reg_losses *= self.reg_lambda\n",
    "        return tf.reduce_mean(loss, name = 'lossf') \\\n",
    "                + reg_losses # reg loss component\n",
    "    \n",
    "def compile(self, loss = 'sigmoid', optimizer = 'sgd'):\n",
    "    \"\"\"Cria grafo da rede neural\"\"\"\n",
    "    self.X = tf.placeholder(tf.float32, \n",
    "                       shape = (None, self.input_dim), \n",
    "                       name = 'X')\n",
    "    self.y = tf.placeholder(tf.int64, shape = (None), name = 'y')\n",
    "\n",
    "    # cria layers\n",
    "    with tf.name_scope('layers'):\n",
    "        layer_in = self.X\n",
    "        for layer in self.layers:\n",
    "            layer_out = layer.output(layer_in)\n",
    "            layer_in = layer_out\n",
    "\n",
    "    # loss function\n",
    "    with tf.name_scope('loss'):\n",
    "        self.lossf = LossFunction(loss).get(self.y, layer_out, self.layers)\n",
    "\n",
    "    # optimizer\n",
    "    with tf.name_scope('train'):\n",
    "        self.train_op = Optimizer(optimizer).get(self.lossf)\n",
    "\n",
    "    # evalution metrics\n",
    "    with tf.name_scope('eval'):\n",
    "        correct = tf.nn.in_top_k(layer_out, self.y, 1)\n",
    "        self.acc = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "    self.init_op = tf.global_variables_initializer()\n",
    "    self.saver = tf.train.Saver()\n",
    "    \n",
    "FeedforwardNeuralNet.compile = compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "model = FeedforwardNeuralNet(input_dim = 28*28)\n",
    "\n",
    "model.add(units=300, activation = Activation('relu'), name = 'h1')\n",
    "model.add(units=100, activation = Activation('relu'), name = 'h2')\n",
    "model.add(units=10, name = 'out')\n",
    "\n",
    "model.compile(optimizer = 'exp_decay_momentum', loss = 'sigmoid_l1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0 loss: 1.54115056 acc: 0.96\n",
      " 1 loss: 0.76100288 acc: 0.98\n",
      " 2 loss: 0.67501564 acc: 0.90\n",
      " 3 loss: 0.61056897 acc: 0.98\n",
      " 4 loss: 0.56673771 acc: 0.96\n",
      " 5 loss: 0.52828478 acc: 0.92\n",
      " 6 loss: 0.49711648 acc: 0.94\n",
      " 7 loss: 0.47358599 acc: 0.96\n",
      " 8 loss: 0.45515542 acc: 0.92\n",
      " 9 loss: 0.43909497 acc: 1.00\n"
     ]
    }
   ],
   "source": [
    "model.fit(mnist.train, n_epochs = 10, batch_size = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/model.ckpt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9443"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(mnist.test.images, mnist.test.labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Em redes neurais biológicas, neurônios têm natureza estocástica. Ou seja, em condições apropriadas para disparar um sinal, eles disparam apenas 50% das vezes, devido a uma série de condições bio-químicas. \n",
    "\n",
    "Uma forma de implementar isso numa rede neural artificial é meramente por eliminar, aleatoriamente, metade dos neurônios da rede. Esta eliminação pode atingir mesmo a entrada. Este processo de eliminação de neurônios durante o treino é chamado de dropout.\n",
    "\n",
    "Há algumas motivações para usar dropout:\n",
    "\n",
    "* Neurônios não podem se co-adaptar, tornando-se mais robustos;\n",
    "* Dropout pode ser visto como um tipo de ensemble, onde $2^H$ ($H$ é o número de neurônios no modelo) redes neurais são treinadas e, juntas, colaboram para a solução final; \n",
    "* Quando o dropout atinge a entrada da rede, ela também opera como um processo de aleatorização similar ao observado em random forests;\n",
    "* Do ponto de vista prático, o uso de dropout implica em aumentos de 1 a 2% no desempenho do sistema (cerca de 40% de redução em taxas de erro). Isso, contudo, _ao custo de aprendizado mais lento e complexo_.\n",
    "\n",
    "Abaixo, temos nosso código modificado para suportar dropout. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compile(self, loss = 'sigmoid', optimizer = 'sgd', \n",
    "            dropout_rate = 0.0): # dropout rate = 0 -> sem dropout\n",
    "    \"\"\"Cria grafo da rede neural\"\"\"\n",
    "    self.X = tf.placeholder(tf.float32, \n",
    "                       shape = (None, self.input_dim), \n",
    "                       name = 'X')\n",
    "    self.y = tf.placeholder(tf.int64, shape = (None), name = 'y')\n",
    "    # in_training indicates if graph is evaluated during training\n",
    "    # During training, using dropout, units are dropped (inclunding input)\n",
    "    # At the end of the process, weights are divided by 1/dropout_rate to \n",
    "    # compensate the average loss of signal resulting from dropping units\n",
    "    self.in_training = tf.placeholder_with_default(False, shape = (), \n",
    "                                                   name = 'in_training')\n",
    "\n",
    "    # cria layers\n",
    "    with tf.name_scope('layers'):\n",
    "        # dropout in input layer\n",
    "        layer_in = self.X if dropout_rate == 0.0 \\\n",
    "            else tf.layers.dropout(self.X, dropout_rate, self.in_training)\n",
    "        for layer in self.layers:\n",
    "            # dropout in other layers\n",
    "            layer_out = layer.output(layer_in) if dropout_rate == 0.0 \\\n",
    "                else tf.layers.dropout(layer.output(layer_in), \n",
    "                                       dropout_rate, self.in_training)\n",
    "            layer_in = layer_out\n",
    "\n",
    "    # loss function\n",
    "    with tf.name_scope('loss'):\n",
    "        # layers are sent to loss function so that the regularizer can be applied\n",
    "        self.lossf = LossFunction(loss).get(self.y, layer_out, self.layers)\n",
    "\n",
    "    # optimizer\n",
    "    with tf.name_scope('train'):\n",
    "        self.train_op = Optimizer(optimizer).get(self.lossf)\n",
    "\n",
    "    # evalution metrics\n",
    "    with tf.name_scope('eval'):\n",
    "        correct = tf.nn.in_top_k(layer_out, self.y, 1)\n",
    "        self.acc = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "    self.init_op = tf.global_variables_initializer()\n",
    "    self.saver = tf.train.Saver()\n",
    "    \n",
    "def fit(self, train_data, n_epochs, batch_size):\n",
    "    \"\"\"Executa treino da rede neural\"\"\"\n",
    "    num_batches = train_data.num_examples // batch_size\n",
    "    with tf.Session() as s:\n",
    "        s.run(self.init_op)\n",
    "        for e in range(n_epochs):\n",
    "            tloss = 0.\n",
    "            for i in range(num_batches):\n",
    "                X_b, y_b = train_data.next_batch(batch_size)\n",
    "                _, loss_e = s.run([self.train_op, self.lossf], \n",
    "                                  feed_dict = {self.X: X_b, self.y: y_b,\n",
    "                                              self.in_training: True})\n",
    "                tloss += loss_e\n",
    "            acc_train = s.run(self.acc, \n",
    "                              feed_dict = {self.X: X_b, self.y: y_b})\n",
    "            print('%2d loss: %.8f acc: %.2f' % (e, tloss/num_batches, acc_train))\n",
    "        self.saver.save(s, '/tmp/model.ckpt')\n",
    "    \n",
    "FeedforwardNeuralNet.compile = compile\n",
    "FeedforwardNeuralNet.fit = fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "model = FeedforwardNeuralNet(input_dim = 28*28)\n",
    "\n",
    "model.add(units=300, activation = Activation('relu'), name = 'h1')\n",
    "model.add(units=100, activation = Activation('relu'), name = 'h2')\n",
    "model.add(units=10, name = 'out')\n",
    "\n",
    "model.compile(optimizer = 'exp_decay_momentum', dropout_rate = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0 loss: 1.57503391 acc: 0.88\n",
      " 1 loss: 0.74757135 acc: 0.96\n",
      " 2 loss: 0.66619881 acc: 0.94\n",
      " 3 loss: 0.60786996 acc: 0.92\n",
      " 4 loss: 0.57249498 acc: 0.96\n",
      " 5 loss: 0.52867732 acc: 0.94\n",
      " 6 loss: 0.50075299 acc: 0.94\n",
      " 7 loss: 0.47409037 acc: 0.94\n",
      " 8 loss: 0.45423833 acc: 0.88\n",
      " 9 loss: 0.43544618 acc: 0.92\n"
     ]
    }
   ],
   "source": [
    "model.fit(mnist.train, n_epochs = 10, batch_size = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/model.ckpt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9491"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(mnist.test.images, mnist.test.labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A taxa de dropout é normalmente mantida em 0.5. Você pode aumentar esse valor se quer tentar reduzir mais o overfitting. Ou diminui-lo, se quer evitar underfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dicas gerais"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Antes da SNNs, as escolhas seriam:\n",
    "\n",
    "* _Inicialização_: Xavier ou He\n",
    "* _Função de ativação_: ELU\n",
    "* _Normalização_: Batch Normalization\n",
    "* _Regularização_: Dropout\n",
    "* _Otimizador_: Adam\n",
    "* _Escala de evolução da taxa de aprendizado_: nenhum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com SNNs, as escolhas são:\n",
    "\n",
    "* _Inicialização_: pequeno valor próximo ao mínimo da normalização mantida pela SNN\n",
    "* _Função de ativação_: SELU\n",
    "* _Normalização_: não necessária\n",
    "* _Regularização_: Dropout para SNNs (ver paper sobre SNNs)\n",
    "* _Otimizador_: Adam\n",
    "* _Escala de evolução da taxa de aprendizado_: nenhum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mas à frente, vamos melhorar mais este resultado usando uma rede convolutiva."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References:\n",
    "\n",
    "https://en.wikipedia.org/wiki/Deep_learning    \n",
    "http://sebastianruder.com/optimizing-gradient-descent/index.html#batchgradientdescent  \n",
    "http://yann.lecun.com/exdb/mnist/  \n",
    "https://www.quora.com/Artificial-Neural-Networks-What-is-the-difference-between-activation-functions  \n",
    "https://www.tensorflow.org/versions/r0.9/tutorials/mnist/pros/index.html  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
