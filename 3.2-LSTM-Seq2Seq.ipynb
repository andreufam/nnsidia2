{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNNs Sequence to Sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nas RNN $n \\times m$ que vimos, a \"interpretação\" ocorre concomitantemente à \"leitura\". Muitas vezes, contudo, é melhor esperar primeiro por uma interpretação do todo para, só então, iniciar o processo de decodificação. \n",
    "\n",
    "Arquiteturas que usam esta estratégias são as __Seq2seq__. Elas consistem normalmente de duas RNNs, uma codificadora e uma decodificadora, que operam como ilustrado a seguir:\n",
    "\n",
    "_sequência-entrada_ -> **[codificador]** -> _representação_ -> **[decodificador]** -> _sequência-saída_\n",
    "\n",
    "Assim, a ideia geral é usar a representação interna de uma rede codificadora para capturar o significado e contexto da entrada. Esta informação é então fornecida para a decodificadora que pode, a partir de um símbolo de partida e da representação da codificadora, ir prevendo a próxima saída decodificada até o fim da sequência.  \n",
    "\n",
    "Vamos estudar esta rede com uma aplicação em um problema muito comum em países de língua inglesa: soletrar uma palavra a partir de sua pronúncia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Soletrando a partir de pronúncias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No problema que vamos abordar, queremos traduzir a pronuncia de uma palavra, dada como uma lista de fonemas, para a grafia da palavra. Este problema é mais simples que _fala para texto_ ou _tradução_ (no sentido de não precisarmos de quantidades colossais de dados para ver algo acontecer [:)]); contudo, uma dificuldade aqui é a avaliação na escrita de palavras nunca vistas antes. Isto é díficil porque (1) há muitas pronúncias com várias transcrições razoáveis além de (2) palavras homônicas com transcrições distintas (_read_, no passado e presente, por exemplo). \n",
    "\n",
    "Um caso clássico do inglês é o tough, though, thought, through, thorough, throughout (qual é qual?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manipulando os dados..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inicialmente temos que ler o dicionário de fonemas da CMU, _The CMU pronouncing dictionary_. Neste exemplo, disponibilizamos uma versão dele como CSV, já restritoa apenas a palavras, evitando símbolos especiais."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pdic = pd.read_csv('data/cmudict-compact.csv', comment=';', \n",
    "                   header = -1, names = ['word', 'pronunciation'],\n",
    "                   keep_default_na = False,encoding='latin1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Em uma pequena amostra do dicionário, podemos ver as pronúncias codificadas com os símbolos da ARPAbet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pdic[40150:40155]\n",
    "len(pdic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embora não estritamente necessária para esta aula, temos abaixo um dicionário que indica para cada símbolo ARPAbet, seu correspondente IPA e uma palavra de exemplo que ilustra o som do fonema. Por exemplo, o som ʒ (ARPAbet ZH) corresponde ao som de /s/ em _pleasure_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "IPA_SYMBOLS = {'AXR':('ɚ','letter'), 'IY':('i','beat'), 'W':('w','wise'), \n",
    "               'DH':('ð','thy'), 'EL':('l̩','bottle'), 'WH':('ʍ','why'), \n",
    "               'Y':('j','yacht'), 'HH':('h','high'), 'CH':('tʃ','China'), \n",
    "               'JH':('dʒ','jive'), 'DX':('ɾ','butter'), 'ZH':('ʒ','pleasure'), \n",
    "               'EM':('m̩','rhythm'), 'D':('d','die'), 'NG':('ŋ','sing'), \n",
    "               'NX':('ɾ̃','winner'), 'TH':('θ','thigh'), 'H':('h','high'), \n",
    "               'AA':('ɑ','bot'), 'IX':('ɨ','rabbit'), 'B':('b','buy'), \n",
    "               'AE':('æ','bat'), 'EH':('ɛ','bet'), 'G':('ɡ','guy'), \n",
    "               'F':('f','fight'), 'AH':('ʌ','butt'), 'K':('k','kite'), \n",
    "               'M':('m','my'), 'L':('l','lie'), 'AO':('ɔ','bought'), \n",
    "               'N':('n','nigh'), 'Q':('ʔ','uh-oh'), 'IH':('ɪ','bit'), \n",
    "               'S':('s','sigh'), 'R':('ɹ','rye'), 'EY':('eɪ','bait'), \n",
    "               'T':('t','tie'), 'AW':('aʊ','bout'), 'V':('v','vie'), \n",
    "               'AY':('aɪ','bite'), 'AX':('ə','about'), 'Z':('z','zoo'), \n",
    "               'ER':('ɝ','bird'), 'UX':('ʉ','dude'), 'P':('p','pie'), \n",
    "               'UW':('u','boot'), 'SH':('ʃ','shy'), 'UH':('ʊ','book'), \n",
    "               'OY':('ɔɪ','boy'), 'OW':('oʊ','boat'), 'EN':('n̩','button')}\n",
    "\n",
    "def get_ipa_symbol(s):\n",
    "    lc = ''\n",
    "    if s[-1].isdigit():\n",
    "        lc = s[-1]\n",
    "        s = s[:-1]\n",
    "    return IPA_SYMBOLS[s][0] + lc if s in IPA_SYMBOLS else ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assim, por exemplo, a palavra _facial_ tem pronúnica _f-eɪ-ʃ-ʌ-l_ (F-EY1-SH-AH0-L, em ARPAbet, ou os sons de /f/ em _fight_, /ai/ em _bait_, /sh/ em _shy_, /u/ em _butt_ e /l/ em _lie_)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para manter o problema em um tamanho razoável, vamos usar apenas uma fração do dicionário de fonemas. Também vamos filtrar as palavras muito curtas, muito longas ou com símbolos especiais."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_samples = 50000  # Number of samples to train on.\n",
    "pdic = pdic.sample(n = num_samples)\n",
    "\n",
    "def filter_input(inp):    \n",
    "    return ((len(inp) < 5 or      # filter long words \n",
    "             len(inp) > 15) or\n",
    "            # filter words with not alphabetical chars\n",
    "            any((not s.isalpha() for s in inp)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Em nosso problema, a entrada serão as sequências de fonemas e a saída, as palavras. O script abaixo extrai todas as entradas (listas de fonemas) e alvos (palavras), bem como os conjuntos de símbolos observados nas entradas e alvos (note que os alvos são sempre precedidos de um símbolo que indica início de sequência ['\\t'] e terminados em um que indica fim de sequência ['\\n']):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Vectorize the data.\n",
    "input_texts = []\n",
    "target_texts = []\n",
    "input_symbols = set()\n",
    "target_symbols = set()\n",
    "for idx, cols in pdic.iterrows():\n",
    "    target = cols['word']\n",
    "    if filter_input(target):\n",
    "        continue\n",
    "    # We use \"tab\" as the \"start sequence\" character\n",
    "    # for the targets, and \"\\n\" as \"end sequence\" character.\n",
    "    target_text = '\\t' + target + '\\n' # sequence of letters\n",
    "    target_texts.append(target_text) \n",
    "    input_text = cols['pronunciation'].split() # sequence of phonemes\n",
    "    input_texts.append(input_text)\n",
    "    for symbol in input_text:\n",
    "        if symbol not in input_symbols:\n",
    "            input_symbols.add(symbol)\n",
    "    for symbol in target_text:\n",
    "        if symbol not in target_symbols:\n",
    "            target_symbols.add(symbol)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Abaixo, obtemos algumas estatísticas sobre nossos dados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 40283\n",
      "Max sequence length for inputs: 16\n",
      "Max sequence length for outputs: 17\n",
      "Number of unique input tokens: 69\n",
      "- AA0 AA1 AA2 AE0 AE1 AE2 AH0 AH1 AH2 AO0 AO1 AO2 AW0 AW1 AW2 AY0 AY1 AY2 B CH D DH EH0 EH1 EH2 ER0 ER1 ER2 EY0 EY1 EY2 F G HH IH0 IH1 IH2 IY0 IY1 IY2 JH K L M N NG OW0 OW1 OW2 OY0 OY1 OY2 P R S SH T TH UH0 UH1 UH2 UW0 UW1 UW2 V W Y Z ZH\n",
      "Number of unique output tokens: 28\n",
      "- '\\t \\n A B C D E F G H I J K L M N O P Q R S T U V W X Y Z'\n"
     ]
    }
   ],
   "source": [
    "input_symbols = sorted(list(input_symbols))\n",
    "target_symbols = sorted(list(target_symbols))\n",
    "num_encoder_tokens = len(input_symbols)\n",
    "num_decoder_tokens = len(target_symbols)\n",
    "max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
    "max_decoder_seq_length = max([len(txt) for txt in target_texts])\n",
    "\n",
    "print('Number of samples:', len(input_texts))\n",
    "print('Max sequence length for inputs:', max_encoder_seq_length)\n",
    "print('Max sequence length for outputs:', max_decoder_seq_length)\n",
    "print('Number of unique input tokens:', num_encoder_tokens)\n",
    "print('-', ' '.join(input_symbols))\n",
    "print('Number of unique output tokens:', num_decoder_tokens)\n",
    "print('-', repr(' '.join(target_symbols)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Abaixo, vamos criar os mapas que vão fornecer os mapeamentos de cada símbolo para o seu índice correspondente. Com isso, iniciamos os vetores de embeddings que serão usados para representar cada um dos símbolos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_token_index = dict(\n",
    "    [(s, i) for i, s in enumerate(input_symbols)])\n",
    "target_token_index = dict(\n",
    "    [(s, i) for i, s in enumerate(target_symbols)])\n",
    "\n",
    "encoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_encoder_seq_length, num_encoder_tokens),\n",
    "    dtype='float32')\n",
    "decoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
    "    dtype='float32')\n",
    "decoder_target_data = np.zeros(\n",
    "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
    "    dtype='float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A codificação que vamos usar aqui é _one-hot-vector_:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
    "    for t, sym in enumerate(input_text):\n",
    "        encoder_input_data[i, t, input_token_index[sym]] = 1.\n",
    "    for t, sym in enumerate(target_text):\n",
    "        # decoder_target_data is ahead of decoder_target_data by one timestep\n",
    "        decoder_input_data[i, t, target_token_index[sym]] = 1.\n",
    "        if t > 0:\n",
    "            # decoder_target_data will be ahead by one timestep\n",
    "            # and will not include the start character.\n",
    "            decoder_target_data[i, t - 1, target_token_index[sym]] = 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Abaixo, podemos visualizar uma das matrizes de representação esparsas geradas. As colunas correspondem aos símbolos e as linhas ao tempo. Cada 1 foi representado como um asterisco em um mar de pontos ;). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'\\tALIZAC\\n'\n",
      "   tnABCDEFGHIJKLMNOPQRSTUVWXYZ\n",
      " 0 ..*.........................\n",
      " 1 .............*..............\n",
      " 2 ..........*.................\n",
      " 3 ...........................*\n",
      " 4 ..*.........................\n",
      " 5 ....*.......................\n",
      " 6 .*..........................\n",
      " 7 ............................\n",
      " 8 ............................\n",
      " 9 ............................\n",
      "10 ............................\n",
      "11 ............................\n",
      "12 ............................\n",
      "13 ............................\n",
      "14 ............................\n",
      "15 ............................\n",
      "16 ............................\n"
     ]
    }
   ],
   "source": [
    "def print_mat_as_map(m):\n",
    "    print('   tnABCDEFGHIJKLMNOPQRSTUVWXYZ')\n",
    "    for i in range(m.shape[0]):\n",
    "        print('%2d ' % i, end='')\n",
    "        for j in range(m.shape[1]):\n",
    "            print('%s' % '.' if m[i,j]==0 else '*', end='')\n",
    "        print('\\n', end='')\n",
    "        \n",
    "print('%s'%repr(target_texts[0]))\n",
    "print_mat_as_map(decoder_target_data[0, :, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Em uma aplicação com palavras (ex: tradução de idiomas), esta codificação seria muito ineficiente em espaço e carga semântica. Nestes casos, vamos optar por uma representação densa, ou seja, um _embedding_ de palavras. Estes serão obtidos de rede neural que modele linguagem. Há muitas disponíveis, incluindo várias arquiteturas baseadas em RNN/LSTM. As mais comumente usadas hoje, contudo, são redes rasas que codificam as palavras considerando suas co-ocorrências em grandes corpora de texto (ex: word2vec e Glove)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementando em tensorflow, usando Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora que os dados foram representados, podemos criar nossa arquitetura seq2seq:\n",
    "\n",
    "<img src=\"images/rnn_s2s0.png\" alt=\"Exemplo de RNN\" style=\"width: 600px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A primeira metade da RNN age como um codificador, processando a sequência de entrada e retornando seu próprio estado interno. As saídas são descartadas e nem aparecem no desenho. O estado interno serve como contexto e condiciona a estimativa do decodificador."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:rgba(229, 255, 204, 0.5); text-align:left; vertical-align: middle; padding:10px 10px;\">\n",
    "\n",
    "Note que como vamos usar unidades LSTMs, o estado interno é constituído pela codificação que a LSTM gerou da entrada ($h_t$) e pela sua memória ($c_t$), conforme ilustrado abaixo:\n",
    "\n",
    "<img src=\"images/lstmcell.png\" alt=\"célula LSTM\" style=\"width: 200px;\"/>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A outra RNN é o decodificador. Ela é treinada para prever o próximo elemento na sequência alvo, dado seus predecessores. Ou seja, para ela a entrada é a sequência alvo e o alvo é a sequência alvo, um passo à frente (esse processo é chamado de 'teacher forcing'). O estado inicial do decodificador é o estado final do codificador. Ou seja, o decodificador aprende a gerar alvos[t+1..] de alvos[t...] condicionado à sequência de entrada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "latent_dim = 256  # Latent dimensionality of the encoding space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define an input sequence and process it.\n",
    "encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
    "encoder = LSTM(latent_dim, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "# We discard `encoder_outputs` and only keep the states.\n",
    "encoder_states = [state_h, state_c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set up the decoder, using `encoder_states` as initial state.\n",
    "decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
    "# We set up our decoder to return full output sequences,\n",
    "# and to return internal states as well. We don't use the\n",
    "# return states in the training model, but we will use them in inference.\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs,\n",
    "                                     initial_state=encoder_states)\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, None, 69)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, None, 28)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, 256), (None, 333824      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   [(None, None, 256),  291840      input_2[0][0]                    \n",
      "                                                                 lstm_1[0][1]                     \n",
      "                                                                 lstm_1[0][2]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, None, 28)     7196        lstm_2[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 632,860\n",
      "Trainable params: 632,860\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define the model that will turn\n",
    "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 64  # Batch size for training.\n",
    "epochs = 20 # 10  # Number of epochs to train for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 32226 samples, validate on 8057 samples\n",
      "Epoch 1/20\n",
      "32226/32226 [==============================] - 36s 1ms/step - loss: 1.2205 - val_loss: 1.0282\n",
      "Epoch 2/20\n",
      "32226/32226 [==============================] - 33s 1ms/step - loss: 0.9215 - val_loss: 0.8299\n",
      "Epoch 3/20\n",
      "32226/32226 [==============================] - 33s 1ms/step - loss: 0.7440 - val_loss: 0.6546\n",
      "Epoch 4/20\n",
      "32226/32226 [==============================] - 33s 1ms/step - loss: 0.5852 - val_loss: 0.5521\n",
      "Epoch 5/20\n",
      "32226/32226 [==============================] - 33s 1ms/step - loss: 0.4729 - val_loss: 0.4469\n",
      "Epoch 6/20\n",
      "32226/32226 [==============================] - 33s 1ms/step - loss: 0.4028 - val_loss: 0.4038\n",
      "Epoch 7/20\n",
      "32226/32226 [==============================] - 33s 1ms/step - loss: 0.3525 - val_loss: 0.3549\n",
      "Epoch 8/20\n",
      "32226/32226 [==============================] - 33s 1ms/step - loss: 0.3142 - val_loss: 0.3369\n",
      "Epoch 9/20\n",
      "32226/32226 [==============================] - 33s 1ms/step - loss: 0.2832 - val_loss: 0.3124\n",
      "Epoch 10/20\n",
      "32226/32226 [==============================] - 33s 1ms/step - loss: 0.2577 - val_loss: 0.2886\n",
      "Epoch 11/20\n",
      "32226/32226 [==============================] - 33s 1ms/step - loss: 0.2366 - val_loss: 0.2828\n",
      "Epoch 12/20\n",
      "32226/32226 [==============================] - 33s 1ms/step - loss: 0.2178 - val_loss: 0.2719\n",
      "Epoch 13/20\n",
      "32226/32226 [==============================] - 33s 1ms/step - loss: 0.2014 - val_loss: 0.2775\n",
      "Epoch 14/20\n",
      "32226/32226 [==============================] - 33s 1ms/step - loss: 0.1865 - val_loss: 0.2625\n",
      "Epoch 15/20\n",
      "32226/32226 [==============================] - 33s 1ms/step - loss: 0.1731 - val_loss: 0.2613\n",
      "Epoch 16/20\n",
      "32226/32226 [==============================] - 33s 1ms/step - loss: 0.1608 - val_loss: 0.2536\n",
      "Epoch 17/20\n",
      "32226/32226 [==============================] - 33s 1ms/step - loss: 0.1489 - val_loss: 0.2606\n",
      "Epoch 18/20\n",
      "32226/32226 [==============================] - 33s 1ms/step - loss: 0.1380 - val_loss: 0.2542\n",
      "Epoch 19/20\n",
      "32226/32226 [==============================] - 33s 1ms/step - loss: 0.1281 - val_loss: 0.2565\n",
      "Epoch 20/20\n",
      "32226/32226 [==============================] - 33s 1ms/step - loss: 0.1185 - val_loss: 0.2620\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda\\lib\\site-packages\\keras\\engine\\network.py:872: UserWarning: Layer lstm_2 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_1/while/Exit_2:0' shape=(?, 256) dtype=float32>, <tf.Tensor 'lstm_1/while/Exit_3:0' shape=(?, 256) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n"
     ]
    }
   ],
   "source": [
    "# Run training\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
    "model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_split=0.2)\n",
    "# Save model\n",
    "model.save('/tmp/ks2s.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para fazer a inferência, iremos usar a seguinte estratégia:\n",
    "\n",
    "1. Obtenha o estado do codificador para a sequência de entrada.\n",
    "2. Inicie com uma sequência alvo de tamanho 1 (apenas o símbolo de início de sequência).\n",
    "3. Dê o estado do codificador e a sequência criada até agora para o decodificador produzir uma distribuição de probabilidade para o próximo símbolo.\n",
    "4. Amostre o próximo símbolo usando a distribuição (no exemplo a sequir, é apenas usado argmax).\n",
    "5. Concatene o símbolo amostrado para a sequêcia alvo\n",
    "6. Repita desde 1 até encontrar o símbolo de fim de sequência ou alcançar o tamanho máximo de representação da saída.\n",
    "\n",
    "Note que esta estratégia poderia ter sido usada para treinar a rede também."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Next: inference mode (sampling).\n",
    "# Here's the drill:\n",
    "# 1) encode input and retrieve initial decoder state\n",
    "# 2) run one step of decoder with this initial state\n",
    "# and a \"start of sequence\" token as target.\n",
    "# Output will be the next target token\n",
    "# 3) Repeat with the current target token and current states\n",
    "\n",
    "# Define sampling models\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "    decoder_inputs, initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs] + decoder_states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos testar o nosso modelo de inferência."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# code a set of phonemes into an input for the RNN\n",
    "def encoder_test(phones):\n",
    "    encoder_test_data = np.zeros((1, max_encoder_seq_length, \n",
    "                                  num_encoder_tokens), dtype='float32')\n",
    "    for t, sym in enumerate(phones.split(' ')):\n",
    "        encoder_test_data[0, t, input_token_index[sym]] = 1.\n",
    "    return encoder_test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# show decoder softmax distribuition \n",
    "def plot_decoded_dist(v):\n",
    "    axis = np.arange(num_decoder_tokens)\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.bar(axis, v, align = 'center')\n",
    "    ax.set_xticks(axis)\n",
    "    ax.set_xticklabels([c for c in '<>ABCDEFGHIJKLMNOPQRSTUVWXYZ'])\n",
    "    plt.xlim([0, num_decoder_tokens])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]]], dtype=float32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_test_data = encoder_test('UW1 L ER0')\n",
    "encoder_test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 256) (1, 256)\n"
     ]
    }
   ],
   "source": [
    "# Encode the input as state vectors.\n",
    "states_value = encoder_model.predict(encoder_test_data)\n",
    "print(states_value[0].shape, states_value[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1, 28) (1, 256) (1, 256)\n"
     ]
    }
   ],
   "source": [
    "# Generate empty target sequence of length 1.\n",
    "target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "# Populate the first character of target sequence with the start character.\n",
    "target_seq[0, 0, target_token_index['\\t']] = 1.\n",
    "decoder_input = [target_seq] + states_value\n",
    "print(decoder_input[0].shape, decoder_input[1].shape, decoder_input[2].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "output_tokens, h, c = decoder_model.predict(decoder_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD8CAYAAACb4nSYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x18aefe951d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_decoded_dist(output_tokens[0,0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Reverse-lookup token index to decode sequences back to\n",
    "# something readable.\n",
    "reverse_input_char_index = dict(\n",
    "    (i, char) for char, i in input_token_index.items())\n",
    "reverse_target_char_index = dict(\n",
    "    (i, char) for char, i in target_token_index.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0, target_token_index['\\t']] = 1.\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict(\n",
    "            [target_seq] + states_value)\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if (sampled_char == '\\n' or\n",
    "           len(decoded_sentence) > max_decoder_seq_length):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "\n",
    "    start = 0 if decoded_sentence[0] != ' ' else 1\n",
    "    return decoded_sentence[start:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Guess         Correct = Phonemes\n",
      "         ALIZAK          ALIZAC   æ1 l ɪ0 z æ0 k\n",
      "      DETHRONED       DETHRONED + d ɪ0 θ ɹ oʊ1 n d\n",
      "        DUMMING         DUMBING   d ʌ1 m ɪ0 ŋ\n",
      "    COMMONSENSE     COMMONSENSE + k ɑ2 m ʌ0 n s ɛ1 n s\n",
      "         BROOKS          BROOKS + b ɹ ʊ1 k s\n",
      "      STRICKLER       STRICKLER + s t ɹ ɪ1 k l ɝ0\n",
      "        VASICEK         VASICEK + v ɑ1 s ɪ0 tʃ ɛ0 k\n",
      "        INSAFOR         INSOFAR   ɪ1 n s ʌ0 f ɑ0 ɹ\n",
      "          GOONS           GOONS + ɡ u1 n z\n",
      "        PIGGOTT         PIGGOTT + p ɪ1 ɡ ʌ0 t\n",
      "       DELADGER        DELAWDER   d ɛ1 l ɔ0 d ɝ0\n",
      "          GORON          GARONE   ɡ ɝ0 oʊ1 n\n",
      "          ALTOS           ALTOS + æ1 l t oʊ0 z\n",
      "       PRETEENS        PRETEENS + p ɹ i2 t i1 n z\n",
      "       PLATONIC        PLATONIC + p l ʌ0 t ɑ1 n ɪ0 k\n",
      "      INCREASES       INCREASES + ɪ2 n k ɹ i1 s ɪ0 z\n",
      "     GARCZYNSKI      GORCZYNSKI   ɡ ɝ0 tʃ ɪ1 n s k i0\n",
      "      DOWNSIZED       DOWNSIZES   d aʊ1 n s aɪ2 z ɪ0 z\n",
      "      ORIGINATE       ORIGINATE + ɝ0 ɪ1 dʒ ʌ0 n eɪ2 t\n",
      "    LEPRECHOUNS     LEPRECHAUNS   l ɛ1 p ɝ0 k ɔ2 n z\n",
      "     BUTTERCUPS      BUTTERCUPS + b ʌ1 t ɝ0 k ʌ2 p s\n",
      "       FREISCHA         FRISCIA   f ɹ i1 s tʃ ʌ0\n",
      "         KELNER         KELLNER   k ɛ1 l n ɝ0\n",
      "       DISJOINT        DISJOINT + d ɪ0 s dʒ ɔɪ1 n t\n",
      "      CLASSROOM       CLASSROOM + k l æ1 s ɹ u2 m\n",
      "         SIMBEL          SYMBOL   s ɪ1 m b ʌ0 l\n",
      "         ZIMMER          ZIMMER + z ɪ1 m ɝ0\n",
      "        CRUMLEY         CRUMLEY + k ɹ ʌ1 m l i0\n",
      "     DERRICKSON       DERICKSON   d ɛ1 ɹ ɪ0 k s ʌ0 n\n",
      "        COFFERS         COFFERS + k ɑ1 f ɝ0 z\n",
      "      ERNOUGHTS          ERNEST   ɝ1 n ʌ0 s t\n",
      "          BELUE          BELLEW   b ɪ0 l u1\n",
      "     STEINBAUGH      STEINBAUGH + s t aɪ1 n b aʊ0\n",
      "        VIGILIA         VIGILIA + v i0 dʒ i1 l i0 ʌ0\n",
      "   UNENFORCEDOR   UNENFORCEABLE   ʌ2 n ɛ0 n f ɔ1 ɹ s ʌ0 b ʌ0 l\n",
      "         LEACED          LEASED   l i1 s t\n",
      "         SHAMEL          SHAMEL + ʃ æ1 m ʌ0 l\n",
      "          GREGO           GREGO + ɡ ɹ ɛ1 ɡ oʊ0\n",
      "       AMBERGER        AMBERGER + æ1 m b ɝ0 ɡ ɝ0\n",
      "       NORWEGEN       NORWEGIAN   n ɔ2 ɹ w i1 dʒ ʌ0 n\n",
      "        CAPALBO         CAPALBO + k ʌ0 p æ1 l b oʊ0\n",
      "      MAMERONEC      MAMARONECK   m ʌ0 m ɛ1 ɹ ʌ0 n ɛ0 k\n",
      "   PALISTINIANS    PALESTINIANS   p æ2 l ɪ0 s t ɪ1 n i0 ʌ0 n z\n",
      "     FANNANCILE    PFANNENSTIEL   f æ1 n ʌ0 n s t i0 l\n",
      "    PREEMINANCE     PREEMINENCE   p ɹ i0 ɛ1 m ʌ0 n ʌ0 n s\n",
      "      HUDSUCKER       HUDSUCKER + h ʌ1 d s ʌ2 k ɝ0\n",
      "       DALLAIRE        DALLAIRE + d ɑ1 l ɛ0 ɹ\n",
      "         PASINO         PASSINO   p ɑ0 s i1 n oʊ0\n",
      "        HABITAT         HABITAT + h æ1 b ʌ0 t æ2 t\n",
      "        SCHOOLS         SCHOOLS + s k u1 l z\n"
     ]
    }
   ],
   "source": [
    "print('%15s %15s = %s' % ('Guess', 'Correct', 'Phonemes'))\n",
    "for seq_index in range(50):\n",
    "    # Take one sequence (part of the training test)\n",
    "    # for trying out decoding.\n",
    "    input_seq = encoder_input_data[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    inputs = ' '.join(input_texts[seq_index])\n",
    "    correct = pdic[pdic['pronunciation']==inputs]['word'].iloc[0]\n",
    "    ok = '+' if decoded_sentence[:-1] == correct else ' '\n",
    "    ipa_inputs = ' '.join([get_ipa_symbol(c) for c in inputs.split(' ')])\n",
    "    print('%15s %15s %s %s'%(decoded_sentence[:-1], correct, ok, ipa_inputs))  \n",
    "    #print('%15s %15s %s %s'%(decoded_sentence[:-1], correct, ok, inputs))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mecanismos de Atenção"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Em nosso modelo, quando o decodificador processa suas entradas, toda a informação de contexto se concentra nos estados recebidos do codificador. Deste modo, a entrada do decodificador é um gargalo e ele não tem como relacionar diretamente aspectos sendo decodificados com os elementos fonte diretamente associados a estes aspectos. \n",
    "\n",
    "Este é um problema particularmente importante com sequências muito longas. Nestes casos, à medida que o decodificador processa a informação, ele tende a ficar menos preso à fonte e mais impressionado com o material que esta produzindo. \n",
    "\n",
    "Por exemplo, imagine que a frase a ser traduzida seja `You can't make an omelet without breaking a few legs`. Ao alcançar a tradução de `legs`, a rede pode achar que é mais provável a palavra `ovos` (`eggs`) porque ela é mais razoável no contexto da sentença sendo gerada (`Você não pode fazer um omelete sem quebrar alguns...`), _a despeito dela não ser a tradução correta_. Este problema se manifesta de várias formas. Em modelos de tradução, por exemplo, o decodificador 'esquece' a concordância de gênero, tempo, pessoa, etc.\n",
    "\n",
    "Para resolver este problema, Bahdanau et al., 2015 (https://arxiv.org/abs/1409.0473) e Luong et al., 2015 (https://arxiv.org/abs/1508.04025) introduziram a ideia de _mecanismos de atenção_. \n",
    "\n",
    "A ideia central é o estabelecimento de uma ligação direta entre as estimativas do codificador e do decodificador, de forma que o codificador preste atenção em informações relevantes do decodificador enquanto ele traduz/decodifica. Como resultado, este mecanismo ajuda muito na tradução de sentenças longas e se tornou o padrão de fato em tradução, com aplicação também em diversos outros problemas (fala, sumarização, _captioning_, etc). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entre os vários mecanismos de atenção na literatura, vamos descrever o proposto por Luong et al., 2015. Ele é implementado no tensorflow. A figura abaixo ilustra a computação da atenção no primeiro passo da decodificação:\n",
    "\n",
    "<img src=\"images/rnn_s2s_attention.png\" alt=\"Exemplo de RNN\" style=\"width: 600px;\"/>\n",
    "\n",
    "A computação consiste em 3 passos:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* __Passo 1__: O estado oculto atual é comparado com todos os estados no codificador (fonte) para derivar os _attention weights_. A figura abaixo ilustra esses pesos (exemplo de Bahdanau et al, 2015):\n",
    "\n",
    " <img src=\"images/attention_vis.jpg\" alt=\"Exemplo de RNN\" style=\"width: 400px;\"/>\n",
    "\n",
    " Note que os attention weights possibilitam uma fácil visualização do alinhamento entre as sentenças de entrada e saída, um subproduto interessante de adotá-los. No exemplo, fica claro que o conceito _european economic area_ está assoaciado (alinhado) a um conceito correspondente no codificador, _zone économique européenne_.\n",
    "\n",
    " Este passo pode ser descrito como:\n",
    "\n",
    " $$\\alpha_{ts} = \\frac{e^{score({\\bf h}_t, {\\bf {\\bar h}}_s)}}{\\sum_{s''}{e^{score({\\bf h}_t, {\\bf {\\bar h}}_s')}}}$$\n",
    "\n",
    " onde a comparação entre os vetores pode ser feita com diferentes funções _score_. É em geral na definição de _score_ que reside a diferença entre vários mecanismos de atenção. Por exemplo, em Bahdanau et al (2015), a função _score_ é dada por:\n",
    "\n",
    " $$score({\\bf h}_t, {\\bf {\\bar h}}_s) = {{\\bf h}_t}^\\intercal {\\bf W} {\\bf {\\bar h}}_s$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* __Passo 2__: os _attention weights_ são agregados para formar o _context vector_. A agregação é feita como uma média ponderada dos estados.\n",
    "\n",
    "$${\\bf c}_t = \\sum_s{\\alpha_{ts} {\\bf {\\bar h}}_s}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* __Passo 3__: o estado oculto atual é então combinado como o _context vector_ ($[{\\bf c}_t; {\\bf h}_t]$) para produzir o _attention vector_ final. Este vetor é fornecido como entrada para a previsão do próximo passo.\n",
    "\n",
    "$$\\alpha_t = f({\\bf c}_t, {\\bf h}_t) = tanh({\\bf W}_c [{\\bf c}_t; {\\bf h}_t])$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O tensorflow implementa diferentes mecanismos de atenção. Recomendo a leitura de Thang Luong, Eugene Brevdo, and Rui Zhao (https://github.com/tensorflow/nmt), para uma visão detalhada destas técnicas e suas melhores implementações em TF."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algumas observações finais\n",
    "\n",
    "* A Arquitetura de uma RNN pode ser bidirecional. Ou seja, cada célula pode se conectar a uma sucessora ($t+1$) e a uma antecessora ($t-1$). No caso de LSTMs, que têm memória com fluxo separado na sequ6encia, a estrutura bidirecional pode ser formada por duas seqências de células, uma da esquerda para direita e outra da direita para esquerda. Assim, cada célula na primeira sequência tem uma correspondente na segunda e elas compartilham entradas e saídas. Modelos bidirecionais tipicamente alcançam resultados melhores que os unidirecionais em muitas aplicações.\n",
    "\n",
    "<img src=\"images/bilstm.png\" alt=\"bi-directional LSTM\" style=\"width: 400px;\"/>\n",
    "\n",
    "* Embora em processamento de texto seja comum a modelagem de palavras de forma discreta, este não é o caso em redes neurais cuja entrada é formada por palavras. As razões para isso são eficiência e perda de informação semântica. Assim, nesses casos, é melhor usar os códigos gerados por redes neurais que modelam linguagens (embeddings). Estes códigos curtos e densos gerados por redes usadas exclusivamente como sistemas codificadores serão o foco das próximas aulas. E a ideia de redes auto-codificadoras também.\n",
    "\n",
    "<img src=\"images/bi-lstm-embeddings.png\" alt=\"bi-directional LSTM\" style=\"width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este curso é baseado em material da [Big Data University](https://bigdatauniversity.com/?utm_source=bducopyrightlink&utm_medium=dswb&utm_campaign=bdu). Assim, segue os termos da [licença do MIT](https://bigdatauniversity.com/mit-license/). Material adicional de François Chollet (https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html), Mikesj (https://github.com/mikesj-public/rnn_spelling_bee/blob/master/spelling_bee_RNN.ipynb), Thang Luong, Eugene Brevdo, and Rui Zhao (https://github.com/tensorflow/nmt)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
